{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":"python","separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"About"},{"location":"publications/","text":"Whitepaper Blogs Outline our vision, the motivation, main ideas and the principles upon which Fybrik is built Additional reading on Fybrik concepts, use cases, components and more","title":"Publications"},{"location":"concepts/architecture/","text":"Architecture Fybrik takes a modular approach to provide an open platform for controlling and securing the use of data across an organization. The figure below showcases the current architecture of the Fybrik platform, running on top of Kubernetes. The core parts of Fybrik are based on Kubernetes controllers and Custom Resource Definitions (CRDs) in order to define its work items and reconcile the state of them. The primary interaction object for a data user (e.g. data scientist) is the FybrikApplication custom resource where a user defines which data should be used for which purpose. The following chart and description describe the architecture and components of Fybrik relative to when they are used. Before the data user (e.g. data scientist) can perform any actions a data operator (e.g. IT admin) has to install Fybrik and modules. Modules Modules (M) describe capabilities that can be included in a data plane. These modules may be existing open source or third party services, or custom ones. The module of a service indicates the capabilities it supports, the formats and interfaces, and how to deploy the service. Modules may describe externally deployed services, or services deployed by fybrik. Examples of modules are those that provide read/write access or produce implicit copies that serve as lower latency caches of remote assets. Modules may also perform actions to enforce data governance policy decisions, such as masking or redaction as examples. Connectors Fybrik connects to external services to receive data governance decisions, metadata about datasets and credentials. Policies, assets and access credentials to the assets have to be defined before the user can run an application. The current abstraction supports 2 different connectors : one for data catalog and one for policy manager. It is designed in an open way so that multiple different catalog and policy frameworks of all kinds of cloud and on-prem systems can be supported. The data steward (e.g. data governance officer) configures policies in an external policy manager over assets defined in an external data catalog. Dataset credentials are retrieved from Vault by using Vault API . Vault uses a custom secret engine implemented with HashiCorp Vault plugins system to retrieve the credentials from where they are stored (data catalog for example). FybrikApplication Once Fybrik is deployed and configured, including catalogs, modules and connectors, a developer can submit a FybrikApplication Custom Resource to Kubernetes. The FybrikApplication holds metadata about the application such as the data assets required by the application, the processing purpose and the method of access the user wishes (protocol e.g. S3 or Arrow flight). The FybrikApplicationController will make sure that all the specs are fulfilled and that the data is read/written/copied/deleted in accord with the data governance policies and the IT config policies. The controller uses the information provided in the FybrikApplication , to check with the data-governance policy manager if the data flow requested is allowed and whether restrictive actions such as masking or hashing have to be applied. Taking into account these governance actions, as well as application requirements, dataset specification, available infrastructure and the IT config policies defined, the controller compiles a plotter. Plotter The plotter specifies a data plane connecting the application to the datasets it requires. More specifically, the plotter lists the modules to use, the capabilities required from these modules, the cluster on which each module should be deployed, as well as the flow of data between the asset and the workload through the chosen modules. For the plotter to be optimal in terms of the defined optimization goals (a.k.a. IT config soft policies ), the controller may use a CSP-based optimizer . If no CSP engine is installed, optimization goals will not be taken into account, and the manager will use the first (but not necessarily optimal) solution that meets all of the other requirements. Blueprint As data assets may reside in different clusters/clouds a Blueprint CRD is created for each cluster, containing the information regarding the services to be deployed or configured in the given cluster. Depending on the setup the PlotterController will use various methods to distribute the blueprints. A single blueprint contains the specification of all assets that shall be accessed in a single cluster by a single application. The BlueprintController makes sure that a blueprint can deploy all needed modules and tracks their status. Once e.g. an implicit-copy module finishes the copy the blueprint is also in a ready state. A read or write module is in ready state as soon as the proxy service such as the arrow-flight module is running. In a multi cluster setup the default distribution implementation is using Razee to control remote blueprints, but other multi-cluster tools could be used as a replacement. The PlotterController also collects statuses and distributes updates of said blueprints. Once all the blueprints on all clusters are ready the plotter is marked as ready, and the overall status is propagated back to the user in the FybrikApplication status.","title":"Architecture"},{"location":"concepts/architecture/#architecture","text":"Fybrik takes a modular approach to provide an open platform for controlling and securing the use of data across an organization. The figure below showcases the current architecture of the Fybrik platform, running on top of Kubernetes. The core parts of Fybrik are based on Kubernetes controllers and Custom Resource Definitions (CRDs) in order to define its work items and reconcile the state of them. The primary interaction object for a data user (e.g. data scientist) is the FybrikApplication custom resource where a user defines which data should be used for which purpose. The following chart and description describe the architecture and components of Fybrik relative to when they are used. Before the data user (e.g. data scientist) can perform any actions a data operator (e.g. IT admin) has to install Fybrik and modules.","title":"Architecture"},{"location":"concepts/architecture/#modules","text":"Modules (M) describe capabilities that can be included in a data plane. These modules may be existing open source or third party services, or custom ones. The module of a service indicates the capabilities it supports, the formats and interfaces, and how to deploy the service. Modules may describe externally deployed services, or services deployed by fybrik. Examples of modules are those that provide read/write access or produce implicit copies that serve as lower latency caches of remote assets. Modules may also perform actions to enforce data governance policy decisions, such as masking or redaction as examples.","title":"Modules"},{"location":"concepts/architecture/#connectors","text":"Fybrik connects to external services to receive data governance decisions, metadata about datasets and credentials. Policies, assets and access credentials to the assets have to be defined before the user can run an application. The current abstraction supports 2 different connectors : one for data catalog and one for policy manager. It is designed in an open way so that multiple different catalog and policy frameworks of all kinds of cloud and on-prem systems can be supported. The data steward (e.g. data governance officer) configures policies in an external policy manager over assets defined in an external data catalog. Dataset credentials are retrieved from Vault by using Vault API . Vault uses a custom secret engine implemented with HashiCorp Vault plugins system to retrieve the credentials from where they are stored (data catalog for example).","title":"Connectors"},{"location":"concepts/architecture/#fybrikapplication","text":"Once Fybrik is deployed and configured, including catalogs, modules and connectors, a developer can submit a FybrikApplication Custom Resource to Kubernetes. The FybrikApplication holds metadata about the application such as the data assets required by the application, the processing purpose and the method of access the user wishes (protocol e.g. S3 or Arrow flight). The FybrikApplicationController will make sure that all the specs are fulfilled and that the data is read/written/copied/deleted in accord with the data governance policies and the IT config policies. The controller uses the information provided in the FybrikApplication , to check with the data-governance policy manager if the data flow requested is allowed and whether restrictive actions such as masking or hashing have to be applied. Taking into account these governance actions, as well as application requirements, dataset specification, available infrastructure and the IT config policies defined, the controller compiles a plotter.","title":"FybrikApplication"},{"location":"concepts/architecture/#plotter","text":"The plotter specifies a data plane connecting the application to the datasets it requires. More specifically, the plotter lists the modules to use, the capabilities required from these modules, the cluster on which each module should be deployed, as well as the flow of data between the asset and the workload through the chosen modules. For the plotter to be optimal in terms of the defined optimization goals (a.k.a. IT config soft policies ), the controller may use a CSP-based optimizer . If no CSP engine is installed, optimization goals will not be taken into account, and the manager will use the first (but not necessarily optimal) solution that meets all of the other requirements.","title":"Plotter"},{"location":"concepts/architecture/#blueprint","text":"As data assets may reside in different clusters/clouds a Blueprint CRD is created for each cluster, containing the information regarding the services to be deployed or configured in the given cluster. Depending on the setup the PlotterController will use various methods to distribute the blueprints. A single blueprint contains the specification of all assets that shall be accessed in a single cluster by a single application. The BlueprintController makes sure that a blueprint can deploy all needed modules and tracks their status. Once e.g. an implicit-copy module finishes the copy the blueprint is also in a ready state. A read or write module is in ready state as soon as the proxy service such as the arrow-flight module is running. In a multi cluster setup the default distribution implementation is using Razee to control remote blueprints, but other multi-cluster tools could be used as a replacement. The PlotterController also collects statuses and distributes updates of said blueprints. Once all the blueprints on all clusters are ready the plotter is marked as ready, and the overall status is propagated back to the user in the FybrikApplication status.","title":"Blueprint"},{"location":"concepts/config-policies/","text":"IT Config Policies and Data Plane Optimization What are IT config policies? IT config policies are the mechanism via which the organization may influence the construction of the data plane, taking into account infrastructure capabilities and costs. Fybrik takes into account the workload context, the data metadata, the data governance policies and the configuration policies when defining the data plane. IT config policies influence what capabilities should be deployed (e.g. read, copy), in which clusters they should be deployed, and the selection of the most appropriate module that implements the capability. Input to policies The input object includes general application data such as workload cluster and application properties, as well as dataset details (user requirements, metadata). Available properties: cluster.name : name of the workload cluster cluster.metadata.region : region of the workload cluster properties : application/workload properties defined in FybrikApplication, e.g. properties.intent request.metadata : asset metadata as defined in catalog taxonomy, e.g request.metadata.geography usage : a set of boolean properties associated with data use: usage.read , usage.write , usage.copy Syntax Policies are written in rego files. Each file declares a package adminconfig . Rules are written in the following syntax: config[{\"capability\": capability, \"decision\": decision}] where - capability represents a required module capability, such as \"read\", \"write\", \"transform\" and \"copy\". decision is a JSON structure that matches Decision defined above. { \"policy\": {\"ID\": <id>, \"description\": <description>, \"version\": <version>}, \"deploy\": <\"True\", \"False\">, \"restrictions\": { \"modules\": <list of restrictions>, \"clusters\": <list of restrictions>, \"storageaccounts\": <list of restrictions>, }, } restriction restricts a property to either a set of values or a value in a given range . For example, the policy above restricts the choice of clusters and modules for a read capability by narrowing the choice of deployment clusters to the workload cluster, and restricting the module type to service. config[{\"capability\": \"read\", \"decision\": decision}] { input.request.usage == \"read\" policy := {\"ID\": \"read-location\", \"description\":\"Deploy read in the workload cluster\", \"version\": \"0.1\"} cluster_restrict := {\"property\": \"name\", \"values\": [ input.workload.cluster.name ] } module_restrict := {\"property\": \"type\", \"values\": [\"service\"]} decision := {\"policy\": policy, \"restrictions\": {\"clusters\": [cluster_restrict], \"modules\": [module_restrict]}} } policy provides policy metadata: unique ID, human-readable description and version restrictions provides restrictions for modules , clusters and storageaccounts . Each restriction provides a list or a range of allowed values for a property of module/cluster/storageaccount object. For example, to restrict a module type to either \"service\" or \"plugin\", we'll use \"type\" as a property, and [ \"service\",\"plugin ] as a list of allowed values. Properties of a module can be found inside FybrikModule Spec. Properties of a storage account are listed inside FybrikStorageAccount . Cluster is not a custom resource. It has the following properties: name: cluster name metadata.region: cluster region metadata.zone: cluster zone deploy receives \"True\"/\"False\" values. These values indicate whether the capability should or should not be deployed. If not specified in the policy, it's up to Fybrik to decide on the capability deployment. Out of the box policies Out of the box policies come with the fybrik deployment. They define the deployment of basic capabilities, such as read, write, copy and delete. package adminconfig # read capability deployment config[{\"capability\": \"read\", \"decision\": decision}] { input.request.usage == \"read\" policy := {\"ID\": \"read-default-enabled\", \"description\":\"Read capability is requested for read workloads\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"deploy\": \"True\"} } # write capability deployment config[{\"capability\": \"write\", \"decision\": decision}] { input.request.usage == \"write\" policy := {\"ID\": \"write-default-enabled\", \"description\":\"Write capability is requested for workloads that write data\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"deploy\": \"True\"} } # copy requested by the user config[{\"capability\": \"copy\", \"decision\": decision}] { input.request.usage == \"copy\" policy := {\"ID\": \"copy-request\", \"description\":\"Copy (ingest) capability is requested by the user\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"deploy\": \"True\"} } # delete capability deployment config[{\"capability\": \"delete\", \"decision\": decision}] { input.request.usage == \"delete\" policy := {\"ID\": \"delete-request\", \"description\":\"Delete capability is requested by the user\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"deploy\": \"True\"} } # do not deploy copy in scenarios different from read or copy config[{\"capability\": \"copy\", \"decision\": decision}] { input.request.usage != \"read\" input.request.usage != \"copy\" policy := {\"ID\": \"copy-disabled\", \"description\":\"Copy capability is not requested\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"deploy\": \"False\"} } # do not deploy read in other scenarios config[{\"capability\": \"read\", \"decision\": decision}] { input.request.usage != \"read\" policy := {\"ID\": \"read-disabled\", \"description\":\"Read capability is not requested\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"deploy\": \"False\"} } # do not deploy write in other scenarios config[{\"capability\": \"write\", \"decision\": decision}] { input.request.usage != \"write\" policy := {\"ID\": \"write-disabled\", \"description\":\"Write capability is not requested\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"deploy\": \"False\"} } # do not deploy delete in other scenarios config[{\"capability\": \"delete\", \"decision\": decision}] { input.request.usage != \"delete\" policy := {\"ID\": \"delete-disabled\", \"description\":\"Delete capability is not requested\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"deploy\": \"False\"} } Extended policies The extended policies define advanced deployment requirements, such as where read or transform modules should run, what should be the scope of module deployments, and more. The policies below are provided as a sample and can be updated for the production deployment. package adminconfig # configure where transformations take place config[{\"capability\": \"transform\", \"decision\": decision}] { policy := {\"ID\": \"transform-geo\", \"description\":\"Governance based transformations must take place in the geography where the data is stored\", \"version\": \"0.1\"} cluster_restrict := {\"property\": \"metadata.region\", \"values\": [input.request.dataset.geography]} decision := {\"policy\": policy, \"restrictions\": {\"clusters\": [cluster_restrict]}} } # configure the scope of the read capability config[{\"capability\": \"read\", \"decision\": decision}] { input.request.usage == \"read\" policy := {\"ID\": \"read-scope\", \"description\":\"Deploy read at the workload scope\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"restrictions\": {\"modules\": [{\"property\": \"capabilities.scope\", \"values\" : [\"workload\"]}]}} } # configure where the read capability will be deployed config[{\"capability\": \"read\", \"decision\": decision}] { input.request.usage == \"read\" policy := {\"ID\": \"read-location\", \"description\":\"Deploy read in the workload cluster\", \"version\": \"0.1\"} cluster_restrict := {\"property\": \"name\", \"values\": [ input.workload.cluster.name ] } decision := {\"policy\": policy, \"restrictions\": {\"clusters\": [cluster_restrict]}} } # allow implicit copies by default config[{\"capability\": \"copy\", \"decision\": decision}] { input.request.usage == \"read\" policy := {\"ID\": \"copy-default\", \"description\":\"Implicit copies are allowed in read scenarios\", \"version\": \"0.1\"} decision := {\"policy\": policy} } How to provide custom policies In order to deploy Fybrik with customized policies, perform the following steps: Clone the github repository of Fybrik for the required release: git clone -b releases/<version> https://github.com/fybrik/fybrik.git Copy the rego files containing customized policies to fybrik/charts/fybrik/files/adminconfig/ folder. Install Fybrik: cd fybrik helm install fybrik-crd charts/fybrik-crd -n fybrik-system --wait helm install fybrik charts/fybrik --set global.tag=master --set global.imagePullPolicy=Always -n fybrik-system --wait How to add start and/or expiry dates to policies By utilizing the time built-in functions of OPA, an effective date and/or expiry date of a policy can be defined. The related built-in functions are: output := time.now_ns() //the current date output := time.parse_rfc3339_ns(value) //the specified date in RFC3339 format parse_rfc3339_ns enables to add the expiry date as well as the the date for the policy to become effective, and now_ns captures the date when policies are applied. Through comparisons, it can be acquired whether the current policy is still valid. Below is an example. package adminconfig # vaild from 2022.1.1, expire on 2022.6.1 config[{\"capability\": \"copy\", \"decision\": decision}] { policy := {\"ID\": \"test-1\", \"description\": \"forbid making copies\", \"version\": \"0.1\"} nowDate := time.now_ns() startDate := time.parse_rfc3339_ns(\"2022-01-01T00:00:00Z\") expiration := time.parse_rfc3339_ns(\"2022-06-01T00:00:00Z\") nowDate >= startDate nowDate < expiration decision := {\"policy\": policy, \"deploy\": \"False\"} } Note that an empty ConfigDecisions map will be returned if the expiration date is exceeded by the time when the policy is applied. How to update policies after Fybrik is already deployed Updating policies is done by updating fybrik-adminconfig config map in the controller plane. To do that, first, download all files to some directory, e.g. /tmp/adminconfig, after that update the files, and finally, upload them to the config map. The steps below demonstrate how to add a new rego file samples/adminconfig/quickstart-policies.rego . #!/bin/bash kubectl get cm fybrik-adminconfig -o json > tmp.json mkdir -p /tmp/adminconfig files=$(cat tmp.json | jq '.data' | jq -r 'keys[]') for k in $files; do name=\".data[\\\"$k\\\"]\"; cat tmp.json | jq -r $name > /tmp/adminconfig/$k; done cp samples/adminconfig/quickstart_policies.rego /tmp/adminconfig/ kubectl create configmap fybrik-adminconfig --from-file=/tmp/adminconfig -o yaml --dry-run=client | kubectl replace -n fybrik-system -f - rm -rf /tmp/adminconfig rm -rf tmp.json Optimization goals In a typical Fybrik deployment there may be several possibilities to create a data plane that satisfies the user requirements, governance and configuration policies. Based on the enterprise policy, an IT administrator may affect the choice of the data plane by defining a policy with optimization goals. An optimization goal attempts to minimize or maximize a specific infrastructure attribute . While IT config policies are always enforced, the data plane optimization is disabled by default. To enable data-plane optimization, the Optimizer component must be enabled as explained here . Syntax Optimization rules are written in rego files in a package adminconfig . Rules are written in the following syntax: optimize[decision] where decision is a JSON structure with the following fields: policy - policy metadata: unique ID, human-readable description and a version. list of goals including attribute name, optimization directive( min or max ) and optionally a weight. For example, the following rule attempts to minimize storage cost in copy scenarios. # minimize storage cost for copy scenarios optimize[decision] { input.request.usage == \"copy\" policy := {\"ID\": \"save-cost\", \"description\":\"Save storage costs\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"strategy\": [{\"attribute\": \"storage-cost\", \"directive\": \"min\"}]} } Weights If more than one goal is provided, they can have a different weight. By default, all weights are equal to 1. For example, the rule below defines two goals with weights in the 4:1 ratio meaning that the optimizer try to optimize distance and storage costs, but will give a higher priority to distance. # minimize distance, minimize storage cost for read scenarios optimize[decision] { input.request.usage == \"read\" policy := {\"ID\": \"general-strategy\", \"description\":\"focus on higher performance while saving storage costs\", \"version\": \"0.1\"} optimize_distance := {\"attribute\": \"distance\", \"directive\": \"min\", \"weight\": \"0.8\"} optimize_storage := {\"attribute\": \"storage-cost\", \"directive\": \"min\", \"weight\": \"0.2\"} decision := {\"policy\": policy, \"strategy\": [optimize_distance,optimize_storage]} }","title":"IT Config Policies and Data Plane Optimization"},{"location":"concepts/config-policies/#it-config-policies-and-data-plane-optimization","text":"","title":"IT Config Policies and Data Plane Optimization"},{"location":"concepts/config-policies/#what-are-it-config-policies","text":"IT config policies are the mechanism via which the organization may influence the construction of the data plane, taking into account infrastructure capabilities and costs. Fybrik takes into account the workload context, the data metadata, the data governance policies and the configuration policies when defining the data plane. IT config policies influence what capabilities should be deployed (e.g. read, copy), in which clusters they should be deployed, and the selection of the most appropriate module that implements the capability.","title":"What are IT config policies?"},{"location":"concepts/config-policies/#input-to-policies","text":"The input object includes general application data such as workload cluster and application properties, as well as dataset details (user requirements, metadata). Available properties: cluster.name : name of the workload cluster cluster.metadata.region : region of the workload cluster properties : application/workload properties defined in FybrikApplication, e.g. properties.intent request.metadata : asset metadata as defined in catalog taxonomy, e.g request.metadata.geography usage : a set of boolean properties associated with data use: usage.read , usage.write , usage.copy","title":"Input to policies"},{"location":"concepts/config-policies/#syntax","text":"Policies are written in rego files. Each file declares a package adminconfig . Rules are written in the following syntax: config[{\"capability\": capability, \"decision\": decision}] where - capability represents a required module capability, such as \"read\", \"write\", \"transform\" and \"copy\". decision is a JSON structure that matches Decision defined above. { \"policy\": {\"ID\": <id>, \"description\": <description>, \"version\": <version>}, \"deploy\": <\"True\", \"False\">, \"restrictions\": { \"modules\": <list of restrictions>, \"clusters\": <list of restrictions>, \"storageaccounts\": <list of restrictions>, }, } restriction restricts a property to either a set of values or a value in a given range . For example, the policy above restricts the choice of clusters and modules for a read capability by narrowing the choice of deployment clusters to the workload cluster, and restricting the module type to service. config[{\"capability\": \"read\", \"decision\": decision}] { input.request.usage == \"read\" policy := {\"ID\": \"read-location\", \"description\":\"Deploy read in the workload cluster\", \"version\": \"0.1\"} cluster_restrict := {\"property\": \"name\", \"values\": [ input.workload.cluster.name ] } module_restrict := {\"property\": \"type\", \"values\": [\"service\"]} decision := {\"policy\": policy, \"restrictions\": {\"clusters\": [cluster_restrict], \"modules\": [module_restrict]}} } policy provides policy metadata: unique ID, human-readable description and version restrictions provides restrictions for modules , clusters and storageaccounts . Each restriction provides a list or a range of allowed values for a property of module/cluster/storageaccount object. For example, to restrict a module type to either \"service\" or \"plugin\", we'll use \"type\" as a property, and [ \"service\",\"plugin ] as a list of allowed values. Properties of a module can be found inside FybrikModule Spec. Properties of a storage account are listed inside FybrikStorageAccount . Cluster is not a custom resource. It has the following properties: name: cluster name metadata.region: cluster region metadata.zone: cluster zone deploy receives \"True\"/\"False\" values. These values indicate whether the capability should or should not be deployed. If not specified in the policy, it's up to Fybrik to decide on the capability deployment.","title":"Syntax"},{"location":"concepts/config-policies/#out-of-the-box-policies","text":"Out of the box policies come with the fybrik deployment. They define the deployment of basic capabilities, such as read, write, copy and delete. package adminconfig # read capability deployment config[{\"capability\": \"read\", \"decision\": decision}] { input.request.usage == \"read\" policy := {\"ID\": \"read-default-enabled\", \"description\":\"Read capability is requested for read workloads\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"deploy\": \"True\"} } # write capability deployment config[{\"capability\": \"write\", \"decision\": decision}] { input.request.usage == \"write\" policy := {\"ID\": \"write-default-enabled\", \"description\":\"Write capability is requested for workloads that write data\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"deploy\": \"True\"} } # copy requested by the user config[{\"capability\": \"copy\", \"decision\": decision}] { input.request.usage == \"copy\" policy := {\"ID\": \"copy-request\", \"description\":\"Copy (ingest) capability is requested by the user\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"deploy\": \"True\"} } # delete capability deployment config[{\"capability\": \"delete\", \"decision\": decision}] { input.request.usage == \"delete\" policy := {\"ID\": \"delete-request\", \"description\":\"Delete capability is requested by the user\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"deploy\": \"True\"} } # do not deploy copy in scenarios different from read or copy config[{\"capability\": \"copy\", \"decision\": decision}] { input.request.usage != \"read\" input.request.usage != \"copy\" policy := {\"ID\": \"copy-disabled\", \"description\":\"Copy capability is not requested\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"deploy\": \"False\"} } # do not deploy read in other scenarios config[{\"capability\": \"read\", \"decision\": decision}] { input.request.usage != \"read\" policy := {\"ID\": \"read-disabled\", \"description\":\"Read capability is not requested\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"deploy\": \"False\"} } # do not deploy write in other scenarios config[{\"capability\": \"write\", \"decision\": decision}] { input.request.usage != \"write\" policy := {\"ID\": \"write-disabled\", \"description\":\"Write capability is not requested\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"deploy\": \"False\"} } # do not deploy delete in other scenarios config[{\"capability\": \"delete\", \"decision\": decision}] { input.request.usage != \"delete\" policy := {\"ID\": \"delete-disabled\", \"description\":\"Delete capability is not requested\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"deploy\": \"False\"} }","title":"Out of the box policies"},{"location":"concepts/config-policies/#extended-policies","text":"The extended policies define advanced deployment requirements, such as where read or transform modules should run, what should be the scope of module deployments, and more. The policies below are provided as a sample and can be updated for the production deployment. package adminconfig # configure where transformations take place config[{\"capability\": \"transform\", \"decision\": decision}] { policy := {\"ID\": \"transform-geo\", \"description\":\"Governance based transformations must take place in the geography where the data is stored\", \"version\": \"0.1\"} cluster_restrict := {\"property\": \"metadata.region\", \"values\": [input.request.dataset.geography]} decision := {\"policy\": policy, \"restrictions\": {\"clusters\": [cluster_restrict]}} } # configure the scope of the read capability config[{\"capability\": \"read\", \"decision\": decision}] { input.request.usage == \"read\" policy := {\"ID\": \"read-scope\", \"description\":\"Deploy read at the workload scope\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"restrictions\": {\"modules\": [{\"property\": \"capabilities.scope\", \"values\" : [\"workload\"]}]}} } # configure where the read capability will be deployed config[{\"capability\": \"read\", \"decision\": decision}] { input.request.usage == \"read\" policy := {\"ID\": \"read-location\", \"description\":\"Deploy read in the workload cluster\", \"version\": \"0.1\"} cluster_restrict := {\"property\": \"name\", \"values\": [ input.workload.cluster.name ] } decision := {\"policy\": policy, \"restrictions\": {\"clusters\": [cluster_restrict]}} } # allow implicit copies by default config[{\"capability\": \"copy\", \"decision\": decision}] { input.request.usage == \"read\" policy := {\"ID\": \"copy-default\", \"description\":\"Implicit copies are allowed in read scenarios\", \"version\": \"0.1\"} decision := {\"policy\": policy} }","title":"Extended policies"},{"location":"concepts/config-policies/#how-to-provide-custom-policies","text":"In order to deploy Fybrik with customized policies, perform the following steps: Clone the github repository of Fybrik for the required release: git clone -b releases/<version> https://github.com/fybrik/fybrik.git Copy the rego files containing customized policies to fybrik/charts/fybrik/files/adminconfig/ folder. Install Fybrik: cd fybrik helm install fybrik-crd charts/fybrik-crd -n fybrik-system --wait helm install fybrik charts/fybrik --set global.tag=master --set global.imagePullPolicy=Always -n fybrik-system --wait","title":"How to provide custom policies"},{"location":"concepts/config-policies/#how-to-add-start-andor-expiry-dates-to-policies","text":"By utilizing the time built-in functions of OPA, an effective date and/or expiry date of a policy can be defined. The related built-in functions are: output := time.now_ns() //the current date output := time.parse_rfc3339_ns(value) //the specified date in RFC3339 format parse_rfc3339_ns enables to add the expiry date as well as the the date for the policy to become effective, and now_ns captures the date when policies are applied. Through comparisons, it can be acquired whether the current policy is still valid. Below is an example. package adminconfig # vaild from 2022.1.1, expire on 2022.6.1 config[{\"capability\": \"copy\", \"decision\": decision}] { policy := {\"ID\": \"test-1\", \"description\": \"forbid making copies\", \"version\": \"0.1\"} nowDate := time.now_ns() startDate := time.parse_rfc3339_ns(\"2022-01-01T00:00:00Z\") expiration := time.parse_rfc3339_ns(\"2022-06-01T00:00:00Z\") nowDate >= startDate nowDate < expiration decision := {\"policy\": policy, \"deploy\": \"False\"} } Note that an empty ConfigDecisions map will be returned if the expiration date is exceeded by the time when the policy is applied.","title":"How to add start and/or expiry dates to policies"},{"location":"concepts/config-policies/#how-to-update-policies-after-fybrik-is-already-deployed","text":"Updating policies is done by updating fybrik-adminconfig config map in the controller plane. To do that, first, download all files to some directory, e.g. /tmp/adminconfig, after that update the files, and finally, upload them to the config map. The steps below demonstrate how to add a new rego file samples/adminconfig/quickstart-policies.rego . #!/bin/bash kubectl get cm fybrik-adminconfig -o json > tmp.json mkdir -p /tmp/adminconfig files=$(cat tmp.json | jq '.data' | jq -r 'keys[]') for k in $files; do name=\".data[\\\"$k\\\"]\"; cat tmp.json | jq -r $name > /tmp/adminconfig/$k; done cp samples/adminconfig/quickstart_policies.rego /tmp/adminconfig/ kubectl create configmap fybrik-adminconfig --from-file=/tmp/adminconfig -o yaml --dry-run=client | kubectl replace -n fybrik-system -f - rm -rf /tmp/adminconfig rm -rf tmp.json","title":"How to update policies after Fybrik is already deployed"},{"location":"concepts/config-policies/#optimization-goals","text":"In a typical Fybrik deployment there may be several possibilities to create a data plane that satisfies the user requirements, governance and configuration policies. Based on the enterprise policy, an IT administrator may affect the choice of the data plane by defining a policy with optimization goals. An optimization goal attempts to minimize or maximize a specific infrastructure attribute . While IT config policies are always enforced, the data plane optimization is disabled by default. To enable data-plane optimization, the Optimizer component must be enabled as explained here .","title":"Optimization goals"},{"location":"concepts/config-policies/#syntax_1","text":"Optimization rules are written in rego files in a package adminconfig . Rules are written in the following syntax: optimize[decision] where decision is a JSON structure with the following fields: policy - policy metadata: unique ID, human-readable description and a version. list of goals including attribute name, optimization directive( min or max ) and optionally a weight. For example, the following rule attempts to minimize storage cost in copy scenarios. # minimize storage cost for copy scenarios optimize[decision] { input.request.usage == \"copy\" policy := {\"ID\": \"save-cost\", \"description\":\"Save storage costs\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"strategy\": [{\"attribute\": \"storage-cost\", \"directive\": \"min\"}]} }","title":"Syntax"},{"location":"concepts/config-policies/#weights","text":"If more than one goal is provided, they can have a different weight. By default, all weights are equal to 1. For example, the rule below defines two goals with weights in the 4:1 ratio meaning that the optimizer try to optimize distance and storage costs, but will give a higher priority to distance. # minimize distance, minimize storage cost for read scenarios optimize[decision] { input.request.usage == \"read\" policy := {\"ID\": \"general-strategy\", \"description\":\"focus on higher performance while saving storage costs\", \"version\": \"0.1\"} optimize_distance := {\"attribute\": \"distance\", \"directive\": \"min\", \"weight\": \"0.8\"} optimize_storage := {\"attribute\": \"storage-cost\", \"directive\": \"min\", \"weight\": \"0.2\"} decision := {\"policy\": policy, \"strategy\": [optimize_distance,optimize_storage]} }","title":"Weights"},{"location":"concepts/connectors/","text":"Connectors The project currently has two extension mechanisms, namely connectors and modules. This page describes what connectors are and how connectors are installed using the default Fybrik installation. What are connectors? Connectors are Open API services that the Fybrik control plane uses to connect to external systems. Specifically, the control plane needs connectors to a data catalog and a data governance policy manager. These connector services are deployed alongside the control plane. Can I write my own connectors? Yes. Fybrik provides some default connectors described in this page but anyone can develop their own connectors. A connector needs to implement one or more of the interfaces described in the API documentation , depending on the connector type. Note that a single Kubernetes service can implement all interfaces if the system it connects to supports the required functionality, but it can also be different services. In addition, to benefit from the Ingress traffic policy feature mentioned in control plane security section ensure that the Pods of your connector have a fybrik.io/componentType: connector label. For TLS configuration please see the above link for details on how fybrik uses TLS. Connector types Data catalog Fybrik assumes the use of an enterprise data catalog. For example, to reference a required data asset in a FybrikApplication resource, you provide a link to the asset in the catalog. The catalog provides metadata about the asset such as security tags. It also provides connection information to describe how to connect to the data source to consume the data. Fybrik uses the metadata provided by the catalog both to enable seamless connectivity to the data and as input to making data governance policy decisions. The data user is not concerned with any of it and just selects the data that it needs regardless of where the data resides. Fybrik is not a data catalog. Instead, it links to existing data catalogs using connectors. Fybrik supports OpenMetadata through the openmetadata-connector . A connector to ODPi Egeria is also available. There is also Katalog , a data catalog stub for testing and evaluation purposes, which uses Kubernetes custom resources. Credential management The connector might need to read credentials stored in HashiCorp Vault. The parameters to login to vault and to read secret are as follows: address: Vault address authPath: Path to kubernetes auth method used to login to Vault role: connector role used to login to Vault - configured to be \"fybrik\" secretPath: Path of the secret holding the credentials in Vault The parameters should be known to the connector upon startup time, except for the vault secret path ( SecretPath ) which is passed as a parameter in each call to the connector usually under Credentials name. An example for Vault Login API call which uses the Vault parameters is as follows: $ curl -v -X POST <address>/<authPath> -H \"Content-Type: application/json\" --data '{\"jwt\": <connector service account token>, \"role\": <role>}' An example for Vault Read Secret API call which uses the Vault parameters is as follows: $ curl --header \"X-Vault-Token: ...\" -X GET https://<address>/<secretPath> More about kubenertes auth method and vault roles can be found in Vault documentation . Policy manager Data governance policies are defined externally in the data governance manager of choice. Enforcing data governance policies requires a Policy Decision Point (PDP) that dictates what enforcement actions need to take place. Fybrik supports a wide and extendable set of enforcement actions to perform on data read, copy, (future) write or delete. These include transformation of data, verification of the data, and various restrictions on the external activity of an application that can access the data. A PDP returns a list of enforcement actions given a set of policies and specific context about the application and the data it uses. Fybrik includes a PDP that is powered by Open Policy Agent (OPA). However, the PDP can also use external policy managers via connectors, to cover some or even all policy types.","title":"Connectors"},{"location":"concepts/connectors/#connectors","text":"The project currently has two extension mechanisms, namely connectors and modules. This page describes what connectors are and how connectors are installed using the default Fybrik installation.","title":"Connectors"},{"location":"concepts/connectors/#what-are-connectors","text":"Connectors are Open API services that the Fybrik control plane uses to connect to external systems. Specifically, the control plane needs connectors to a data catalog and a data governance policy manager. These connector services are deployed alongside the control plane.","title":"What are connectors?"},{"location":"concepts/connectors/#can-i-write-my-own-connectors","text":"Yes. Fybrik provides some default connectors described in this page but anyone can develop their own connectors. A connector needs to implement one or more of the interfaces described in the API documentation , depending on the connector type. Note that a single Kubernetes service can implement all interfaces if the system it connects to supports the required functionality, but it can also be different services. In addition, to benefit from the Ingress traffic policy feature mentioned in control plane security section ensure that the Pods of your connector have a fybrik.io/componentType: connector label. For TLS configuration please see the above link for details on how fybrik uses TLS.","title":"Can I write my own connectors?"},{"location":"concepts/connectors/#connector-types","text":"","title":"Connector types"},{"location":"concepts/connectors/#data-catalog","text":"Fybrik assumes the use of an enterprise data catalog. For example, to reference a required data asset in a FybrikApplication resource, you provide a link to the asset in the catalog. The catalog provides metadata about the asset such as security tags. It also provides connection information to describe how to connect to the data source to consume the data. Fybrik uses the metadata provided by the catalog both to enable seamless connectivity to the data and as input to making data governance policy decisions. The data user is not concerned with any of it and just selects the data that it needs regardless of where the data resides. Fybrik is not a data catalog. Instead, it links to existing data catalogs using connectors. Fybrik supports OpenMetadata through the openmetadata-connector . A connector to ODPi Egeria is also available. There is also Katalog , a data catalog stub for testing and evaluation purposes, which uses Kubernetes custom resources.","title":"Data catalog"},{"location":"concepts/connectors/#credential-management","text":"The connector might need to read credentials stored in HashiCorp Vault. The parameters to login to vault and to read secret are as follows: address: Vault address authPath: Path to kubernetes auth method used to login to Vault role: connector role used to login to Vault - configured to be \"fybrik\" secretPath: Path of the secret holding the credentials in Vault The parameters should be known to the connector upon startup time, except for the vault secret path ( SecretPath ) which is passed as a parameter in each call to the connector usually under Credentials name. An example for Vault Login API call which uses the Vault parameters is as follows: $ curl -v -X POST <address>/<authPath> -H \"Content-Type: application/json\" --data '{\"jwt\": <connector service account token>, \"role\": <role>}' An example for Vault Read Secret API call which uses the Vault parameters is as follows: $ curl --header \"X-Vault-Token: ...\" -X GET https://<address>/<secretPath> More about kubenertes auth method and vault roles can be found in Vault documentation .","title":"Credential management"},{"location":"concepts/connectors/#policy-manager","text":"Data governance policies are defined externally in the data governance manager of choice. Enforcing data governance policies requires a Policy Decision Point (PDP) that dictates what enforcement actions need to take place. Fybrik supports a wide and extendable set of enforcement actions to perform on data read, copy, (future) write or delete. These include transformation of data, verification of the data, and various restrictions on the external activity of an application that can access the data. A PDP returns a list of enforcement actions given a set of policies and specific context about the application and the data it uses. Fybrik includes a PDP that is powered by Open Policy Agent (OPA). However, the PDP can also use external policy managers via connectors, to cover some or even all policy types.","title":"Policy manager"},{"location":"concepts/introduction/","text":"Introduction Fybrik is a cloud native platform to unify data access and governance, enabling business agility while securing enterprise data. By providing access and use of data only via the platform, Fybrik brings together access and governance for data, greatly reducing risk of data loss. Fybrik allows: Data users to use data in a self-service model without manual processes. Fybrik eliminates the need of a data user to confer with data stewards, and to deal with credentials. The data user can use common tools and frameworks for reading from and exporting data to data lakes or data warehouses. Data stewards to control data usage by applications. The data steward can use the organization's policy manager and data catalog of choice and let Fybrik automatically enforce data governance policies, whether they be based on laws, industry standards or enterprise policies. Data operators to automate data lifecycle management. Fybrik eliminates the need for manual processes and custom jobs created by data operators. Instead, Fybrik provids data operators with config policies to optimize the data flows orchestrated by fybrik. How does it work? The inputs to Fybrik are declarative definitions with separation of aspects: Data stewards input definitions related to data governance and security. Data users input definitions related to data usage in the business logic of their applications. Data operators input definitions related to infrastructure and available resources. Upon creation or change of any definition, Fybrik compiles together relevant inputs into a plotter describing the flow of data between the application and the data sources/destinations (data plane). The plotter augments the application workload and data sources with additional services and functions packed as pluggable modules. This creates a data path that: Integrates business logic with non-functional data centric requirements such as enabling data access regardless of its physical location, caching, lineage tracking, etc. Enforces governance relating to the data and its lifecycle; including limiting what data the business logic can access, performing transformations as needed, controlling what the business logic can export and to where. Makes data available in locations where it is needed. Thus, in a multi cluster scenario it may copy data from one location to another, something known as an implicit copy. The implicit copy is deleted when no longer needed. Modularity Fybrik is an open solution that can be extended to work with a wide range of tools and data stores. For example, the injectable modules and the connectors to external systems (e.g., to a data catalog) can all be third party. The logic used by fybrik to generate the data planes is customizable. An organization can determine how best its infrastructure should be leveraged via config policies . Applications Fybrik considers applications as first level entities. Before running a workload, an application needs to be registered to a Fybrik control plane by applying a FybrikApplication resource. This is the declarative definition provided by the data user. The registration provides context about the application such as the purpose for which it's running, the data assets that it accesses, and a selector to identify the workload. Additional context such as geo-location is extracted from the platform. The actions taken by Fybrik are based on policies and the context of the application. Specifically, Fybrik does not consider end-users of an application. It is the responsibility of the application to implement mechanisms such as end user authentication if required, e.g. using Istio authorization with JWT . There are specific situations in which there is no workload associated with a FybrikApplication resource. Examples of these are requests to ingest data into a governed environment, or (future) requests to clean up data in the governed environment based on data governance policies. Security While Fybrik handles enforcement of data governance policies, if one could access the data not through the platform then we lose control over data usage. For this reason, Fybrik does not let user applications ever observe data access credentials, neither for externally created data assets nor for data assets created by the Fybrik control plane and applications running in it. Instead, modules run in the data path to handle access to data, including passing the data access credentials to upstream data stores. Security is preserved by authorizing the applications based on their Pod identities. Multicluster Fybrik supports data paths that access data stores that are external to the cluster such as cloud managed object stores or databases as well as data stores within the cluster such as databases running in Kubernetes. All applications and modules however will run within a cluster that has Fybrik installed. Multi-cloud and hybrid cloud scenarios are supported out of the box by running Fybrik in multiple Kubernetes clusters and configuring the manager to use a multi-cluster coordination mechanism such as Razee. This enables cases such as running, for example, transformations on-prem while creating an implicit copy of an on-prem SoR table to a public cloud storage system.","title":"Introduction"},{"location":"concepts/introduction/#introduction","text":"Fybrik is a cloud native platform to unify data access and governance, enabling business agility while securing enterprise data. By providing access and use of data only via the platform, Fybrik brings together access and governance for data, greatly reducing risk of data loss. Fybrik allows: Data users to use data in a self-service model without manual processes. Fybrik eliminates the need of a data user to confer with data stewards, and to deal with credentials. The data user can use common tools and frameworks for reading from and exporting data to data lakes or data warehouses. Data stewards to control data usage by applications. The data steward can use the organization's policy manager and data catalog of choice and let Fybrik automatically enforce data governance policies, whether they be based on laws, industry standards or enterprise policies. Data operators to automate data lifecycle management. Fybrik eliminates the need for manual processes and custom jobs created by data operators. Instead, Fybrik provids data operators with config policies to optimize the data flows orchestrated by fybrik.","title":"Introduction"},{"location":"concepts/introduction/#how-does-it-work","text":"The inputs to Fybrik are declarative definitions with separation of aspects: Data stewards input definitions related to data governance and security. Data users input definitions related to data usage in the business logic of their applications. Data operators input definitions related to infrastructure and available resources. Upon creation or change of any definition, Fybrik compiles together relevant inputs into a plotter describing the flow of data between the application and the data sources/destinations (data plane). The plotter augments the application workload and data sources with additional services and functions packed as pluggable modules. This creates a data path that: Integrates business logic with non-functional data centric requirements such as enabling data access regardless of its physical location, caching, lineage tracking, etc. Enforces governance relating to the data and its lifecycle; including limiting what data the business logic can access, performing transformations as needed, controlling what the business logic can export and to where. Makes data available in locations where it is needed. Thus, in a multi cluster scenario it may copy data from one location to another, something known as an implicit copy. The implicit copy is deleted when no longer needed.","title":"How does it work?"},{"location":"concepts/introduction/#modularity","text":"Fybrik is an open solution that can be extended to work with a wide range of tools and data stores. For example, the injectable modules and the connectors to external systems (e.g., to a data catalog) can all be third party. The logic used by fybrik to generate the data planes is customizable. An organization can determine how best its infrastructure should be leveraged via config policies .","title":"Modularity"},{"location":"concepts/introduction/#applications","text":"Fybrik considers applications as first level entities. Before running a workload, an application needs to be registered to a Fybrik control plane by applying a FybrikApplication resource. This is the declarative definition provided by the data user. The registration provides context about the application such as the purpose for which it's running, the data assets that it accesses, and a selector to identify the workload. Additional context such as geo-location is extracted from the platform. The actions taken by Fybrik are based on policies and the context of the application. Specifically, Fybrik does not consider end-users of an application. It is the responsibility of the application to implement mechanisms such as end user authentication if required, e.g. using Istio authorization with JWT . There are specific situations in which there is no workload associated with a FybrikApplication resource. Examples of these are requests to ingest data into a governed environment, or (future) requests to clean up data in the governed environment based on data governance policies.","title":"Applications"},{"location":"concepts/introduction/#security","text":"While Fybrik handles enforcement of data governance policies, if one could access the data not through the platform then we lose control over data usage. For this reason, Fybrik does not let user applications ever observe data access credentials, neither for externally created data assets nor for data assets created by the Fybrik control plane and applications running in it. Instead, modules run in the data path to handle access to data, including passing the data access credentials to upstream data stores. Security is preserved by authorizing the applications based on their Pod identities.","title":"Security"},{"location":"concepts/introduction/#multicluster","text":"Fybrik supports data paths that access data stores that are external to the cluster such as cloud managed object stores or databases as well as data stores within the cluster such as databases running in Kubernetes. All applications and modules however will run within a cluster that has Fybrik installed. Multi-cloud and hybrid cloud scenarios are supported out of the box by running Fybrik in multiple Kubernetes clusters and configuring the manager to use a multi-cluster coordination mechanism such as Razee. This enables cases such as running, for example, transformations on-prem while creating an implicit copy of an on-prem SoR table to a public cloud storage system.","title":"Multicluster"},{"location":"concepts/modules/","text":"Modules The project currently has two extension mechanisms, namely connectors and modules. This page describes what modules are and how they are leveraged by the Fybrik control plane to build the data plane flow. What are modules? As described in the architecture page, the control plane generates a description of a data plane based on policies and application requirements. This is known as a blueprint, and includes components that are deployed by the control plane to fulfill different data-centric requirements. For example, a component that can mask data can be used to enforce a data masking policy, or a component that copies data may be used to create a local data copy to meet performance requirements, etc. Modules are the way to describe such data plane components and make them available to the control plane. A module is packaged as a Helm chart that the control plane can install to a workload's data plane. To make a module available to the control plane it must be registered by applying a FybrikModule custom resource. The functionality described by the module may be deployed (a) per workload, or (b) it may be composed of one or more components that run independent of the workload and its associated control plane. In the case of (a), the control plane handles the deployment of the functional component. In the case of (b) where the functionality of the module runs independently and handles requests from multiple workloads, a client module is what is deployed by the control plane. This client module passes parameters to the external component(s) and monitors the status and results of the requests to the external component(s). The following diagram shows an example with an Arrow Flight module that is fully deployed by the control plane and a second module where the client is deployed by the control plane but the ETL component providing the functionality has been independently deployed and supports multiple workloads. Components that make up a module There are several parts to a module: Optional external component(s): deployed and managed independently of Fybrik. Module Workload : the workload that runs once the Helm chart is installed by the control plane. Can be a client to the external component(s) or be independent. Module Helm Chart : the package containing the module workload that the control plane installs as part of a data plane. FybrikModule YAML : describes the functional capabilities, supported interfaces, and has links to the Module Helm chart. Registering a module To make the control plane aware of the module so that it can be included in appropriate workload data flows, the administrator must apply the FybrikModule YAML in the fybrik-system namespace. This makes the control plane aware of the existence of the module. Note that it does not check that the module's helm chart exists. For example, the following registers the arrow-flight-module : kubectl apply -f https://raw.githubusercontent.com/fybrik/arrow-flight-module/master/module.yaml -n fybrik-system When is a module used? There are four main data flows in which modules may be used: Read - preparing data to be read and/or actually reading the data. Write - writing a new data set or appending data to an existing data set. Copy - for performing an implicit data copy on behalf of the application. The decision to do an implicit copy is made by the control plane, typically for performance or governance reasons. Delete - for deleting objects or data sets. A module may be used in one or more of these flows, as is indicated in the module's yaml file. Control plane choice of modules A user workload description FybrikApplicaton includes a list of the data sets required, the technologies that will be used to access them, the access type (e.g. read, copy), information about the location and reason for the use of the data. This information together with input from data and enterprise policies , determine which modules are chosen by the control plane and where they are deployed. Available modules The table below lists the currently available modules: Name Description FybrikModule Prerequisite arrow-flight-module reading and writing datasets while performing data transformations using Arrow Flight https://raw.githubusercontent.com/fybrik/arrow-flight-module/master/module.yaml airbyte-module reading datasets from data sources supported by the Airbyte tool https://raw.githubusercontent.com/fybrik/airbyte-module/main/module.yaml delete-module deletes s3 objects https://raw.githubusercontent.com/fybrik/delete-module/main/module.yaml implicit-copy copies data between any two supported data stores, for example S3 and Kafka, and applies transformations. https://raw.githubusercontent.com/fybrik/data-movement-operator/master/modules/implicit-copy-batch-module.yaml https://raw.githubusercontent.com/fybrik/data-movement-operator/master/modules/implicit-copy-stream-module.yaml FybrikStorageAccount resource deployed in the control plane namespace to hold the details of the storage which is used by the module for coping the data dremio-module configures a Dremio cluster for data access in Fybrik https://raw.githubusercontent.com/fybrik/dremio-module/main/dremio-module.yaml trino-module configures a Trino cluster for data access in Fybrik https://raw.githubusercontent.com/fybrik/trino-module/main/trino-module.yaml REST-read-example-module an example of Fybrik read module that uses REST protocol to connect to a FHIR server to obtain medical records https://raw.githubusercontent.com/fybrik/REST-read-example/main/restFHIRmodule.yaml hello-world-read-module an example of a read module for Fybrik https://raw.githubusercontent.com/fybrik/hello-world-read-module/main/hello-world-read-module.yaml template-module a template github repository that helps develop a Fybrik module faster and easier Contributing Read Module Development for details on the components that make up a module and how to create a module.","title":"Modules"},{"location":"concepts/modules/#modules","text":"The project currently has two extension mechanisms, namely connectors and modules. This page describes what modules are and how they are leveraged by the Fybrik control plane to build the data plane flow.","title":"Modules"},{"location":"concepts/modules/#what-are-modules","text":"As described in the architecture page, the control plane generates a description of a data plane based on policies and application requirements. This is known as a blueprint, and includes components that are deployed by the control plane to fulfill different data-centric requirements. For example, a component that can mask data can be used to enforce a data masking policy, or a component that copies data may be used to create a local data copy to meet performance requirements, etc. Modules are the way to describe such data plane components and make them available to the control plane. A module is packaged as a Helm chart that the control plane can install to a workload's data plane. To make a module available to the control plane it must be registered by applying a FybrikModule custom resource. The functionality described by the module may be deployed (a) per workload, or (b) it may be composed of one or more components that run independent of the workload and its associated control plane. In the case of (a), the control plane handles the deployment of the functional component. In the case of (b) where the functionality of the module runs independently and handles requests from multiple workloads, a client module is what is deployed by the control plane. This client module passes parameters to the external component(s) and monitors the status and results of the requests to the external component(s). The following diagram shows an example with an Arrow Flight module that is fully deployed by the control plane and a second module where the client is deployed by the control plane but the ETL component providing the functionality has been independently deployed and supports multiple workloads.","title":"What are modules?"},{"location":"concepts/modules/#components-that-make-up-a-module","text":"There are several parts to a module: Optional external component(s): deployed and managed independently of Fybrik. Module Workload : the workload that runs once the Helm chart is installed by the control plane. Can be a client to the external component(s) or be independent. Module Helm Chart : the package containing the module workload that the control plane installs as part of a data plane. FybrikModule YAML : describes the functional capabilities, supported interfaces, and has links to the Module Helm chart.","title":"Components that make up a module"},{"location":"concepts/modules/#registering-a-module","text":"To make the control plane aware of the module so that it can be included in appropriate workload data flows, the administrator must apply the FybrikModule YAML in the fybrik-system namespace. This makes the control plane aware of the existence of the module. Note that it does not check that the module's helm chart exists. For example, the following registers the arrow-flight-module : kubectl apply -f https://raw.githubusercontent.com/fybrik/arrow-flight-module/master/module.yaml -n fybrik-system","title":"Registering a module"},{"location":"concepts/modules/#when-is-a-module-used","text":"There are four main data flows in which modules may be used: Read - preparing data to be read and/or actually reading the data. Write - writing a new data set or appending data to an existing data set. Copy - for performing an implicit data copy on behalf of the application. The decision to do an implicit copy is made by the control plane, typically for performance or governance reasons. Delete - for deleting objects or data sets. A module may be used in one or more of these flows, as is indicated in the module's yaml file.","title":"When is a module used?"},{"location":"concepts/modules/#control-plane-choice-of-modules","text":"A user workload description FybrikApplicaton includes a list of the data sets required, the technologies that will be used to access them, the access type (e.g. read, copy), information about the location and reason for the use of the data. This information together with input from data and enterprise policies , determine which modules are chosen by the control plane and where they are deployed.","title":"Control plane choice of modules"},{"location":"concepts/modules/#available-modules","text":"The table below lists the currently available modules: Name Description FybrikModule Prerequisite arrow-flight-module reading and writing datasets while performing data transformations using Arrow Flight https://raw.githubusercontent.com/fybrik/arrow-flight-module/master/module.yaml airbyte-module reading datasets from data sources supported by the Airbyte tool https://raw.githubusercontent.com/fybrik/airbyte-module/main/module.yaml delete-module deletes s3 objects https://raw.githubusercontent.com/fybrik/delete-module/main/module.yaml implicit-copy copies data between any two supported data stores, for example S3 and Kafka, and applies transformations. https://raw.githubusercontent.com/fybrik/data-movement-operator/master/modules/implicit-copy-batch-module.yaml https://raw.githubusercontent.com/fybrik/data-movement-operator/master/modules/implicit-copy-stream-module.yaml FybrikStorageAccount resource deployed in the control plane namespace to hold the details of the storage which is used by the module for coping the data dremio-module configures a Dremio cluster for data access in Fybrik https://raw.githubusercontent.com/fybrik/dremio-module/main/dremio-module.yaml trino-module configures a Trino cluster for data access in Fybrik https://raw.githubusercontent.com/fybrik/trino-module/main/trino-module.yaml REST-read-example-module an example of Fybrik read module that uses REST protocol to connect to a FHIR server to obtain medical records https://raw.githubusercontent.com/fybrik/REST-read-example/main/restFHIRmodule.yaml hello-world-read-module an example of a read module for Fybrik https://raw.githubusercontent.com/fybrik/hello-world-read-module/main/hello-world-read-module.yaml template-module a template github repository that helps develop a Fybrik module faster and easier","title":"Available modules"},{"location":"concepts/modules/#contributing","text":"Read Module Development for details on the components that make up a module and how to create a module.","title":"Contributing"},{"location":"concepts/optimizer/","text":"Optimizer The optimizer component builds an optimal data-plane plotter for a given FybrikApplication custom resource, taking into account: Available resources (modules, clusters, storage accounts) Application specification (e.g., geography, data access protocol) Specifications of the datasets required by the application Governance actions required by the data-governance policy manager IT Configuration policies , including optimization goals The optimizer translates all the above inputs into a monolith Constraint Satisfaction Problem (CSP) and solves it using a third-party CSP solver. The solver returns an optimal solution in terms of the specified optimization goals. The solution is then translated into a plotter. The plotter specifies which modules should be deployed in which clusters, using which storage accounts and which configuration. It also describes how data flows between the modules. Finally, the plotter is deployed to the specified clusters (via cluster-specific blueprints), resulting in a data plane that connects the required datasets to the application. Note: The optimizer component is currently disabled by default, meaning all optimization goals are being ignored. Enabling it is simple and is explained here . Also note that in the rare case of the CSP solver failing to produce any solution (which is not due to conflicting polices), Fybrik will fall back to producing a plotter without the CSP solver, but while ignoring all optimization goals. The Constraint Satisfaction Problem is written as a FlatZinc model . This allows using any CSP solver that supports the FlatZinc format. Currently, the default solver is the one provided by Google OR-Tools . Check this list for other solvers supporting FlatZinc. Configuring a solver different than the default solver is explained here .","title":"Optimizer"},{"location":"concepts/optimizer/#optimizer","text":"The optimizer component builds an optimal data-plane plotter for a given FybrikApplication custom resource, taking into account: Available resources (modules, clusters, storage accounts) Application specification (e.g., geography, data access protocol) Specifications of the datasets required by the application Governance actions required by the data-governance policy manager IT Configuration policies , including optimization goals The optimizer translates all the above inputs into a monolith Constraint Satisfaction Problem (CSP) and solves it using a third-party CSP solver. The solver returns an optimal solution in terms of the specified optimization goals. The solution is then translated into a plotter. The plotter specifies which modules should be deployed in which clusters, using which storage accounts and which configuration. It also describes how data flows between the modules. Finally, the plotter is deployed to the specified clusters (via cluster-specific blueprints), resulting in a data plane that connects the required datasets to the application. Note: The optimizer component is currently disabled by default, meaning all optimization goals are being ignored. Enabling it is simple and is explained here . Also note that in the rare case of the CSP solver failing to produce any solution (which is not due to conflicting polices), Fybrik will fall back to producing a plotter without the CSP solver, but while ignoring all optimization goals. The Constraint Satisfaction Problem is written as a FlatZinc model . This allows using any CSP solver that supports the FlatZinc format. Currently, the default solver is the one provided by Google OR-Tools . Check this list for other solvers supporting FlatZinc. Configuring a solver different than the default solver is explained here .","title":"Optimizer"},{"location":"concepts/storage_manager/","text":"Storage manager In several use-cases Fybrik needs to allocate storage for data. One use case is implicit copy of a dataset in read scenarios made for performance, cost or governance sake. A second scenario is when a new dataset is created by the workload. In this case Fybrik allocates the storage and registers the new dataset in the data catalog. A third use case is explicit copy - i.e. the user indicates that a copy of an existing dataset should be made. As in the second use case, here too Fybrik allocates storage for the data and registers the new dataset in the data catalog. When we say that Fybrik allocates storage, we actually mean that Fybrik allocates a portion of an existing storage account for use by the given dataset. Fybrik must be informed what storage accounts are available, and how to access them. This information is currently provided via the FybrikStorageAccount CRD. Storage manager is a Fybrik component responsible for allocating storage in known storage accounts and for freeing previously allocated storage. Upon a successful storage allocation, the component forms a connection object that is passed to modules that write data to this storage. Deployment Storage manager runs as a container in the manager pod. A default Fybrik deployment uses its open-source implementation as a docker image specified in Fybrik values.yaml as storageManager.image . Can I write my own storage manager implementation? Yes. Custom implementations are required to support the interface described in the Storage manager API documentation . Next, generate a custom docker image and replace the default image used by Fybrik, as described here . Storage account An instance of FybrikStorageAccount defines properties of the shared storage for a specific connection type. Example of a storage account for S3: spec: id: <storage account id> type: s3 secretRef: <name of the secret that holds credentials to the object store> geography: <storage location> s3: region: <region> endpoint: <endpoint> What storage types are supported? The current implementation supports S3 and MySQL storage. Storage allocation results in creating a new S3 bucket or MySQL database. When storage is de-allocated, the dataset is deleted, and the generated bucket/database is deleted. In the future, the deletion of a bucket/database will be controlled by IT configuration policies. In the future other storage types might be supported as well. We strongly encourage contributions to extend the supported types. How to support a new storage type? Development Add a new connection schema to the taxonomy . Support the new type according to Storage manager API documentation and create a new docker image. When adding a new type to the existing open-source implementation, a new package should be created in pkg/storage/impl and imported inside pkg/storage/handler.go . For example, after adding support for kafka in pkg/storage/impl/kafka : _ \"fybrik.io/fybrik/pkg/storage/impl/kafka\" Deployment Specify the storage manager image in Fybrik values.yaml as storageManager.image . Ensure existence of FybrikModule instances that are able to write/copy to this storage. Update the capabilities in module yamls accordingly. Example: supportedInterfaces: - source: protocol: <new storage type> Prepare FybrikStorageAccount resources with the shared storage information. Example: apiVersion: app.fybrik.io/v1beta2 kind: FybrikStorageAccount metadata: name: theshire-account spec: id: theshire-storage-account type: <new storage type> secretRef: theshire-credentials geography: theshire <new storage type>: <attribute1>: <value1> <attribute2>: <value2> Update infrastructure attributes related to the storage accounts, e.g., cost. Optionally update IT config policies to specify when the new storage can/should be selected Redeploy Fybrik with the changes.","title":"Storage manager"},{"location":"concepts/storage_manager/#storage-manager","text":"In several use-cases Fybrik needs to allocate storage for data. One use case is implicit copy of a dataset in read scenarios made for performance, cost or governance sake. A second scenario is when a new dataset is created by the workload. In this case Fybrik allocates the storage and registers the new dataset in the data catalog. A third use case is explicit copy - i.e. the user indicates that a copy of an existing dataset should be made. As in the second use case, here too Fybrik allocates storage for the data and registers the new dataset in the data catalog. When we say that Fybrik allocates storage, we actually mean that Fybrik allocates a portion of an existing storage account for use by the given dataset. Fybrik must be informed what storage accounts are available, and how to access them. This information is currently provided via the FybrikStorageAccount CRD. Storage manager is a Fybrik component responsible for allocating storage in known storage accounts and for freeing previously allocated storage. Upon a successful storage allocation, the component forms a connection object that is passed to modules that write data to this storage.","title":"Storage manager"},{"location":"concepts/storage_manager/#deployment","text":"Storage manager runs as a container in the manager pod. A default Fybrik deployment uses its open-source implementation as a docker image specified in Fybrik values.yaml as storageManager.image .","title":"Deployment"},{"location":"concepts/storage_manager/#can-i-write-my-own-storage-manager-implementation","text":"Yes. Custom implementations are required to support the interface described in the Storage manager API documentation . Next, generate a custom docker image and replace the default image used by Fybrik, as described here .","title":"Can I write my own storage manager implementation?"},{"location":"concepts/storage_manager/#storage-account","text":"An instance of FybrikStorageAccount defines properties of the shared storage for a specific connection type. Example of a storage account for S3: spec: id: <storage account id> type: s3 secretRef: <name of the secret that holds credentials to the object store> geography: <storage location> s3: region: <region> endpoint: <endpoint>","title":"Storage account"},{"location":"concepts/storage_manager/#what-storage-types-are-supported","text":"The current implementation supports S3 and MySQL storage. Storage allocation results in creating a new S3 bucket or MySQL database. When storage is de-allocated, the dataset is deleted, and the generated bucket/database is deleted. In the future, the deletion of a bucket/database will be controlled by IT configuration policies. In the future other storage types might be supported as well. We strongly encourage contributions to extend the supported types.","title":"What storage types are supported?"},{"location":"concepts/storage_manager/#how-to-support-a-new-storage-type","text":"","title":"How to support a new storage type?"},{"location":"concepts/storage_manager/#development","text":"Add a new connection schema to the taxonomy . Support the new type according to Storage manager API documentation and create a new docker image. When adding a new type to the existing open-source implementation, a new package should be created in pkg/storage/impl and imported inside pkg/storage/handler.go . For example, after adding support for kafka in pkg/storage/impl/kafka : _ \"fybrik.io/fybrik/pkg/storage/impl/kafka\"","title":"Development"},{"location":"concepts/storage_manager/#deployment_1","text":"Specify the storage manager image in Fybrik values.yaml as storageManager.image . Ensure existence of FybrikModule instances that are able to write/copy to this storage. Update the capabilities in module yamls accordingly. Example: supportedInterfaces: - source: protocol: <new storage type> Prepare FybrikStorageAccount resources with the shared storage information. Example: apiVersion: app.fybrik.io/v1beta2 kind: FybrikStorageAccount metadata: name: theshire-account spec: id: theshire-storage-account type: <new storage type> secretRef: theshire-credentials geography: theshire <new storage type>: <attribute1>: <value1> <attribute2>: <value2> Update infrastructure attributes related to the storage accounts, e.g., cost. Optionally update IT config policies to specify when the new storage can/should be selected Redeploy Fybrik with the changes.","title":"Deployment"},{"location":"concepts/taxonomy/","text":"Taxonomy A taxonomy defines the terms and related values that need to be commonly understood and supported across the components in the system. Fybrik interacts with multiple external components, such as the data catalog, data governance policy manager, and modules. In order for fybrik to orchestrate the data plane of a given workload it is essential that all the components involved use common terms. For example, if a data governance policy refers to a particular transform it is crucial that the module implementing that transform refer to it in the same way, or for fybrik to be able to map between the disparate terms. Some components that use taxonomy: FybrikApplication yaml - information provided about the workload and the datasets. Fybrik manager ( FybrikApplication controller ) - validates that the data is used in accord with the data governance policies and the IT config policies. Data catalog - provides metadata about the asset. Data Governance Policy Manager - defines the governance policies to follow. Config Policy Manager - defines the IT policies to follow. FybrikModules - describe capabilities that can be included in a data plane. Default taxonomies are provided by fybrik in a JSON file format, and are meant as a starting point. Different actors can expand these. Issues Addressed by Taxonomy The taxonomy addresses the following: Redundancy: No need for the same structures and values to be hardcoded in multiple places, such as in the fybrik manager and in the plugins. Validation: Validates structures and values passed between components. Dynamic Updates: New terms and new values can be added, removed and updated dynamically. For example, one can add new enforcement actions, new connection types, new purposes, etc without needing to redeploy fybrik. Taxonomy Contributors Different actors and components define the contents of different parts of the taxonomy. The following table describes the taxonomy and which component most logically owns each part of it. Taxonomy Contributing Component Actor Example Values catalog Data Catalog Data Steward data stores, formats, metadata application Policy Manager, Data Catalog Governance Officer roles, intents module Modules Module Developer capabilities, transforms If, for example, a Data Governance Officer writes a policy that limits the use of sensitive data for marketing, then the possible valid intents such as marketing would be defined by him in the Data Policy Manager. These values must be added to fybrik's taxonomy, either manually or via an automated feed, so that fybrik can validate the intent provided in a FybrikApplication yaml when a user's workload requests data. As new capabilities, transforms, data types, and protocols are made available via FybrikModules, fybrik's module taxonomy must be updated. Once updated these capabilities are available for use by other components, such as the Data Catalog and Data Governance Policy manager should they choose to leverage them. Validation Points Fybrik validates the structures and values it receives from all external components. For interface components ( FybrikApplication and FybrikModule ), validation occurs when the resource is created, updated or deleted. How validation errors are received depends on whether fybrik is deployed with webhooks or not. If webhooks are deployed, errors are received from the kubernetes command (ex: kubectl apply ) and no resource is created. If webhooks are not deployed, validation is done in the resource's controller. If there is an error, the resource is created but its status will contain the error. (Note: These resources will need to manually be removed by the person creating them.) Summary The taxonomy mechanism enables independent components to work together, without the need for version updates and redeployment as capabilities change.","title":"Taxonomy"},{"location":"concepts/taxonomy/#taxonomy","text":"A taxonomy defines the terms and related values that need to be commonly understood and supported across the components in the system. Fybrik interacts with multiple external components, such as the data catalog, data governance policy manager, and modules. In order for fybrik to orchestrate the data plane of a given workload it is essential that all the components involved use common terms. For example, if a data governance policy refers to a particular transform it is crucial that the module implementing that transform refer to it in the same way, or for fybrik to be able to map between the disparate terms. Some components that use taxonomy: FybrikApplication yaml - information provided about the workload and the datasets. Fybrik manager ( FybrikApplication controller ) - validates that the data is used in accord with the data governance policies and the IT config policies. Data catalog - provides metadata about the asset. Data Governance Policy Manager - defines the governance policies to follow. Config Policy Manager - defines the IT policies to follow. FybrikModules - describe capabilities that can be included in a data plane. Default taxonomies are provided by fybrik in a JSON file format, and are meant as a starting point. Different actors can expand these.","title":"Taxonomy"},{"location":"concepts/taxonomy/#issues-addressed-by-taxonomy","text":"The taxonomy addresses the following: Redundancy: No need for the same structures and values to be hardcoded in multiple places, such as in the fybrik manager and in the plugins. Validation: Validates structures and values passed between components. Dynamic Updates: New terms and new values can be added, removed and updated dynamically. For example, one can add new enforcement actions, new connection types, new purposes, etc without needing to redeploy fybrik.","title":"Issues Addressed by Taxonomy"},{"location":"concepts/taxonomy/#taxonomy-contributors","text":"Different actors and components define the contents of different parts of the taxonomy. The following table describes the taxonomy and which component most logically owns each part of it. Taxonomy Contributing Component Actor Example Values catalog Data Catalog Data Steward data stores, formats, metadata application Policy Manager, Data Catalog Governance Officer roles, intents module Modules Module Developer capabilities, transforms If, for example, a Data Governance Officer writes a policy that limits the use of sensitive data for marketing, then the possible valid intents such as marketing would be defined by him in the Data Policy Manager. These values must be added to fybrik's taxonomy, either manually or via an automated feed, so that fybrik can validate the intent provided in a FybrikApplication yaml when a user's workload requests data. As new capabilities, transforms, data types, and protocols are made available via FybrikModules, fybrik's module taxonomy must be updated. Once updated these capabilities are available for use by other components, such as the Data Catalog and Data Governance Policy manager should they choose to leverage them.","title":"Taxonomy Contributors"},{"location":"concepts/taxonomy/#validation-points","text":"Fybrik validates the structures and values it receives from all external components. For interface components ( FybrikApplication and FybrikModule ), validation occurs when the resource is created, updated or deleted. How validation errors are received depends on whether fybrik is deployed with webhooks or not. If webhooks are deployed, errors are received from the kubernetes command (ex: kubectl apply ) and no resource is created. If webhooks are not deployed, validation is done in the resource's controller. If there is an error, the resource is created but its status will contain the error. (Note: These resources will need to manually be removed by the person creating them.)","title":"Validation Points"},{"location":"concepts/taxonomy/#summary","text":"The taxonomy mechanism enables independent components to work together, without the need for version updates and redeployment as capabilities change.","title":"Summary"},{"location":"concepts/vault_plugins/","text":"HashiCorp Vault plugins HashiCorp Vault plugins are standalone applications that Vault server executes to enable third-party secret engines and auth methods. After their enablement during Vault server initialization, the plugins can be used as a regular auth or secrets backends. This project uses secrets plugins to retrieve dataset credentials by the running modules . The plugins retrieve the credentials from where they are stored, for example, in a data catalog or in a kubernetes secret. Vault-plugin-secrets-kubernetes-reader plugin is an example of Vault custom secret plugin which retrieves dataset credentials stored in a kubernetes secret. Additional secret plugins can be developed to retrieve credentials additional location. This tutorial can serve as a good starting point to learn about Vault plugin development. Details on adding a new Vault plugin for Fybrik can be found in this task .","title":"HashiCorp Vault plugins"},{"location":"concepts/vault_plugins/#hashicorp-vault-plugins","text":"HashiCorp Vault plugins are standalone applications that Vault server executes to enable third-party secret engines and auth methods. After their enablement during Vault server initialization, the plugins can be used as a regular auth or secrets backends. This project uses secrets plugins to retrieve dataset credentials by the running modules . The plugins retrieve the credentials from where they are stored, for example, in a data catalog or in a kubernetes secret. Vault-plugin-secrets-kubernetes-reader plugin is an example of Vault custom secret plugin which retrieves dataset credentials stored in a kubernetes secret. Additional secret plugins can be developed to retrieve credentials additional location. This tutorial can serve as a good starting point to learn about Vault plugin development. Details on adding a new Vault plugin for Fybrik can be found in this task .","title":"HashiCorp Vault plugins"},{"location":"contribute/","text":"Contribute Fybrik is open for contributions and welcomes anyone who wishes to contribute and take part in our journey towards success. This section contains information and guidelines to help you contribute more easily to the project. We would love for you to get involved Join our community in GitHub Discussions","title":"About"},{"location":"contribute/#contribute","text":"Fybrik is open for contributions and welcomes anyone who wishes to contribute and take part in our journey towards success. This section contains information and guidelines to help you contribute more easily to the project. We would love for you to get involved Join our community in GitHub Discussions","title":"Contribute"},{"location":"contribute/build-test/","text":"Build and Test Build the project images make docker-build Run unit tests make test Some tests for controllers are written in a fashion that they can be run on a simulated environment using envtest or on an already existing Kubernetes cluster (or local kind cluster). The default is to use envtest. In order to run the tests in a local cluster the following environment variables can be set: NO_SIMULATED_PROGRESS = true USE_EXISTING_CLUSTER = true make -C manager test Please be aware that the controller is running locally in this case! If a controller is already deployed onto the cluster then the tests can be run with the command below. This will ensure that the tests are only creating custom resources on the cluster and checking their status: USE_EXISTING_CONTROLLER = true NO_SIMULATED_PROGRESS = true USE_EXISTING_CLUSTER = true make -C manager test Environment variables description Environment variable Default Description USE_EXISTING_CLUSTER false This variable controls if an existing K8s cluster should be used or not. If not envtest will spin up an artificial environment that includes a local etcd setup. NO_SIMULATED_PROGRESS false This variable can be used by tests that can manually simulate progress of e.g. jobs or pods. e.g. the simulated test environment from testEnv does not progress pods etc while when testing against an external Kubernetes cluster this will actually run pods. USE_EXISTING_CONTROLLER false This variable controls if a controller should be set up and run by this test suite or if an external one should be used. E.g. in integration tests running against an existing setup a controller is already existing in the Kubernetes cluster and should not be started by the test as two controllers competing may influence the test. Running integration tests With the following you will then setup a kind cluster with the local registry, build and push current docker images and finally run the integration tests on it: make run-integration-tests It is sometimes useful to call the integration test commands step by step, e.g., if you want to only repeat a specific step which failed without having to rerun the entire sequence. You can find the commands of the run-integration-tests target in the Makefile . You can run make kind-cleanup to delete the created clusters when you're done. Building in a multi cluster environment As Fybrik can run in a multi-cluster environment there is also a test environment that can be used that simulates this scenario. Using kind one can spin up two separate kubernetes clusters with different contexts and develop and test in these. Two kind clusters that share the same kind-registry can be set up using: make kind-setup-multi You can run make kind-cleanup to delete the created clusters when you're done.","title":"Build and Test"},{"location":"contribute/build-test/#build-and-test","text":"","title":"Build and Test"},{"location":"contribute/build-test/#build-the-project-images","text":"make docker-build","title":"Build the project images"},{"location":"contribute/build-test/#run-unit-tests","text":"make test Some tests for controllers are written in a fashion that they can be run on a simulated environment using envtest or on an already existing Kubernetes cluster (or local kind cluster). The default is to use envtest. In order to run the tests in a local cluster the following environment variables can be set: NO_SIMULATED_PROGRESS = true USE_EXISTING_CLUSTER = true make -C manager test Please be aware that the controller is running locally in this case! If a controller is already deployed onto the cluster then the tests can be run with the command below. This will ensure that the tests are only creating custom resources on the cluster and checking their status: USE_EXISTING_CONTROLLER = true NO_SIMULATED_PROGRESS = true USE_EXISTING_CLUSTER = true make -C manager test","title":"Run unit tests"},{"location":"contribute/build-test/#environment-variables-description","text":"Environment variable Default Description USE_EXISTING_CLUSTER false This variable controls if an existing K8s cluster should be used or not. If not envtest will spin up an artificial environment that includes a local etcd setup. NO_SIMULATED_PROGRESS false This variable can be used by tests that can manually simulate progress of e.g. jobs or pods. e.g. the simulated test environment from testEnv does not progress pods etc while when testing against an external Kubernetes cluster this will actually run pods. USE_EXISTING_CONTROLLER false This variable controls if a controller should be set up and run by this test suite or if an external one should be used. E.g. in integration tests running against an existing setup a controller is already existing in the Kubernetes cluster and should not be started by the test as two controllers competing may influence the test.","title":"Environment variables description"},{"location":"contribute/build-test/#running-integration-tests","text":"With the following you will then setup a kind cluster with the local registry, build and push current docker images and finally run the integration tests on it: make run-integration-tests It is sometimes useful to call the integration test commands step by step, e.g., if you want to only repeat a specific step which failed without having to rerun the entire sequence. You can find the commands of the run-integration-tests target in the Makefile . You can run make kind-cleanup to delete the created clusters when you're done.","title":"Running integration tests"},{"location":"contribute/build-test/#building-in-a-multi-cluster-environment","text":"As Fybrik can run in a multi-cluster environment there is also a test environment that can be used that simulates this scenario. Using kind one can spin up two separate kubernetes clusters with different contexts and develop and test in these. Two kind clusters that share the same kind-registry can be set up using: make kind-setup-multi You can run make kind-cleanup to delete the created clusters when you're done.","title":"Building in a multi cluster environment"},{"location":"contribute/environment/","text":"Development Environment This page describes what you need to install as a developer and contributor to this project, for setting up a development environment. Operating system Linux and Mac OS operating systems are officially supported. Windows users should consider using Windows Subsystem for Linux 2 (WSL 2), a remote Linux machine, or any other solution such as a virtual machine. Dependencies Install the following on your machine: go 1.19 Docker make jq unzip Mac only : brew install coreutils (installs the timeout command) Then, run the following command to install additional dependencies: make install-tools This installs additional dependencies to hack/tools/bin . The make targets (e.g., make test ) are configured to use the binaries from hack/tools/bin . However, you may want to add some of these tools to your system PATH for direct usage from your terminal (e.g., for using kubectl ). NOTE: If you want to contribute to the documentation on the website, and preview the website locally, you should install Mkdocs-Material and follow a few more steps . Editors The project is predominantly written in Go, so we recommend Visual Studio Code for its good Go support. Alternatively you can select from Editors Docker hub rate limits As docker hub introduced rate limits on docker image downloads this may affect development using the local kind setup. One option to fix the limit is to use a docker hub login for downloading the images. The environment will run a docker registry as a proxy for all public images. This registry runs in a docker container next to the kind clusters. export DOCKERHUB_USERNAME = 'your docker hub username' export DOCKERHUB_PASSWORD = 'your password' Optional Code Improvement and Verification Tools Fybrik repositories use different linters to validate and improve code. golangci-lint works as a Go linters aggregator, which includes a lot of linter such as staticcheck , revive , goimports and more. You can check its configuration in the .golangci.yml files. Integrating golangci-lint with VS Code Change the default lint to golanci-lint in VS Code: Install golangci-lint: https://golangci-lint.run/usage/install/ Open VS Code setting.json : Open the Command Palette: Ctrl+Shift+P In the dropdown search box, search for \"Open Settings (JSON)\" Open setting.json Add to setting.json the following: \"go.lintTool\":\"golangci-lint\", \"go.lintFlags\": [ \"--fast\", \"--allow-parallel-runners\" ] Golangci-lint automatically discovers .golangci.yml in the working project, you don't need to configure it in VS Code settings. To integrate with other IDEs: https://golangci-lint.run/usage/integrations/ If you wish to run golangci-lint on cmd, run in the desired directory: golangci-lint run --fast Pre-commit pre-commit is an optional tool that inspect the snapshot that's about to be committed according to the configured hooks, in our case, golangci-lint . Pre-commit configuration is in .pre-commit-config.yaml How to use: Install pre-commit: https://pre-commit.com/ In the repository, run: pre-commit install Now, pre-commit will automatically validate all your commits. To run commits without pre-commit validation add the --no-verify flag to git commit .","title":"Development Environment"},{"location":"contribute/environment/#development-environment","text":"This page describes what you need to install as a developer and contributor to this project, for setting up a development environment.","title":"Development Environment"},{"location":"contribute/environment/#operating-system","text":"Linux and Mac OS operating systems are officially supported. Windows users should consider using Windows Subsystem for Linux 2 (WSL 2), a remote Linux machine, or any other solution such as a virtual machine.","title":"Operating system"},{"location":"contribute/environment/#dependencies","text":"Install the following on your machine: go 1.19 Docker make jq unzip Mac only : brew install coreutils (installs the timeout command) Then, run the following command to install additional dependencies: make install-tools This installs additional dependencies to hack/tools/bin . The make targets (e.g., make test ) are configured to use the binaries from hack/tools/bin . However, you may want to add some of these tools to your system PATH for direct usage from your terminal (e.g., for using kubectl ). NOTE: If you want to contribute to the documentation on the website, and preview the website locally, you should install Mkdocs-Material and follow a few more steps .","title":"Dependencies"},{"location":"contribute/environment/#editors","text":"The project is predominantly written in Go, so we recommend Visual Studio Code for its good Go support. Alternatively you can select from Editors","title":"Editors"},{"location":"contribute/environment/#docker-hub-rate-limits","text":"As docker hub introduced rate limits on docker image downloads this may affect development using the local kind setup. One option to fix the limit is to use a docker hub login for downloading the images. The environment will run a docker registry as a proxy for all public images. This registry runs in a docker container next to the kind clusters. export DOCKERHUB_USERNAME = 'your docker hub username' export DOCKERHUB_PASSWORD = 'your password'","title":"Docker hub rate limits"},{"location":"contribute/environment/#optional-code-improvement-and-verification-tools","text":"Fybrik repositories use different linters to validate and improve code. golangci-lint works as a Go linters aggregator, which includes a lot of linter such as staticcheck , revive , goimports and more. You can check its configuration in the .golangci.yml files.","title":"Optional Code Improvement and Verification Tools"},{"location":"contribute/environment/#integrating-golangci-lint-with-vs-code","text":"Change the default lint to golanci-lint in VS Code: Install golangci-lint: https://golangci-lint.run/usage/install/ Open VS Code setting.json : Open the Command Palette: Ctrl+Shift+P In the dropdown search box, search for \"Open Settings (JSON)\" Open setting.json Add to setting.json the following: \"go.lintTool\":\"golangci-lint\", \"go.lintFlags\": [ \"--fast\", \"--allow-parallel-runners\" ] Golangci-lint automatically discovers .golangci.yml in the working project, you don't need to configure it in VS Code settings. To integrate with other IDEs: https://golangci-lint.run/usage/integrations/ If you wish to run golangci-lint on cmd, run in the desired directory: golangci-lint run --fast","title":"Integrating golangci-lint with VS Code"},{"location":"contribute/environment/#pre-commit","text":"pre-commit is an optional tool that inspect the snapshot that's about to be committed according to the configured hooks, in our case, golangci-lint . Pre-commit configuration is in .pre-commit-config.yaml How to use: Install pre-commit: https://pre-commit.com/ In the repository, run: pre-commit install Now, pre-commit will automatically validate all your commits. To run commits without pre-commit validation add the --no-verify flag to git commit .","title":"Pre-commit"},{"location":"contribute/flow/","text":"GitHub Workflow This page describes the GitHub workflow that contributors should follow. Issues and pull requests Contributing to Fybrik is done following the GitHub workflow of Pull Requests. You should usually open a pull request in the following situations: Start work on a contribution that was that you\u2019ve already discussed in an issue. Submit trivial fixes (for example, a typo, a broken link or an obvious error). A pull request doesn\u2019t have to represent finished work. It\u2019s usually better to open a draft pull request early on, so others can watch or give feedback on your progress. Here\u2019s how to submit a pull request: Fork the main repository Clone the forked repository locally . Connect your local to the original \u201cupstream\u201d repository by adding it as a remote. git clone git@github.com: $( git config user.name ) /fybrik.git git remote add upstream https://github.com/fybrik/fybrik.git git remote set-url --push upstream no_push Pull in changes from \u201cupstream\u201d often so that you stay up to date so that when you submit your pull request, merge conflicts will be less likely. git fetch upstream master git checkout master git merge upstream/master git push origin master Create a branch for your edits from master. Note that you should never add edits to the master branch itself. git checkout -b <branch name> Make commits of logical units , ensuring that commit messages are in the proper format . Push your changes to the created branch in your fork of the repository. Open a pull request to the original repository. Reference any relevant issues or supporting documentation in your PR (for example, \u201cCloses #37.\u201d) As always, you must follow code style , ensure that all tests pass , and add any new tests as appropriate. Thanks for your contribution! Normalize the code To ensure the code is formatted uniformly we use various linters which are invoked using make verify Format of the Commit Message The project follows a rough convention for commit messages that is designed to answer two questions: what changed and why. The subject line should feature the what and the body of the commit should describe the why. Every commit must also include a DCO Sign Off at the end of the commit message. By doing this you state that you certify the Developer Certificate of Origin . This can be automated by adding the -s flag to git commit . You can also mass sign-off a whole PR with git rebase --signoff master . Example commit message: scripts: add the test-cluster command this uses tmux to setup a test cluster that you can easily kill and start for debugging. Fixes #38 Signed-off-by: Legal Name <your.email@example.com> The format can be described more formally as follows: <subsystem>: <what changed> <BLANK LINE> <why this change was made> <BLANK LINE> <footer> <BLANK LINE> <signoff> The first line is the subject and should be no longer than 70 characters, the second line is always blank, and other lines should be wrapped at 80 characters. This allows the message to be easier to read on GitHub as well as in various git tools.","title":"GitHub Workflow"},{"location":"contribute/flow/#github-workflow","text":"This page describes the GitHub workflow that contributors should follow.","title":"GitHub Workflow"},{"location":"contribute/flow/#issues-and-pull-requests","text":"Contributing to Fybrik is done following the GitHub workflow of Pull Requests. You should usually open a pull request in the following situations: Start work on a contribution that was that you\u2019ve already discussed in an issue. Submit trivial fixes (for example, a typo, a broken link or an obvious error). A pull request doesn\u2019t have to represent finished work. It\u2019s usually better to open a draft pull request early on, so others can watch or give feedback on your progress. Here\u2019s how to submit a pull request: Fork the main repository Clone the forked repository locally . Connect your local to the original \u201cupstream\u201d repository by adding it as a remote. git clone git@github.com: $( git config user.name ) /fybrik.git git remote add upstream https://github.com/fybrik/fybrik.git git remote set-url --push upstream no_push Pull in changes from \u201cupstream\u201d often so that you stay up to date so that when you submit your pull request, merge conflicts will be less likely. git fetch upstream master git checkout master git merge upstream/master git push origin master Create a branch for your edits from master. Note that you should never add edits to the master branch itself. git checkout -b <branch name> Make commits of logical units , ensuring that commit messages are in the proper format . Push your changes to the created branch in your fork of the repository. Open a pull request to the original repository. Reference any relevant issues or supporting documentation in your PR (for example, \u201cCloses #37.\u201d) As always, you must follow code style , ensure that all tests pass , and add any new tests as appropriate. Thanks for your contribution!","title":"Issues and pull requests"},{"location":"contribute/flow/#normalize-the-code","text":"To ensure the code is formatted uniformly we use various linters which are invoked using make verify","title":"Normalize the code"},{"location":"contribute/flow/#format-of-the-commit-message","text":"The project follows a rough convention for commit messages that is designed to answer two questions: what changed and why. The subject line should feature the what and the body of the commit should describe the why. Every commit must also include a DCO Sign Off at the end of the commit message. By doing this you state that you certify the Developer Certificate of Origin . This can be automated by adding the -s flag to git commit . You can also mass sign-off a whole PR with git rebase --signoff master . Example commit message: scripts: add the test-cluster command this uses tmux to setup a test cluster that you can easily kill and start for debugging. Fixes #38 Signed-off-by: Legal Name <your.email@example.com> The format can be described more formally as follows: <subsystem>: <what changed> <BLANK LINE> <why this change was made> <BLANK LINE> <footer> <BLANK LINE> <signoff> The first line is the subject and should be no longer than 70 characters, the second line is always blank, and other lines should be wrapped at 80 characters. This allows the message to be easier to read on GitHub as well as in various git tools.","title":"Format of the Commit Message"},{"location":"contribute/logging/","text":"Logging This page describes the information that your code should provide in all log entries it generates, and some tools fybrik provides to ensure consistency across components. Background Log entries should be written to stdout and stderr. Fybrik does not collect nor aggregate logs. This may be done by external tools. (ex: logstash, fluentd, etc.) A globally unique identifier for each FybrikApplication instance is passed to all control plane and data plane components to be included in log entries. This enables corrrelation of log entries across different logs and clusters for the specific instance, even if the name of the FybrikApplication is reused over time. Log Entry Contents All fybrik components, whether control plane or data plane components, should write log entries to stdout and stderr in json format. The contents of the log entries are detailed in fybrik.io/pkg/logging/logging.go. The fybrik control plane uses zerolog for its golang components, and provides a library of fybrik specific helper functions to be used with it. Examples of how to use zerolog: https://github.com/rs/zerolog/blob/master/log_example_test.go TBD - fybrik logging helper functions for python and java. Log Entry Verbosity The choice of a log level should take into account in which environments the logged information is relevant: production, testing, or development. Although the administrator can configure the verbosity as desired, the following are typical configurations for the different environments. All environments Errors should always be logged, and preferably with as much information as possible. To this end, the function LogStructure in in pkg/logging/logging.go converts golang structures to json for inclussion in the log. Please note that panic and fatal should be used sparingly. panic (zerolog.PanicLevel, 5) - Errors that prevent the component from operating correctly and handling requests Ex: fybrik control plane did not deploy correctly Ex: Data plane component crashed and cannot handle requests fatal (zerolog.FatalLevel, 4) - Errors that prevent the component from successfully completing a particular task Ex: fybrikapplication controller cannot generate a plotter Ex: Arrow/Flight server used to read data cannot access data store error (zerolog.ErrorLevel, 3) - Errors that are not fatal nor panic, but that the user / request initiator is made aware of (typical production setting for stable solution) Ex: Dataset requested in fybrikapplication.spec is not allowed to be used Ex: Query to Arrow/Flight server used to read data returns an error because of incorrect dataset ID warn (zerolog.WarnLevel, 2) - Errors not shared with the user / request initiator, typically from which the component recovers on its own Production All of the previous plus: - info (zerolog.InfoLevel, 1) - High level health information that makes it clear the overall status, but without much detail (highest level used in production) Testing All of the previous plus: - debug (zerolog.DebugLevel, 0) - Additional information needed to help identify problems (typically used during testing) Development All of the previous plus: - trace (zerolog.TraceLevel, -1) - For tracing step by step flow of control (typically used during development) JSON Logging Standard Format All Fybrik components should generate logging information in a standard format. This information will be used by different actors for different purposes, so as much relevant information as possible needs to be captured in a consistent format. We list all mandatory and optional fields to be used by all Fybrik components. In addition to the fields we list, Fybrik components may include extra fields as needed. Mandatory Fields The fields in this section are typically generated by the logging libraries. level - log level (\u2018panic\u2019, \u2018fatal\u2019, \u2018error\u2019, \u2018warn\u2019, \u2018info\u2019, \u2018debug\u2019, or \u2018trace\u2019) time - timestamp of the log event. Timestamps should be in ISO8601 format with time offset from UTC or timezone. Example: \u20182022-02-16T10:46:21+02:00\u2019 caller - the code line which generated the error (file name + line number). Example: manager/main.go:319 Optional Fields app.fybrik.io/app-uuid - unique identifier for kubernetes FybrikApplication, used to correlate log messages across components for a particular FybrikApplication instance. It is also unique over time so one may differentiate between FybrikApplication instances with the same name created at different times message - string message for the log entry. Either this field or message_id must be included message_id - unique identifier indicating the message string that should be used. This is used instead of a message string for messages that need to support internationalization, such as those that go to users funcName - method or function in which the error occurred DataSetID - unique identifier for the data set ForUser - True if this should be shared with the end user in fybrikapplication status or events. False otherwise ForArchive - True if this should be archived long term. For example, if it contains full contents of FybrikApplication and its status and should be stored for auditing purposes cluster - cluster name on which the process generating the entry ran component - name of the component generating the log entry action - current operation being called. For example, \u201ccreate_catalog\u201d or \u201cupdate_asset\u201d response_time - response time of the current operation in milliseconds. Can be used in monitoring dashboards such as Kibana error \u2013 the error code or message returned to the fybrik component upon an unsuccessful action. Additional context should usually be provided in the accompanying message field Environment Variables LOGGING_VERBOSITY - should be set to one of the levels described in the previous section. PRETTY_LOGGING - If true log entries are in human readable format. If false, they are in json. Should only be true during development, since json is preferred to enable easy parsing by aggregator tools. Logging of Structures Fybrik provides a helper function called LogStructure in pkg/logging/logging.go for writing Go structures in json format to the log. It supports different verbosity levels, and thus can be used in production, testing and development environments.","title":"Logging"},{"location":"contribute/logging/#logging","text":"This page describes the information that your code should provide in all log entries it generates, and some tools fybrik provides to ensure consistency across components.","title":"Logging"},{"location":"contribute/logging/#background","text":"Log entries should be written to stdout and stderr. Fybrik does not collect nor aggregate logs. This may be done by external tools. (ex: logstash, fluentd, etc.) A globally unique identifier for each FybrikApplication instance is passed to all control plane and data plane components to be included in log entries. This enables corrrelation of log entries across different logs and clusters for the specific instance, even if the name of the FybrikApplication is reused over time.","title":"Background"},{"location":"contribute/logging/#log-entry-contents","text":"All fybrik components, whether control plane or data plane components, should write log entries to stdout and stderr in json format. The contents of the log entries are detailed in fybrik.io/pkg/logging/logging.go. The fybrik control plane uses zerolog for its golang components, and provides a library of fybrik specific helper functions to be used with it. Examples of how to use zerolog: https://github.com/rs/zerolog/blob/master/log_example_test.go TBD - fybrik logging helper functions for python and java.","title":"Log Entry Contents"},{"location":"contribute/logging/#log-entry-verbosity","text":"The choice of a log level should take into account in which environments the logged information is relevant: production, testing, or development. Although the administrator can configure the verbosity as desired, the following are typical configurations for the different environments.","title":"Log Entry Verbosity"},{"location":"contribute/logging/#all-environments","text":"Errors should always be logged, and preferably with as much information as possible. To this end, the function LogStructure in in pkg/logging/logging.go converts golang structures to json for inclussion in the log. Please note that panic and fatal should be used sparingly. panic (zerolog.PanicLevel, 5) - Errors that prevent the component from operating correctly and handling requests Ex: fybrik control plane did not deploy correctly Ex: Data plane component crashed and cannot handle requests fatal (zerolog.FatalLevel, 4) - Errors that prevent the component from successfully completing a particular task Ex: fybrikapplication controller cannot generate a plotter Ex: Arrow/Flight server used to read data cannot access data store error (zerolog.ErrorLevel, 3) - Errors that are not fatal nor panic, but that the user / request initiator is made aware of (typical production setting for stable solution) Ex: Dataset requested in fybrikapplication.spec is not allowed to be used Ex: Query to Arrow/Flight server used to read data returns an error because of incorrect dataset ID warn (zerolog.WarnLevel, 2) - Errors not shared with the user / request initiator, typically from which the component recovers on its own","title":"All environments"},{"location":"contribute/logging/#production","text":"All of the previous plus: - info (zerolog.InfoLevel, 1) - High level health information that makes it clear the overall status, but without much detail (highest level used in production)","title":"Production"},{"location":"contribute/logging/#testing","text":"All of the previous plus: - debug (zerolog.DebugLevel, 0) - Additional information needed to help identify problems (typically used during testing)","title":"Testing"},{"location":"contribute/logging/#development","text":"All of the previous plus: - trace (zerolog.TraceLevel, -1) - For tracing step by step flow of control (typically used during development)","title":"Development"},{"location":"contribute/logging/#json-logging-standard-format","text":"All Fybrik components should generate logging information in a standard format. This information will be used by different actors for different purposes, so as much relevant information as possible needs to be captured in a consistent format. We list all mandatory and optional fields to be used by all Fybrik components. In addition to the fields we list, Fybrik components may include extra fields as needed.","title":"JSON Logging Standard Format"},{"location":"contribute/logging/#mandatory-fields","text":"The fields in this section are typically generated by the logging libraries. level - log level (\u2018panic\u2019, \u2018fatal\u2019, \u2018error\u2019, \u2018warn\u2019, \u2018info\u2019, \u2018debug\u2019, or \u2018trace\u2019) time - timestamp of the log event. Timestamps should be in ISO8601 format with time offset from UTC or timezone. Example: \u20182022-02-16T10:46:21+02:00\u2019 caller - the code line which generated the error (file name + line number). Example: manager/main.go:319","title":"Mandatory Fields"},{"location":"contribute/logging/#optional-fields","text":"app.fybrik.io/app-uuid - unique identifier for kubernetes FybrikApplication, used to correlate log messages across components for a particular FybrikApplication instance. It is also unique over time so one may differentiate between FybrikApplication instances with the same name created at different times message - string message for the log entry. Either this field or message_id must be included message_id - unique identifier indicating the message string that should be used. This is used instead of a message string for messages that need to support internationalization, such as those that go to users funcName - method or function in which the error occurred DataSetID - unique identifier for the data set ForUser - True if this should be shared with the end user in fybrikapplication status or events. False otherwise ForArchive - True if this should be archived long term. For example, if it contains full contents of FybrikApplication and its status and should be stored for auditing purposes cluster - cluster name on which the process generating the entry ran component - name of the component generating the log entry action - current operation being called. For example, \u201ccreate_catalog\u201d or \u201cupdate_asset\u201d response_time - response time of the current operation in milliseconds. Can be used in monitoring dashboards such as Kibana error \u2013 the error code or message returned to the fybrik component upon an unsuccessful action. Additional context should usually be provided in the accompanying message field","title":"Optional Fields"},{"location":"contribute/logging/#environment-variables","text":"LOGGING_VERBOSITY - should be set to one of the levels described in the previous section. PRETTY_LOGGING - If true log entries are in human readable format. If false, they are in json. Should only be true during development, since json is preferred to enable easy parsing by aggregator tools.","title":"Environment Variables"},{"location":"contribute/logging/#logging-of-structures","text":"Fybrik provides a helper function called LogStructure in pkg/logging/logging.go for writing Go structures in json format to the log. It supports different verbosity levels, and thus can be used in production, testing and development environments.","title":"Logging of Structures"},{"location":"contribute/modules/","text":"Module Development This page describes what must be provided when contributing a module . A module is packaged as a Helm chart that the control plane can install to a workload's data plane. To first make a module available to the control plane, it must be registered by applying a FybrikModule custom resource. After that, a FybrikApplication can be submitted. For each FybrikApplication submitted, Fybrik chooses the best modules to accommodate the data needs (specified in the FybrikApplication) and deploys them. Each module is deployed using its Helm chart. Fybrik passes the needed configuration parameters to the module using the Helm mechanism for passing values . Steps for creating a module Implement the logic of the module you are contributing. The implementation can either be directly in the Module logic or in an external component. If the logic is in an external component, then the module logic should act as a client - i.e. receiving parameters from the control plane and passing them to the external component. Create and publish the Docker image that contains the module logic. Create and publish the Module Helm Chart that will be used by the control plane to deploy the module, update it, and delete it as necessary. Create the FybrikModule YAML which describes the capabilities of the module, in which flows it should be considered for inclusion, its supported interfaces, and the link to the module helm chart. Test the new module. These steps are described in the following sections in more detail, so that you can create your own modules for use by Fybrik. Note that a new module is maintained in its own git repository, separate from the Fybrik repository. Before you start, consider using the template module repository. with this template, you can easily create a repository for your own module that contains most of the needed templates and files. Module logic A running module is associated with a specific user workload and is deployed by the control plane. It may implement the logic required by itself, or it may be a client interface to an external component. The former will have module type \"server\" and the latter \"config\". Configuration Modules receive the parameters that define the configuration needed for the module (such as data asset connection information, required transformations, and so on) as Helm chart values. An example of parameters passed, can be found in the Helm values passed to the module section. To read the parameters, most modules define a conf.yaml file that grabs the relevant Helm values, and is copied to the environment of the module container. An example of a conf.yaml of the AirByte module can be found here . An example of a conif.py file of the AirByte module that reads the parameters can be found here . Credential management Modules that access or write data need credentials in order to access the data store. The credentials are retrieved from HashiCorp Vault . The parameters to login to vault and to read secret are passed as part of the arguments to the module Helm chart. An example for Vault Login API call which uses the Vault parameters is as follows: $ curl -v -X POST <address>/<authPath> -H \"Content-Type: application/json\" --data '{\"jwt\": <module service account token>, \"role\": <role>}' An example for Vault Read Secret API call which uses the Vault parameters is as follows: $ curl --header \"X-Vault-Token: ...\" -X GET https://<address>/<secretPath> Fybrik repository contains a Python Vault package that modules can use to retrieve the credentials. An example of the arrow flight module using Vault to retrieve credentials, in order to login to s3, can be found here . Docker image For a module to be installed using a Helm chart, a docker image needs to be published with the logic of the module. Follow the docker packaging guide if you are unfamiliar with publishing a Docker image. To see an example, see the Dockerfile of the Arrow Flight module. Module Helm Chart For any module chosen by the control plane to be part of the data path, the control plane needs to be able to install/remove/upgrade an instance of the module. Fybrik uses Helm to provide this functionality. Follow the Helm getting started guide if you are unfamiliar with Helm. Note that Helm 3.7 or above is required. The names of the Kubernetes resources deployed by the module helm chart must contain the release name to avoid resource conflicts. A Kubernetes service resource which is used to access the module must have a name equal to the release name (this service name is also used in the optional spec.capabilities.api.endpoint.hostname field). Helm values passed to the module Because the chart is installed by the control plane, the input values to the chart will contain the following information: .Values.assets - a list of asset arguments such as datastores, transformations, etc. .Values.context - application context .Values.labels - labels specified in FybrikApplication .Values.uuid - a unique id of FybrikApplication An example of values passed to a module(values.sample.yaml): labels: app.fybrik.io/app-name: my-notebook-read namespace: fybrik-notebook-sample uuid: 12345678 context: intent: \"Fraud Detection\" assets: - args: - connection: name: s3 s3: bucket: fybrik-test-bucket endpoint: s3.eu-gb.cloud-object-storage.appdomain.cloud object_key: test1.parquet format: parquet vault: read: address: http://vault.fybrik-system:8200 authPath: /v1/auth/kubernetes/login role: module secretPath: /v1/kubernetes-secrets/data-creds?namespace=fybrik-notebook-sample assetID: \"test1\" capability: read transformations: - name: \"RedactAction\" RedactAction: columns: - col1 - col2 If the module logic needs to return information to the user, that information should be written to the NOTES.txt of the helm chart. For a full example see the Arrow Flight Module chart . NOTE : Helm values that are passed from Fybrik to the modules, override the default values defined in the values.yaml file. To add additional parameters to be passed to the module, it is recommended to use the conf.yaml file. Publishing the Helm Chart Once your Helm chart is ready, you need to push it to a OCI-based registry such as ghcr.io . This allows the control plane of Fybrik to later pull the chart whenever it needs to be installed. You can use the hack/make-rules/helm.mk Makefile, or manually push the chart as described in the link : helm registry login -u <username> <registry> helm package <chart folder> -d <local-chart-path> helm push <local-chart-path> oci://<registry>/<path> FybrikModule YAML FybrikModule is a kubernetes Custom Resource Definition (custom resource) which describes to the control plane the functionality provided by the module. The FybrikModule custom resource has no controller. The specification of the FybrikModule Kubernetes custom resource is available in the API documentation . The YAML file begins with standard Kubernetes metadata followed by the FybrikModule specification: apiVersion : app.fybrik.io/v1beta1 kind : FybrikModule # always this value metadata : name : \"<module name>\" # the name of your new module labels : name : \"<module name>\" # the name of your new module version : \"<semantic version>\" namespace : <module namespace> # the namespace should match the adminCRsNamespace entry in values.yaml. The default is \"fybrik-system\". spec : ... The child fields of spec are described next. spec.chart This is a link to a the Helm chart stored in the image registry . This is similar to how a Kubernetes Pod references a container image. See Module Helm chart for more details. spec: chart: name: \"<helm chart link>\" # e.g.: ghcr.io/username/chartname:chartversion values: image.tag: v0.0.1 spec.statusIndicators Used for tracking the status of the module in terms of success or failure. In many cases this can be omitted and the status will be detected automatically. if the Helm chart includes standard Kubernetes resources such as Deployment and Service, then the status is automatically detected. If however Custom Resource Definitions are used, then the status may not be automatically detected and statusIndicators should be specified. statusIndicators : - kind : \"<module name>\" successCondition : \"<condition>\" # ex: status.status == SUCCEEDED failureCondition : \"<condition>\" # ex: status.status == FAILED errorMessage : \"<field path>\" # ex: status.error spec.dependencies A dependency has a type and a name . Currently dependencies of type module are supported, indicating that another module must also be installed for this module to work. dependencies : - type : module #currently the only option is a dependency on another module deployed by the control plane name : <dependent module name> spec.type The type field may be one of the following vaues: 1) service - Indicates that module implements the modules logic, and is deployed by the fybrik control plane. 2) config - In this case the logic is performed by a component deployed externally, i.e. not by the fybrik control plane. Such components can be assumed to support multiple workloads. 3) plugin (FUTURE) - This type of module enables a sub-set of often used capabilities to be implemented once and re-used by any module that supports plugins of the declared type. spec.pluginType (Future Functionality) The types of plugins supported by this module. Example: vault, fybrik-wasm ... spec.capabilities Each module may support one or more capabilities. Currently there are four capabilities: read for enabling an application to read data or prepare data for being read, write for enabling an application to write data, and copy for performing an implicit data copy on behalf of the application, and transform for altering data based on governance policies. A module provides one or more of these capabilities. capabilities.capability Indicates which of the types of capabilities this instance describes. capability : # Indicate the capabilities for which the control plane should consider using this module - read # optional - write # optional - copy # optional - transform # optional capability.scope The capability provided by the module may work on one of several different scopes: workload - deployed once by fybrik and available for use by the data planes of all the datasets asset - deployed by fybrik for each dataset cluster - deployed outside of fybrik and can be used by multiple fybbrik workloads in a given cluster scope : <scope of the capability> # cluster, workload, asset capabilites.supportedInterfaces Lists the supported data services from which the module can read data (sources) and to which it can write (sinks). There can be multiple sources and sinks. For each, a protocol and format are provided. protocol field can take a value such as kafka , s3 , db2 , fybrik-arrow-flight , etc. format field can take a value such as avro , parquet , json , or csv . Note that a module that targets copy flows will omit the api field and contain just source and sink , a module that only supports reading data assets will omit the sink field and only contain api and source capabilites.api describes the api exposed by the module to the user's workload for the particular capability. protocol field can take a value such as kafka , s3 , db2 , fybrik-arrow-flight , etc dataformat field can take a value such as parquet , csv , avro , etc endpoint field describes the endpoint exposed the module capabilites.api.endpoint describes the endpoint from a networking perspective: hostname field is the hostname to be used when accessing the module. Equals the release name. Can be omitted. port field is the port of the service exposed by the module. scheme field can take a value such as http , https , grpc , grpc+tls , jdbc:oracle:thin:@ , etc An example for a module that copies data from a db2 database table to an s3 bucket in parquet format. capabilities : - capability : copy supportedInterfaces : - source : protocol : db2 sink : protocol : s3 dataformat : parquet An example for a module that has an API for reading data, and supports reading both parquet and csv formats from s3. capabilities : - capability : read api : protocol : fybrik-arrow-flight endpoint : port : 80 scheme : grpc supportedInterfaces : - source : protocol : s3 dataformat : parquet - flow : read source : protocol : s3 dataformat : csv capabilites.actions are taken from a defined Enforcement Actions Taxonomy a module that does not perform any transformation on the data may omit the capabilities.actions field. The following is an example of how a module would declare that it knows how to redact, remove or encrypt data. Additional properties may be associated with each action. capabilities : - read : actions : - name : \"RedactAction\" - name : \"RemoveAction\" - name : \"EncryptAction\" Full Examples The following are examples of YAMLs from fully implemented modules: An example YAML for a module that copies from db2 to s3 and includes transformation actions And an example arrow flight read module YAML, also with transformation support Getting Started In order to help module developers get started there are two example \"hello world\" modules: Hello world module Hello world read module An example of a fully functional module is the arrow flight module . Test Register the module to make the control plane aware of it. Create an FybrikApplication YAML for a user workload, ensuring that the data set and other parameters included in it, together with the governance policies defined in the policy manager, will result in your module being chosen based on the control plane logic . Apply the FybrikApplication YAML. View the FybrikApplication status . Run the user workload and review the results to check if they are what is expected.","title":"Module Development"},{"location":"contribute/modules/#module-development","text":"This page describes what must be provided when contributing a module . A module is packaged as a Helm chart that the control plane can install to a workload's data plane. To first make a module available to the control plane, it must be registered by applying a FybrikModule custom resource. After that, a FybrikApplication can be submitted. For each FybrikApplication submitted, Fybrik chooses the best modules to accommodate the data needs (specified in the FybrikApplication) and deploys them. Each module is deployed using its Helm chart. Fybrik passes the needed configuration parameters to the module using the Helm mechanism for passing values .","title":"Module Development"},{"location":"contribute/modules/#steps-for-creating-a-module","text":"Implement the logic of the module you are contributing. The implementation can either be directly in the Module logic or in an external component. If the logic is in an external component, then the module logic should act as a client - i.e. receiving parameters from the control plane and passing them to the external component. Create and publish the Docker image that contains the module logic. Create and publish the Module Helm Chart that will be used by the control plane to deploy the module, update it, and delete it as necessary. Create the FybrikModule YAML which describes the capabilities of the module, in which flows it should be considered for inclusion, its supported interfaces, and the link to the module helm chart. Test the new module. These steps are described in the following sections in more detail, so that you can create your own modules for use by Fybrik. Note that a new module is maintained in its own git repository, separate from the Fybrik repository. Before you start, consider using the template module repository. with this template, you can easily create a repository for your own module that contains most of the needed templates and files.","title":"Steps for creating a module"},{"location":"contribute/modules/#module-logic","text":"A running module is associated with a specific user workload and is deployed by the control plane. It may implement the logic required by itself, or it may be a client interface to an external component. The former will have module type \"server\" and the latter \"config\".","title":"Module logic"},{"location":"contribute/modules/#configuration","text":"Modules receive the parameters that define the configuration needed for the module (such as data asset connection information, required transformations, and so on) as Helm chart values. An example of parameters passed, can be found in the Helm values passed to the module section. To read the parameters, most modules define a conf.yaml file that grabs the relevant Helm values, and is copied to the environment of the module container. An example of a conf.yaml of the AirByte module can be found here . An example of a conif.py file of the AirByte module that reads the parameters can be found here .","title":"Configuration"},{"location":"contribute/modules/#credential-management","text":"Modules that access or write data need credentials in order to access the data store. The credentials are retrieved from HashiCorp Vault . The parameters to login to vault and to read secret are passed as part of the arguments to the module Helm chart. An example for Vault Login API call which uses the Vault parameters is as follows: $ curl -v -X POST <address>/<authPath> -H \"Content-Type: application/json\" --data '{\"jwt\": <module service account token>, \"role\": <role>}' An example for Vault Read Secret API call which uses the Vault parameters is as follows: $ curl --header \"X-Vault-Token: ...\" -X GET https://<address>/<secretPath> Fybrik repository contains a Python Vault package that modules can use to retrieve the credentials. An example of the arrow flight module using Vault to retrieve credentials, in order to login to s3, can be found here .","title":"Credential management"},{"location":"contribute/modules/#docker-image","text":"For a module to be installed using a Helm chart, a docker image needs to be published with the logic of the module. Follow the docker packaging guide if you are unfamiliar with publishing a Docker image. To see an example, see the Dockerfile of the Arrow Flight module.","title":"Docker image"},{"location":"contribute/modules/#module-helm-chart","text":"For any module chosen by the control plane to be part of the data path, the control plane needs to be able to install/remove/upgrade an instance of the module. Fybrik uses Helm to provide this functionality. Follow the Helm getting started guide if you are unfamiliar with Helm. Note that Helm 3.7 or above is required. The names of the Kubernetes resources deployed by the module helm chart must contain the release name to avoid resource conflicts. A Kubernetes service resource which is used to access the module must have a name equal to the release name (this service name is also used in the optional spec.capabilities.api.endpoint.hostname field).","title":"Module Helm Chart"},{"location":"contribute/modules/#helm-values-passed-to-the-module","text":"Because the chart is installed by the control plane, the input values to the chart will contain the following information: .Values.assets - a list of asset arguments such as datastores, transformations, etc. .Values.context - application context .Values.labels - labels specified in FybrikApplication .Values.uuid - a unique id of FybrikApplication An example of values passed to a module(values.sample.yaml): labels: app.fybrik.io/app-name: my-notebook-read namespace: fybrik-notebook-sample uuid: 12345678 context: intent: \"Fraud Detection\" assets: - args: - connection: name: s3 s3: bucket: fybrik-test-bucket endpoint: s3.eu-gb.cloud-object-storage.appdomain.cloud object_key: test1.parquet format: parquet vault: read: address: http://vault.fybrik-system:8200 authPath: /v1/auth/kubernetes/login role: module secretPath: /v1/kubernetes-secrets/data-creds?namespace=fybrik-notebook-sample assetID: \"test1\" capability: read transformations: - name: \"RedactAction\" RedactAction: columns: - col1 - col2 If the module logic needs to return information to the user, that information should be written to the NOTES.txt of the helm chart. For a full example see the Arrow Flight Module chart . NOTE : Helm values that are passed from Fybrik to the modules, override the default values defined in the values.yaml file. To add additional parameters to be passed to the module, it is recommended to use the conf.yaml file.","title":"Helm values passed to the module"},{"location":"contribute/modules/#publishing-the-helm-chart","text":"Once your Helm chart is ready, you need to push it to a OCI-based registry such as ghcr.io . This allows the control plane of Fybrik to later pull the chart whenever it needs to be installed. You can use the hack/make-rules/helm.mk Makefile, or manually push the chart as described in the link : helm registry login -u <username> <registry> helm package <chart folder> -d <local-chart-path> helm push <local-chart-path> oci://<registry>/<path>","title":"Publishing the Helm Chart"},{"location":"contribute/modules/#fybrikmodule-yaml","text":"FybrikModule is a kubernetes Custom Resource Definition (custom resource) which describes to the control plane the functionality provided by the module. The FybrikModule custom resource has no controller. The specification of the FybrikModule Kubernetes custom resource is available in the API documentation . The YAML file begins with standard Kubernetes metadata followed by the FybrikModule specification: apiVersion : app.fybrik.io/v1beta1 kind : FybrikModule # always this value metadata : name : \"<module name>\" # the name of your new module labels : name : \"<module name>\" # the name of your new module version : \"<semantic version>\" namespace : <module namespace> # the namespace should match the adminCRsNamespace entry in values.yaml. The default is \"fybrik-system\". spec : ... The child fields of spec are described next.","title":"FybrikModule YAML"},{"location":"contribute/modules/#specchart","text":"This is a link to a the Helm chart stored in the image registry . This is similar to how a Kubernetes Pod references a container image. See Module Helm chart for more details. spec: chart: name: \"<helm chart link>\" # e.g.: ghcr.io/username/chartname:chartversion values: image.tag: v0.0.1","title":"spec.chart"},{"location":"contribute/modules/#specstatusindicators","text":"Used for tracking the status of the module in terms of success or failure. In many cases this can be omitted and the status will be detected automatically. if the Helm chart includes standard Kubernetes resources such as Deployment and Service, then the status is automatically detected. If however Custom Resource Definitions are used, then the status may not be automatically detected and statusIndicators should be specified. statusIndicators : - kind : \"<module name>\" successCondition : \"<condition>\" # ex: status.status == SUCCEEDED failureCondition : \"<condition>\" # ex: status.status == FAILED errorMessage : \"<field path>\" # ex: status.error","title":"spec.statusIndicators"},{"location":"contribute/modules/#specdependencies","text":"A dependency has a type and a name . Currently dependencies of type module are supported, indicating that another module must also be installed for this module to work. dependencies : - type : module #currently the only option is a dependency on another module deployed by the control plane name : <dependent module name>","title":"spec.dependencies"},{"location":"contribute/modules/#spectype","text":"The type field may be one of the following vaues: 1) service - Indicates that module implements the modules logic, and is deployed by the fybrik control plane. 2) config - In this case the logic is performed by a component deployed externally, i.e. not by the fybrik control plane. Such components can be assumed to support multiple workloads. 3) plugin (FUTURE) - This type of module enables a sub-set of often used capabilities to be implemented once and re-used by any module that supports plugins of the declared type.","title":"spec.type"},{"location":"contribute/modules/#specplugintype","text":"(Future Functionality) The types of plugins supported by this module. Example: vault, fybrik-wasm ...","title":"spec.pluginType"},{"location":"contribute/modules/#speccapabilities","text":"Each module may support one or more capabilities. Currently there are four capabilities: read for enabling an application to read data or prepare data for being read, write for enabling an application to write data, and copy for performing an implicit data copy on behalf of the application, and transform for altering data based on governance policies. A module provides one or more of these capabilities. capabilities.capability Indicates which of the types of capabilities this instance describes. capability : # Indicate the capabilities for which the control plane should consider using this module - read # optional - write # optional - copy # optional - transform # optional capability.scope The capability provided by the module may work on one of several different scopes: workload - deployed once by fybrik and available for use by the data planes of all the datasets asset - deployed by fybrik for each dataset cluster - deployed outside of fybrik and can be used by multiple fybbrik workloads in a given cluster scope : <scope of the capability> # cluster, workload, asset capabilites.supportedInterfaces Lists the supported data services from which the module can read data (sources) and to which it can write (sinks). There can be multiple sources and sinks. For each, a protocol and format are provided. protocol field can take a value such as kafka , s3 , db2 , fybrik-arrow-flight , etc. format field can take a value such as avro , parquet , json , or csv . Note that a module that targets copy flows will omit the api field and contain just source and sink , a module that only supports reading data assets will omit the sink field and only contain api and source capabilites.api describes the api exposed by the module to the user's workload for the particular capability. protocol field can take a value such as kafka , s3 , db2 , fybrik-arrow-flight , etc dataformat field can take a value such as parquet , csv , avro , etc endpoint field describes the endpoint exposed the module capabilites.api.endpoint describes the endpoint from a networking perspective: hostname field is the hostname to be used when accessing the module. Equals the release name. Can be omitted. port field is the port of the service exposed by the module. scheme field can take a value such as http , https , grpc , grpc+tls , jdbc:oracle:thin:@ , etc An example for a module that copies data from a db2 database table to an s3 bucket in parquet format. capabilities : - capability : copy supportedInterfaces : - source : protocol : db2 sink : protocol : s3 dataformat : parquet An example for a module that has an API for reading data, and supports reading both parquet and csv formats from s3. capabilities : - capability : read api : protocol : fybrik-arrow-flight endpoint : port : 80 scheme : grpc supportedInterfaces : - source : protocol : s3 dataformat : parquet - flow : read source : protocol : s3 dataformat : csv capabilites.actions are taken from a defined Enforcement Actions Taxonomy a module that does not perform any transformation on the data may omit the capabilities.actions field. The following is an example of how a module would declare that it knows how to redact, remove or encrypt data. Additional properties may be associated with each action. capabilities : - read : actions : - name : \"RedactAction\" - name : \"RemoveAction\" - name : \"EncryptAction\"","title":"spec.capabilities"},{"location":"contribute/modules/#full-examples","text":"The following are examples of YAMLs from fully implemented modules: An example YAML for a module that copies from db2 to s3 and includes transformation actions And an example arrow flight read module YAML, also with transformation support","title":"Full Examples"},{"location":"contribute/modules/#getting-started","text":"In order to help module developers get started there are two example \"hello world\" modules: Hello world module Hello world read module An example of a fully functional module is the arrow flight module .","title":"Getting Started"},{"location":"contribute/modules/#test","text":"Register the module to make the control plane aware of it. Create an FybrikApplication YAML for a user workload, ensuring that the data set and other parameters included in it, together with the governance policies defined in the policy manager, will result in your module being chosen based on the control plane logic . Apply the FybrikApplication YAML. View the FybrikApplication status . Run the user workload and review the results to check if they are what is expected.","title":"Test"},{"location":"contribute/openapi/","text":"Using OpenAPI Generator to Generate Code for a Data Catalog Connector (in the Go Language) The Fybrik repository contains specification files that detail the data catalog connector API. These files include datacatalog.spec.yaml and taxonomy.json . They detail all the fields that the data catalog connector should expect for each of the supported operations: createAsset, getAssetInfo, deleteAsset, and updateAsset. The OpenAPI generator is a tool that can be used to generate the skeleton code for REST servers in a variety of programming languages, given a specification file (written in adherence to the OpenAPI standard ). In our case, we used the OpenAPI generator to generate skeleton code for a Fybrik data catalog connector server, in the go programming language. As expected, this skeleton code does not provide any functionality, since the specification file details only the API, not the functionality. Also, the behavior of the actual connector code must surely depend on the data catalog chosen to organize the Fyrbik assets. Here is a snippet of the Makefile used to automatically generate code: generate-code: git clone https://github.com/fybrik/fybrik/ cd fybrik && git checkout ${FYBRIK_VERSION} docker run --rm \\ -v ${PWD}:/local \\ -u \"${USER_ID}:${GROUP_ID}\" \\ openapitools/openapi-generator-cli generate -g go-server \\ --git-user-id=${GIT_USER_ID} \\ --git-repo-id=${GIT_REPO_ID} \\ --config=/local/openapi-configs/go-server-config.yaml \\ -o /local/api \\ -i /local/fybrik/connectors/api/datacatalog.spec.yaml rm -Rf fybrik One thing that the automatically generated code is very good at (besides providing us with the REST server framework) is checking the validity of the REST requests. The server code automatically rejects any requests whose body does not adhere to the specification. For instance, if any fields are missing from the request body, the server code would fail the request and return the relevant error code and error message. We found one main problem with the auto-generated code, and it had to do with the Connection structure. The Connection structure has one mandatory field, called name . According to the taxonomy, there could be additional properties, and there are no limitations on their names and types. The problem we encountered had to do with the additional properties. Here is an example of the connection information for a typical asset: asset.yaml . The connection information includes the mandatory name field, but also an additional property s3 with a few subfields. We found that assets with such connection information were rejected by the server with the following error: \"json: unknown field \\\"s3\\\"\" . This problem was easily overcome when we commented out the d.DisallowUnknownFields() line in the CreateAsset method. However, we encountered a more difficult problem when it turned out that these \"unknown fields\" were ignored and were not kept in the CreateAssetRequest objects. As a result, the unknown fields could not have been sent to the data catalog, and were removed from the asset metadata. There are several ways to overcome this problem. The best way we found so far was to avoid using the CreateAssetRequest struct automatically generated by the OpenAPI generator. Instead, we used a CreateAssetRequest struct defined as part of the Fybrik code . The connection subfield of the Fybrik CreateAssetRequest struct has a different definition , which includes both the name field and additional properties, as required.","title":"Using OpenAPI Generator"},{"location":"contribute/openapi/#using-openapi-generator-to-generate-code-for-a-data-catalog-connector","text":"(in the Go Language) The Fybrik repository contains specification files that detail the data catalog connector API. These files include datacatalog.spec.yaml and taxonomy.json . They detail all the fields that the data catalog connector should expect for each of the supported operations: createAsset, getAssetInfo, deleteAsset, and updateAsset. The OpenAPI generator is a tool that can be used to generate the skeleton code for REST servers in a variety of programming languages, given a specification file (written in adherence to the OpenAPI standard ). In our case, we used the OpenAPI generator to generate skeleton code for a Fybrik data catalog connector server, in the go programming language. As expected, this skeleton code does not provide any functionality, since the specification file details only the API, not the functionality. Also, the behavior of the actual connector code must surely depend on the data catalog chosen to organize the Fyrbik assets. Here is a snippet of the Makefile used to automatically generate code: generate-code: git clone https://github.com/fybrik/fybrik/ cd fybrik && git checkout ${FYBRIK_VERSION} docker run --rm \\ -v ${PWD}:/local \\ -u \"${USER_ID}:${GROUP_ID}\" \\ openapitools/openapi-generator-cli generate -g go-server \\ --git-user-id=${GIT_USER_ID} \\ --git-repo-id=${GIT_REPO_ID} \\ --config=/local/openapi-configs/go-server-config.yaml \\ -o /local/api \\ -i /local/fybrik/connectors/api/datacatalog.spec.yaml rm -Rf fybrik One thing that the automatically generated code is very good at (besides providing us with the REST server framework) is checking the validity of the REST requests. The server code automatically rejects any requests whose body does not adhere to the specification. For instance, if any fields are missing from the request body, the server code would fail the request and return the relevant error code and error message. We found one main problem with the auto-generated code, and it had to do with the Connection structure. The Connection structure has one mandatory field, called name . According to the taxonomy, there could be additional properties, and there are no limitations on their names and types. The problem we encountered had to do with the additional properties. Here is an example of the connection information for a typical asset: asset.yaml . The connection information includes the mandatory name field, but also an additional property s3 with a few subfields. We found that assets with such connection information were rejected by the server with the following error: \"json: unknown field \\\"s3\\\"\" . This problem was easily overcome when we commented out the d.DisallowUnknownFields() line in the CreateAsset method. However, we encountered a more difficult problem when it turned out that these \"unknown fields\" were ignored and were not kept in the CreateAssetRequest objects. As a result, the unknown fields could not have been sent to the data catalog, and were removed from the asset metadata. There are several ways to overcome this problem. The best way we found so far was to avoid using the CreateAssetRequest struct automatically generated by the OpenAPI generator. Instead, we used a CreateAssetRequest struct defined as part of the Fybrik code . The connection subfield of the Fybrik CreateAssetRequest struct has a different definition , which includes both the name field and additional properties, as required.","title":"Using OpenAPI Generator to Generate Code for a Data Catalog Connector"},{"location":"contribute/documentation/","text":"Contribute Documentation The content of this website is the documentation of the project. The documentation is managed in /site/docs as markdown files. MkDocs and the Material theme are used to generate the website from these markdown files. Reference pages are auto generated from the source code. Therefore, if you change Kubernetes Custom Resource Definitions or the connectors API then you must add reasonable documentation comments. The rest of the documentation pages are written manually. Contributing to the documentation is therefore similar to code contribution and follows the same process of using pull requests. However, when writing documentation you must also follow the formatting and style guidelines. Before opening a pull request, to preview the website locally, you should install Mkdocs-Material and follow a few more steps .","title":"Contribute Documentation"},{"location":"contribute/documentation/#contribute-documentation","text":"The content of this website is the documentation of the project. The documentation is managed in /site/docs as markdown files. MkDocs and the Material theme are used to generate the website from these markdown files. Reference pages are auto generated from the source code. Therefore, if you change Kubernetes Custom Resource Definitions or the connectors API then you must add reasonable documentation comments. The rest of the documentation pages are written manually. Contributing to the documentation is therefore similar to code contribution and follows the same process of using pull requests. However, when writing documentation you must also follow the formatting and style guidelines. Before opening a pull request, to preview the website locally, you should install Mkdocs-Material and follow a few more steps .","title":"Contribute Documentation"},{"location":"contribute/documentation/formatting/","text":"Formatting Standards This page shows the formatting standards for the Fybrik documentation. Link to other pages using relative links When linking between pages in the documentation you can simply use the regular Markdown linking syntax, including the relative path to the Markdown document you wish to link to. For example: Please see the [ project license ]( license.md ) for further details. If the target documentation file is in another directory you'll need to make sure to include any relative directory path in the link: Please see the [ project license ]( ../about/license.md ) for further details. Prefer SVG format for diagrams Place image files in the docs/static directory. Use regular Markdown syntax for images. For example: ![](../static/myimage.svg) To make localization easier and enhance accessibility, the preferred image format is SVG. We recommend to use draw.io for creating images and diagrams. Use Export as to save your image in SVG format. Keep the Include a copy of my diagram option checked to allow later loading the SVG in draw.io and be sure to check Embed images if you diagram includes any. If your diagram depicts a process, try to avoid adding the descriptions of the steps to the diagram. Instead, only add the numbers of the steps to the diagram and add the descriptions of the steps as a numbered list in the document. Ensure that the numbers on the list match the numbers on your diagram. This approach helps make diagrams easier to understand and the content more accessible. Do not wrap lines Never wrap lines after a fixed number of characters or in a middle of a senstence. Instead, configure your editor to soft-wrap when editing documentation. Do Don't This is a long line. This is a long line. Use angle brackets for placeholders Use angle brackets for placeholders in commands or code samples. Tell the reader what the placeholder represents. For example: Display information about a pod: $ kubectl describe pod <pod-name> Where <pod-name> is the name of one of your pods. Use bold to emphasize user interface elements Do Don't Click Fork . Click \"Fork\". Select Other . Select 'Other'. Use bold to emphasize important text Use bold to emphasize text that is particularly important. Avoid overusing bold as it reduces its impact and readability. Do Don't Examples of bad configurations: Examples of bad configurations : The maximum length of the name field is 256 characters . The maximum length of the name field is 256 characters . Don't use capitalization for emphasis Only use the original capitalization found in the code or configuration files when referencing those values directly. Use back-ticks `` around the referenced value to make the connection explicit. For example, use IsolationPolicy , not Isolation Policy or isolation policy . If you are not referencing values or code directly, use normal sentence capitalization, for example, \"The isolation policy configuration takes place in a YAML file.\" Use italics to emphasize new terms Do Don't A cluster is a set of nodes ... A \"cluster\" is a set of nodes ... These components form the control plane . These components form the control plane . Use back-ticks around file names, directories, and paths Do Don't Open the foo.yaml file. Open the foo.yaml file. Go to the /content/docs/architecture directory. Go to the /content/docs/architecture directory. Open the /data/args.yaml file. Open the /data/args.yaml file. Use back-ticks around inline code and commands Do Don't The foo run command creates a Deployment . The \"foo run\" command creates a Deployment . For declarative management, use foo apply . For declarative management, use \"foo apply\". Use code-blocks for commands you intend readers to execute. Only use inline code and commands to mention specific labels, flags, values, functions, objects, variables, modules, or commands. Use back-ticks around object field names Do Don't Set the value of the ports field in the configuration file. Set the value of the \"ports\" field in the configuration file. The value of the rule field is a Rule object. The value of the \"rule\" field is a Rule object.","title":"Formatting Standards"},{"location":"contribute/documentation/formatting/#formatting-standards","text":"This page shows the formatting standards for the Fybrik documentation.","title":"Formatting Standards"},{"location":"contribute/documentation/formatting/#link-to-other-pages-using-relative-links","text":"When linking between pages in the documentation you can simply use the regular Markdown linking syntax, including the relative path to the Markdown document you wish to link to. For example: Please see the [ project license ]( license.md ) for further details. If the target documentation file is in another directory you'll need to make sure to include any relative directory path in the link: Please see the [ project license ]( ../about/license.md ) for further details.","title":"Link to other pages using relative links"},{"location":"contribute/documentation/formatting/#prefer-svg-format-for-diagrams","text":"Place image files in the docs/static directory. Use regular Markdown syntax for images. For example: ![](../static/myimage.svg) To make localization easier and enhance accessibility, the preferred image format is SVG. We recommend to use draw.io for creating images and diagrams. Use Export as to save your image in SVG format. Keep the Include a copy of my diagram option checked to allow later loading the SVG in draw.io and be sure to check Embed images if you diagram includes any. If your diagram depicts a process, try to avoid adding the descriptions of the steps to the diagram. Instead, only add the numbers of the steps to the diagram and add the descriptions of the steps as a numbered list in the document. Ensure that the numbers on the list match the numbers on your diagram. This approach helps make diagrams easier to understand and the content more accessible.","title":"Prefer SVG format for diagrams"},{"location":"contribute/documentation/formatting/#do-not-wrap-lines","text":"Never wrap lines after a fixed number of characters or in a middle of a senstence. Instead, configure your editor to soft-wrap when editing documentation. Do Don't This is a long line. This is a long line.","title":"Do not wrap lines"},{"location":"contribute/documentation/formatting/#use-angle-brackets-for-placeholders","text":"Use angle brackets for placeholders in commands or code samples. Tell the reader what the placeholder represents. For example: Display information about a pod: $ kubectl describe pod <pod-name> Where <pod-name> is the name of one of your pods.","title":"Use angle brackets for placeholders"},{"location":"contribute/documentation/formatting/#use-bold-to-emphasize-user-interface-elements","text":"Do Don't Click Fork . Click \"Fork\". Select Other . Select 'Other'.","title":"Use bold to emphasize user interface elements"},{"location":"contribute/documentation/formatting/#use-bold-to-emphasize-important-text","text":"Use bold to emphasize text that is particularly important. Avoid overusing bold as it reduces its impact and readability. Do Don't Examples of bad configurations: Examples of bad configurations : The maximum length of the name field is 256 characters . The maximum length of the name field is 256 characters .","title":"Use bold to emphasize important text"},{"location":"contribute/documentation/formatting/#dont-use-capitalization-for-emphasis","text":"Only use the original capitalization found in the code or configuration files when referencing those values directly. Use back-ticks `` around the referenced value to make the connection explicit. For example, use IsolationPolicy , not Isolation Policy or isolation policy . If you are not referencing values or code directly, use normal sentence capitalization, for example, \"The isolation policy configuration takes place in a YAML file.\"","title":"Don't use capitalization for emphasis"},{"location":"contribute/documentation/formatting/#use-italics-to-emphasize-new-terms","text":"Do Don't A cluster is a set of nodes ... A \"cluster\" is a set of nodes ... These components form the control plane . These components form the control plane .","title":"Use italics to emphasize new terms"},{"location":"contribute/documentation/formatting/#use-back-ticks-around-file-names-directories-and-paths","text":"Do Don't Open the foo.yaml file. Open the foo.yaml file. Go to the /content/docs/architecture directory. Go to the /content/docs/architecture directory. Open the /data/args.yaml file. Open the /data/args.yaml file.","title":"Use back-ticks around file names, directories, and paths"},{"location":"contribute/documentation/formatting/#use-back-ticks-around-inline-code-and-commands","text":"Do Don't The foo run command creates a Deployment . The \"foo run\" command creates a Deployment . For declarative management, use foo apply . For declarative management, use \"foo apply\". Use code-blocks for commands you intend readers to execute. Only use inline code and commands to mention specific labels, flags, values, functions, objects, variables, modules, or commands.","title":"Use back-ticks around inline code and commands"},{"location":"contribute/documentation/formatting/#use-back-ticks-around-object-field-names","text":"Do Don't Set the value of the ports field in the configuration file. Set the value of the \"ports\" field in the configuration file. The value of the rule field is a Rule object. The value of the \"rule\" field is a Rule object.","title":"Use back-ticks around object field names"},{"location":"contribute/documentation/style/","text":"Style Guide This page provides basic style guidance for keeping the documentation of Fybrik clear and understandable . Choose the right title Use a short, keyword-rich title that captures the intent of the document and draws the reader in. Ensure that the title clearly and concisely conveys the content or subject matter and is meaningful to a global audience. The text for the title of the document must use title case. Capitalize the first letter of every word except conjunctions and prepositions. Do Don't # Security Architecture # Security architecture # Code of Conduct # Code Of Conduct Use sentence case for headings Use sentence case for the headings in your document. Only capitalize the first word of the heading, except for proper nouns or acronyms. Do Don't Configuring rate limits Configuring Rate Limits Using Envoy for ingress Using envoy for ingress Using HTTPS Using https Use present tense Do Don't This command starts a proxy. This command will start a proxy. Exception: Use future or past tense if it is required to convey the correct meaning. This exception is extremely rare and should be avoided. Use active voice Do Don't You can explore the API using a browser. The API can be explored using a browser. The YAML file specifies the replica count. The replica count is specified in the YAML file. Use simple and direct language Use simple and direct language. Avoid using unnecessary phrases, such as saying \"please.\" Do Don't To create a ReplicaSet , ... In order to create a ReplicaSet , ... See the configuration file. Please see the configuration file. View the Pods. With this next command, we'll view the Pods. Prefer shorter words over longer alternatives Do Don't This tool helps scaling up pods. This tool facilitates scaling up pods. Pilot uses the purpose field to ... Pilot utilizes the purpose field to ... Address the reader as \"you\" Do Don't You can create a Deployment by ... We'll create a Deployment by ... In the preceding output, you can see... In the preceding output, we can see ... Avoid using \"we\" Using \"we\" in a sentence can be confusing, because the reader might not know whether they're part of the \"we\" you're describing. Do Don't Version 1.4 includes ... In version 1.4, we have added ... Fybrik provides a new feature for ... We provide a new feature ... This page teaches you how to use pods. In this page, we are going to learn about pods. Avoid jargon and idioms Some readers speak English as a second language. Avoid jargon and idioms to help make their understanding easier. Do Don't Internally, ... Under the hood, ... Create a new cluster. Turn up a new cluster. Initially, ... Out of the box, ... Avoid statements that will soon be out of date Avoid using wording that becomes outdated quickly like \"currently\" and \"new\". A feature that is new today is not new for long. Do Don't In version 1.4, ... In the current version, ... The Federation feature provides ... The new Federation feature provides ... Avoid statements about the future Avoid making promises or giving hints about the future. If you need to talk about a feature in development, add a clear indication under the front matter that identifies the information accordingly: Warning This page describes a feature that is not yet released The only exceptions to this rule are design or architecture documents that can describe a vision. However, you must clearly distiquish between implemented features and a vision. Create useful links There are good hyperlinks, and bad hyperlinks. The common practice of calling links here or click here are examples of bad hyperlinks. Check out this excellent article explaining what makes a good hyperlink and try to keep these guidelines in mind when creating or reviewing site content.","title":"Style Guide"},{"location":"contribute/documentation/style/#style-guide","text":"This page provides basic style guidance for keeping the documentation of Fybrik clear and understandable .","title":"Style Guide"},{"location":"contribute/documentation/style/#choose-the-right-title","text":"Use a short, keyword-rich title that captures the intent of the document and draws the reader in. Ensure that the title clearly and concisely conveys the content or subject matter and is meaningful to a global audience. The text for the title of the document must use title case. Capitalize the first letter of every word except conjunctions and prepositions. Do Don't # Security Architecture # Security architecture # Code of Conduct # Code Of Conduct","title":"Choose the right title"},{"location":"contribute/documentation/style/#use-sentence-case-for-headings","text":"Use sentence case for the headings in your document. Only capitalize the first word of the heading, except for proper nouns or acronyms. Do Don't Configuring rate limits Configuring Rate Limits Using Envoy for ingress Using envoy for ingress Using HTTPS Using https","title":"Use sentence case for headings"},{"location":"contribute/documentation/style/#use-present-tense","text":"Do Don't This command starts a proxy. This command will start a proxy. Exception: Use future or past tense if it is required to convey the correct meaning. This exception is extremely rare and should be avoided.","title":"Use present tense"},{"location":"contribute/documentation/style/#use-active-voice","text":"Do Don't You can explore the API using a browser. The API can be explored using a browser. The YAML file specifies the replica count. The replica count is specified in the YAML file.","title":"Use active voice"},{"location":"contribute/documentation/style/#use-simple-and-direct-language","text":"Use simple and direct language. Avoid using unnecessary phrases, such as saying \"please.\" Do Don't To create a ReplicaSet , ... In order to create a ReplicaSet , ... See the configuration file. Please see the configuration file. View the Pods. With this next command, we'll view the Pods.","title":"Use simple and direct language"},{"location":"contribute/documentation/style/#prefer-shorter-words-over-longer-alternatives","text":"Do Don't This tool helps scaling up pods. This tool facilitates scaling up pods. Pilot uses the purpose field to ... Pilot utilizes the purpose field to ...","title":"Prefer shorter words over longer alternatives"},{"location":"contribute/documentation/style/#address-the-reader-as-you","text":"Do Don't You can create a Deployment by ... We'll create a Deployment by ... In the preceding output, you can see... In the preceding output, we can see ...","title":"Address the reader as \"you\""},{"location":"contribute/documentation/style/#avoid-using-we","text":"Using \"we\" in a sentence can be confusing, because the reader might not know whether they're part of the \"we\" you're describing. Do Don't Version 1.4 includes ... In version 1.4, we have added ... Fybrik provides a new feature for ... We provide a new feature ... This page teaches you how to use pods. In this page, we are going to learn about pods.","title":"Avoid using \"we\""},{"location":"contribute/documentation/style/#avoid-jargon-and-idioms","text":"Some readers speak English as a second language. Avoid jargon and idioms to help make their understanding easier. Do Don't Internally, ... Under the hood, ... Create a new cluster. Turn up a new cluster. Initially, ... Out of the box, ...","title":"Avoid jargon and idioms"},{"location":"contribute/documentation/style/#avoid-statements-that-will-soon-be-out-of-date","text":"Avoid using wording that becomes outdated quickly like \"currently\" and \"new\". A feature that is new today is not new for long. Do Don't In version 1.4, ... In the current version, ... The Federation feature provides ... The new Federation feature provides ...","title":"Avoid statements that will soon be out of date"},{"location":"contribute/documentation/style/#avoid-statements-about-the-future","text":"Avoid making promises or giving hints about the future. If you need to talk about a feature in development, add a clear indication under the front matter that identifies the information accordingly: Warning This page describes a feature that is not yet released The only exceptions to this rule are design or architecture documents that can describe a vision. However, you must clearly distiquish between implemented features and a vision.","title":"Avoid statements about the future"},{"location":"contribute/documentation/style/#create-useful-links","text":"There are good hyperlinks, and bad hyperlinks. The common practice of calling links here or click here are examples of bad hyperlinks. Check out this excellent article explaining what makes a good hyperlink and try to keep these guidelines in mind when creating or reviewing site content.","title":"Create useful links"},{"location":"get-started/OneClickDemo/","text":"One Click Demo This guide contains a script that you can run with a single command, to see a demo of Fybrik in action. The demo demonstrates the following sequence: A kind Kubernetes cluster is installed and Fybrik and all its dependencies are deployed to the cluster. You can also try to do this step by step in the QuickStart segment. A data operator registers a data asset in a data catalog used by Fybrik, and tags it as financial data. You can also try to do this, and the next parts of the demo, step by step in the notebook-read-sample segment. A data governance officer defines data policies, such as which columns contain PII (personally identifiable information), and submits them to Fybrik. A data user submits a request to read the data asset using Fybrik. Fybrik fetches the data asset, and automatically redacts columns according to the data policies. The data user can consume the governed data instantly. Requirements Linux based OS or MacOS with a working bash terminal. Docker installed and deployed . The demo will make a bin folder at your current directory with all the other required dependencies for Fybrik. The Demo We recommend trying Fybrik with its main data catalog OpenMetaData , but it takes ~25 minutes on most machines, so grab a coffee while you wait! Alternatively you can try Fybrik with Katalog, a data catalog stub, strictly used for demos, to see a demo that takes ~5 minutes. Demo with OpenMetaData Demo with Katalog export FYBRIK_VERSION = master ; curl https://raw.githubusercontent.com/fybrik/fybrik/master/samples/OneClickDemo/OneClickDemo-OMD.sh | bash - export FYBRIK_VERSION = master ; curl https://raw.githubusercontent.com/fybrik/fybrik/master/samples/OneClickDemo/OneClickDemo-Katalog.sh | bash - NOTE : At the end of the demo, you will see in your terminal a sample from a table that the data user consumed. one of the columns will display XXXXX instead of values, indicating that it has been automatically redacted due to data policies. Cleanup To stop the local kind kubernetes cluster booted up on your machine in this demo, and to remove the folder created with the dependencies for Fybrik, run this. bin/kind delete cluster --name = kind-fybrik-installation-sample rm -rf bin rm res.out WARNING : If you already had a bin folder at your current directory these commands will delete it and its contents.","title":"OneClickDemo"},{"location":"get-started/OneClickDemo/#one-click-demo","text":"This guide contains a script that you can run with a single command, to see a demo of Fybrik in action. The demo demonstrates the following sequence: A kind Kubernetes cluster is installed and Fybrik and all its dependencies are deployed to the cluster. You can also try to do this step by step in the QuickStart segment. A data operator registers a data asset in a data catalog used by Fybrik, and tags it as financial data. You can also try to do this, and the next parts of the demo, step by step in the notebook-read-sample segment. A data governance officer defines data policies, such as which columns contain PII (personally identifiable information), and submits them to Fybrik. A data user submits a request to read the data asset using Fybrik. Fybrik fetches the data asset, and automatically redacts columns according to the data policies. The data user can consume the governed data instantly.","title":"One Click Demo"},{"location":"get-started/OneClickDemo/#requirements","text":"Linux based OS or MacOS with a working bash terminal. Docker installed and deployed . The demo will make a bin folder at your current directory with all the other required dependencies for Fybrik.","title":"Requirements"},{"location":"get-started/OneClickDemo/#the-demo","text":"We recommend trying Fybrik with its main data catalog OpenMetaData , but it takes ~25 minutes on most machines, so grab a coffee while you wait! Alternatively you can try Fybrik with Katalog, a data catalog stub, strictly used for demos, to see a demo that takes ~5 minutes. Demo with OpenMetaData Demo with Katalog export FYBRIK_VERSION = master ; curl https://raw.githubusercontent.com/fybrik/fybrik/master/samples/OneClickDemo/OneClickDemo-OMD.sh | bash - export FYBRIK_VERSION = master ; curl https://raw.githubusercontent.com/fybrik/fybrik/master/samples/OneClickDemo/OneClickDemo-Katalog.sh | bash - NOTE : At the end of the demo, you will see in your terminal a sample from a table that the data user consumed. one of the columns will display XXXXX instead of values, indicating that it has been automatically redacted due to data policies.","title":"The Demo"},{"location":"get-started/OneClickDemo/#cleanup","text":"To stop the local kind kubernetes cluster booted up on your machine in this demo, and to remove the folder created with the dependencies for Fybrik, run this. bin/kind delete cluster --name = kind-fybrik-installation-sample rm -rf bin rm res.out WARNING : If you already had a bin folder at your current directory these commands will delete it and its contents.","title":"Cleanup"},{"location":"get-started/quickstart/","text":"Quick Start Guide Follow this guide to install Fybrik using default parameters that are suitable for experimentation on a single cluster. For a One Click Demo of Fybrik and a read data scenario, refer to OneClickDemo . Before you begin Ensure that you have the following: Helm 3.7.0 or greater must be installed and configured on your machine. Kubectl 1.23 or newer must be installed on your machine. Access to a Kubernetes cluster such as Kind as a cluster administrator. Kubernetes version support range is 1.23 - 1.25 although older versions may work well. Add required Helm repositories helm repo add jetstack https://charts.jetstack.io helm repo add hashicorp https://helm.releases.hashicorp.com helm repo add fybrik-charts https://fybrik.github.io/charts helm repo update Install cert-manager Fybrik requires cert-manager to be installed to your cluster 1 . Many clusters already include cert-manager. Check if cert-manager namespace exists in your cluster and only run the following if it doesn't exist: helm install cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --version v1.6.2 \\ --create-namespace \\ --set installCRDs = true \\ --wait --timeout 120s Install Hashicorp Vault and plugins Hashicorp Vault and a secrets-kubernetes-reader plugin are used by Fybrik for credential management. Install the latest development version from GitHub The published Helm charts are only available for released versions. To install the dev version install the charts from the source code. For example: Kubernetes OpenShift git clone https://github.com/fybrik/fybrik.git cd fybrik helm dependency update charts/vault helm install vault charts/vault --create-namespace -n fybrik-system \\ --set \"vault.injector.enabled=false\" \\ --set \"vault.server.dev.enabled=true\" \\ --values charts/vault/env/dev/vault-single-cluster-values.yaml kubectl wait --for = condition = ready --all pod -n fybrik-system --timeout = 120s git clone https://github.com/fybrik/fybrik.git cd fybrik helm dependency update charts/vault helm install vault charts/vault --create-namespace -n fybrik-system \\ --set \"vault.injector.enabled=false\" \\ --set \"vault.server.dev.enabled=true\" \\ --values charts/vault/env/dev/vault-single-cluster-values.yaml \\ --values charts/vault/vault-openshift-values.yaml kubectl wait --for = condition = ready --all pod -n fybrik-system --timeout = 120s Run the following to install vault and the plugin in development mode: Kubernetes OpenShift helm install vault fybrik-charts/vault --create-namespace -n fybrik-system \\ --set \"vault.injector.enabled=false\" \\ --set \"vault.server.dev.enabled=true\" \\ --values https://raw.githubusercontent.com/fybrik/fybrik/master/charts/vault/env/dev/vault-single-cluster-values.yaml kubectl wait --for = condition = ready --all pod -n fybrik-system --timeout = 120s helm install vault fybrik-charts/vault --create-namespace -n fybrik-system \\ --set \"vault.injector.enabled=false\" \\ --set \"vault.server.dev.enabled=true\" \\ --values https://raw.githubusercontent.com/fybrik/fybrik/master/charts/vault/env/dev/vault-single-cluster-values.yaml \\ --values https://raw.githubusercontent.com/fybrik/fybrik/master/charts/vault/vault-openshift-values.yaml kubectl wait --for = condition = ready --all pod -n fybrik-system --timeout = 120s Install data catalog Fybrik assumes the existence of a data catalog that contains the metadata and connection information for data assets. Fybrik currently supports: OpenMetadata : An open-source end-to-end metadata management solution that includes data discovery, governance, data quality, observability, collaboration, and lineage. Katalog : a data catalog stub used for testing and evaluation purposes. If you plan to use Katalog , you can skip to the next section , but keep in mind that Katalog is mostly suitable for development and testing. To use OpenMetadata, you can either use an existing deployment, or run the following commands to deploy OpenMetadata in kubernetes. Note: OpenMetadata deployment requires a cluster storage provisioner that has PersistentVolume capability of ReadWriteMany Access Mode. Below we provide examples of OpenMetadata installations on a single node kind cluster (for development and testing) and an OpenShift cluster on IBM Cloud . For other deployments please check OpenMetadata Kubernetes deployment A single node Kind cluster IBM OpenShift Existing deployment export FYBRIK_BRANCH = master curl https://raw.githubusercontent.com/fybrik/fybrik/master/third_party/openmetadata/install_OM.sh | bash - The installation of OpenMetadata could take a long time (around 20 minutes on a VM running kind Kubernetes). Alternatively, if you want to change the OpenMetadata configuration parameters, run: export FYBRIK_BRANCH = master curl https://raw.githubusercontent.com/fybrik/fybrik/master/third_party/openmetadata/install_OM.sh | bash -s -- --operation getFiles This command downloads the installation files to a temporary directory. Follow the instructions that appear on screen to change the configuration parameters and then run make . Once the installation is over, be sure to remove the temporary directory. export FYBRIK_BRANCH = master curl https://raw.githubusercontent.com/fybrik/fybrik/master/third_party/openmetadata/install_OM.sh | bash -s -- --k8s-type ibm-openshift The installation of OpenMetadata could take a long time (around 20 minutes on a VM running kind Kubernetes). Alternatively, if you want to change the OpenMetadata configuration parameters, run: export FYBRIK_BRANCH = master curl https://raw.githubusercontent.com/fybrik/fybrik/master/third_party/openmetadata/install_OM.sh | bash -s -- --k8s-type ibm-openshift --operation getFiles This command downloads the installation files to a temporary directory. Follow the instructions that appear on screen to change the configuration parameters and then run make . Once the installation is over, be sure to remove the temporary directory. If you want to use an existing OpenMetadata deployment, you have to configure it according to Fybrik requirements: Run the following commands to download the configuration files: export FYBRIK_BRANCH = master curl https://raw.githubusercontent.com/fybrik/fybrik/master/third_party/openmetadata/install_OM.sh | bash -s -- --operation getFiles Follow the instructions that appear on screen to change the OpenMetadata location and credentials ( OPENMETADATA_ENDPOINT , OPENMETADATA_USER and OPENMETADATA_PASSWORD ) and then run make prepare-openmetadata-for-fybrik . Running make installs OpenMetadata in the open-metadata namespace. To install OpenMetadata in another namespace, or to change the credentials of the different services used by OpenMetadata, edit the variables in the Makefile.env file. Install control plane The control plane includes a manager service that connects to a data catalog and to a policy manager. With OpenMetadata With Katalog Install the Fybrik release with OpenMetadata as the data catalog and with Open Policy Agent as the policy manager. NOTE : When installing fybrik with OpenMetadata as its data catalog, you need to specify the API endpoint for OpenMetadata. The default value for that endpoint is http://openmetadata.open-metadata:8585/api . If you are using a different OpenMetadata deployment, replace the openmetadataConnector.openmetadata_endpoint value in the helm installation command. The published Helm charts are only available for released versions. To install the dev version, please install the charts from the source code. git clone https://github.com/fybrik/fybrik.git cd fybrik helm install fybrik-crd charts/fybrik-crd -n fybrik-system --wait helm install fybrik charts/fybrik --set global.tag = master --set coordinator.catalog = openmetadata --set openmetadataConnector.openmetadata_endpoint = http://openmetadata.open-metadata:8585/api -n fybrik-system --wait Install the Fybrik release with Katalog as the data catalog and with Open Policy Agent as the policy manager. The published Helm charts are only available for released versions. To install the dev version, please install the charts from the source code. git clone https://github.com/fybrik/fybrik.git cd fybrik helm install fybrik-crd charts/fybrik-crd -n fybrik-system --wait helm install fybrik charts/fybrik --set global.tag = master --set coordinator.catalog = katalog -n fybrik-system --wait Install modules Install the latest development version from GitHub To apply the latest development version of arrow-flight-module: kubectl apply -f https://raw.githubusercontent.com/fybrik/arrow-flight-module/master/module.yaml -n fybrik-system Modules are plugins that the control plane deploys whenever required. The arrow flight module enables reading data through Apache Arrow Flight API. Install the latest 2 release of arrow-flight-module: kubectl apply -f https://github.com/fybrik/arrow-flight-module/releases/latest/download/module.yaml -n fybrik-system Fybrik version 0.6.0 and lower should use cert-manager 1.2.0 \u21a9 Refer to the documentation of arrow-flight-module for other versions \u21a9","title":"Quickstart"},{"location":"get-started/quickstart/#quick-start-guide","text":"Follow this guide to install Fybrik using default parameters that are suitable for experimentation on a single cluster. For a One Click Demo of Fybrik and a read data scenario, refer to OneClickDemo .","title":"Quick Start Guide"},{"location":"get-started/quickstart/#before-you-begin","text":"Ensure that you have the following: Helm 3.7.0 or greater must be installed and configured on your machine. Kubectl 1.23 or newer must be installed on your machine. Access to a Kubernetes cluster such as Kind as a cluster administrator. Kubernetes version support range is 1.23 - 1.25 although older versions may work well.","title":"Before you begin"},{"location":"get-started/quickstart/#add-required-helm-repositories","text":"helm repo add jetstack https://charts.jetstack.io helm repo add hashicorp https://helm.releases.hashicorp.com helm repo add fybrik-charts https://fybrik.github.io/charts helm repo update","title":"Add required Helm repositories"},{"location":"get-started/quickstart/#install-cert-manager","text":"Fybrik requires cert-manager to be installed to your cluster 1 . Many clusters already include cert-manager. Check if cert-manager namespace exists in your cluster and only run the following if it doesn't exist: helm install cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --version v1.6.2 \\ --create-namespace \\ --set installCRDs = true \\ --wait --timeout 120s","title":"Install cert-manager"},{"location":"get-started/quickstart/#install-hashicorp-vault-and-plugins","text":"Hashicorp Vault and a secrets-kubernetes-reader plugin are used by Fybrik for credential management. Install the latest development version from GitHub The published Helm charts are only available for released versions. To install the dev version install the charts from the source code. For example: Kubernetes OpenShift git clone https://github.com/fybrik/fybrik.git cd fybrik helm dependency update charts/vault helm install vault charts/vault --create-namespace -n fybrik-system \\ --set \"vault.injector.enabled=false\" \\ --set \"vault.server.dev.enabled=true\" \\ --values charts/vault/env/dev/vault-single-cluster-values.yaml kubectl wait --for = condition = ready --all pod -n fybrik-system --timeout = 120s git clone https://github.com/fybrik/fybrik.git cd fybrik helm dependency update charts/vault helm install vault charts/vault --create-namespace -n fybrik-system \\ --set \"vault.injector.enabled=false\" \\ --set \"vault.server.dev.enabled=true\" \\ --values charts/vault/env/dev/vault-single-cluster-values.yaml \\ --values charts/vault/vault-openshift-values.yaml kubectl wait --for = condition = ready --all pod -n fybrik-system --timeout = 120s Run the following to install vault and the plugin in development mode: Kubernetes OpenShift helm install vault fybrik-charts/vault --create-namespace -n fybrik-system \\ --set \"vault.injector.enabled=false\" \\ --set \"vault.server.dev.enabled=true\" \\ --values https://raw.githubusercontent.com/fybrik/fybrik/master/charts/vault/env/dev/vault-single-cluster-values.yaml kubectl wait --for = condition = ready --all pod -n fybrik-system --timeout = 120s helm install vault fybrik-charts/vault --create-namespace -n fybrik-system \\ --set \"vault.injector.enabled=false\" \\ --set \"vault.server.dev.enabled=true\" \\ --values https://raw.githubusercontent.com/fybrik/fybrik/master/charts/vault/env/dev/vault-single-cluster-values.yaml \\ --values https://raw.githubusercontent.com/fybrik/fybrik/master/charts/vault/vault-openshift-values.yaml kubectl wait --for = condition = ready --all pod -n fybrik-system --timeout = 120s","title":"Install Hashicorp Vault and plugins"},{"location":"get-started/quickstart/#install-data-catalog","text":"Fybrik assumes the existence of a data catalog that contains the metadata and connection information for data assets. Fybrik currently supports: OpenMetadata : An open-source end-to-end metadata management solution that includes data discovery, governance, data quality, observability, collaboration, and lineage. Katalog : a data catalog stub used for testing and evaluation purposes. If you plan to use Katalog , you can skip to the next section , but keep in mind that Katalog is mostly suitable for development and testing. To use OpenMetadata, you can either use an existing deployment, or run the following commands to deploy OpenMetadata in kubernetes. Note: OpenMetadata deployment requires a cluster storage provisioner that has PersistentVolume capability of ReadWriteMany Access Mode. Below we provide examples of OpenMetadata installations on a single node kind cluster (for development and testing) and an OpenShift cluster on IBM Cloud . For other deployments please check OpenMetadata Kubernetes deployment A single node Kind cluster IBM OpenShift Existing deployment export FYBRIK_BRANCH = master curl https://raw.githubusercontent.com/fybrik/fybrik/master/third_party/openmetadata/install_OM.sh | bash - The installation of OpenMetadata could take a long time (around 20 minutes on a VM running kind Kubernetes). Alternatively, if you want to change the OpenMetadata configuration parameters, run: export FYBRIK_BRANCH = master curl https://raw.githubusercontent.com/fybrik/fybrik/master/third_party/openmetadata/install_OM.sh | bash -s -- --operation getFiles This command downloads the installation files to a temporary directory. Follow the instructions that appear on screen to change the configuration parameters and then run make . Once the installation is over, be sure to remove the temporary directory. export FYBRIK_BRANCH = master curl https://raw.githubusercontent.com/fybrik/fybrik/master/third_party/openmetadata/install_OM.sh | bash -s -- --k8s-type ibm-openshift The installation of OpenMetadata could take a long time (around 20 minutes on a VM running kind Kubernetes). Alternatively, if you want to change the OpenMetadata configuration parameters, run: export FYBRIK_BRANCH = master curl https://raw.githubusercontent.com/fybrik/fybrik/master/third_party/openmetadata/install_OM.sh | bash -s -- --k8s-type ibm-openshift --operation getFiles This command downloads the installation files to a temporary directory. Follow the instructions that appear on screen to change the configuration parameters and then run make . Once the installation is over, be sure to remove the temporary directory. If you want to use an existing OpenMetadata deployment, you have to configure it according to Fybrik requirements: Run the following commands to download the configuration files: export FYBRIK_BRANCH = master curl https://raw.githubusercontent.com/fybrik/fybrik/master/third_party/openmetadata/install_OM.sh | bash -s -- --operation getFiles Follow the instructions that appear on screen to change the OpenMetadata location and credentials ( OPENMETADATA_ENDPOINT , OPENMETADATA_USER and OPENMETADATA_PASSWORD ) and then run make prepare-openmetadata-for-fybrik . Running make installs OpenMetadata in the open-metadata namespace. To install OpenMetadata in another namespace, or to change the credentials of the different services used by OpenMetadata, edit the variables in the Makefile.env file.","title":"Install data catalog"},{"location":"get-started/quickstart/#install-control-plane","text":"The control plane includes a manager service that connects to a data catalog and to a policy manager. With OpenMetadata With Katalog Install the Fybrik release with OpenMetadata as the data catalog and with Open Policy Agent as the policy manager. NOTE : When installing fybrik with OpenMetadata as its data catalog, you need to specify the API endpoint for OpenMetadata. The default value for that endpoint is http://openmetadata.open-metadata:8585/api . If you are using a different OpenMetadata deployment, replace the openmetadataConnector.openmetadata_endpoint value in the helm installation command. The published Helm charts are only available for released versions. To install the dev version, please install the charts from the source code. git clone https://github.com/fybrik/fybrik.git cd fybrik helm install fybrik-crd charts/fybrik-crd -n fybrik-system --wait helm install fybrik charts/fybrik --set global.tag = master --set coordinator.catalog = openmetadata --set openmetadataConnector.openmetadata_endpoint = http://openmetadata.open-metadata:8585/api -n fybrik-system --wait Install the Fybrik release with Katalog as the data catalog and with Open Policy Agent as the policy manager. The published Helm charts are only available for released versions. To install the dev version, please install the charts from the source code. git clone https://github.com/fybrik/fybrik.git cd fybrik helm install fybrik-crd charts/fybrik-crd -n fybrik-system --wait helm install fybrik charts/fybrik --set global.tag = master --set coordinator.catalog = katalog -n fybrik-system --wait","title":"Install control plane"},{"location":"get-started/quickstart/#install-modules","text":"Install the latest development version from GitHub To apply the latest development version of arrow-flight-module: kubectl apply -f https://raw.githubusercontent.com/fybrik/arrow-flight-module/master/module.yaml -n fybrik-system Modules are plugins that the control plane deploys whenever required. The arrow flight module enables reading data through Apache Arrow Flight API. Install the latest 2 release of arrow-flight-module: kubectl apply -f https://github.com/fybrik/arrow-flight-module/releases/latest/download/module.yaml -n fybrik-system Fybrik version 0.6.0 and lower should use cert-manager 1.2.0 \u21a9 Refer to the documentation of arrow-flight-module for other versions \u21a9","title":"Install modules"},{"location":"reference/crds/","text":"API Reference Packages: app.fybrik.io/v1beta1 app.fybrik.io/v1beta2 katalog.fybrik.io/v1alpha1 app.fybrik.io/v1beta1 Resource Types: Blueprint FybrikApplication FybrikModule FybrikStorageAccount Plotter Blueprint \u21a9 Parent Blueprint is the Schema for the blueprints API Name Type Description Required apiVersion string app.fybrik.io/v1beta1 true kind string Blueprint true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object BlueprintSpec defines the desired state of Blueprint, which defines the components of the workload's data path that run in a particular cluster. In a single cluster environment there is one blueprint per workload (FybrikApplication). In a multi-cluster environment there is one Blueprint per cluster per workload (FybrikApplication). true status object BlueprintStatus defines the observed state of Blueprint This includes readiness, error message, and indicators for the Kubernetes resources owned by the Blueprint for cleanup and status monitoring false Blueprint.spec \u21a9 Parent BlueprintSpec defines the desired state of Blueprint, which defines the components of the workload's data path that run in a particular cluster. In a single cluster environment there is one blueprint per workload (FybrikApplication). In a multi-cluster environment there is one Blueprint per cluster per workload (FybrikApplication). Name Type Description Required cluster string Cluster indicates the cluster on which the Blueprint runs true modules map[string]object Modules is a map which contains modules that indicate the data path components that run in this cluster The map key is moduleInstanceName which is the unique name for the deployed instance related to this workload true modulesNamespace string ModulesNamespace is the namespace where modules should be allocated true application object ApplicationContext is a context of the origin FybrikApplication (labels, properties, etc.) false Blueprint.spec.modules[key] \u21a9 Parent BlueprintModule is a copy of a FybrikModule Custom Resource. It contains the information necessary to instantiate a datapath component, including the parameters relevant for the particular workload. Name Type Description Required chart object Chart contains the location of the helm chart with info detailing how to deploy true name string Name of the FybrikModule on which this is based true arguments object Arguments are the input parameters for a specific instance of a module. false assetIds []string assetIDs indicate the assets processed by this module. Included so we can track asset status as well as module status in the future. false network object Network specifies the module communication with a workload or other modules false Blueprint.spec.modules[key].chart \u21a9 Parent Chart contains the location of the helm chart with info detailing how to deploy Name Type Description Required name string Name of helm chart true chartPullSecret string Name of secret containing helm registry credentials false values map[string]string Values to pass to helm chart installation false Blueprint.spec.modules[key].arguments \u21a9 Parent Arguments are the input parameters for a specific instance of a module. Name Type Description Required assets []object Assets define asset related arguments, such as data source, transformations, etc. false Blueprint.spec.modules[key].arguments.assets[index] \u21a9 Parent AssetContext defines the input parameters for modules that access an asset Name Type Description Required assetID string AssetID identifies the asset to be used for accessing the data when it is ready It is copied from the FybrikApplication resource true capability string Capability of the module true args []object List of datastores associated with the asset false transformations []object Transformations are different types of processing that may be done to the data as it is copied. false Blueprint.spec.modules[key].arguments.assets[index].args[index] \u21a9 Parent DataStore contains the details for accessing the data that are sent by catalog connectors Credentials for accessing the data are stored in Vault, in the location represented by Vault property. Name Type Description Required connection object Connection has the relevant details for accessing the data (url, table, ssl, etc.) true format string Format represents data format (e.g. parquet) as received from catalog connectors false vault map[string]object Holds details for retrieving credentials by the modules from Vault store. It is a map so that different credentials can be stored for the different DataFlow operations. false Blueprint.spec.modules[key].arguments.assets[index].args[index].connection \u21a9 Parent Connection has the relevant details for accessing the data (url, table, ssl, etc.) Name Type Description Required name string Name of the connection to the data source true Blueprint.spec.modules[key].arguments.assets[index].args[index].vault[key] \u21a9 Parent Holds details for retrieving credentials from Vault store. Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true Blueprint.spec.modules[key].arguments.assets[index].transformations[index] \u21a9 Parent Action to be performed on the data, e.g., masking Name Type Description Required name string Action name true Blueprint.spec.modules[key].network \u21a9 Parent Network specifies the module communication with a workload or other modules Name Type Description Required egress []object Egress (internal modules) false endpoint boolean Endpoint indicates whether the module service is used as an endpoint by the workload application false ingress []object Ingress (internal modules) false urls []string External services and datasets in the form of hostname + port or a hostname only (e.g., s3 endpoint), or a CIDR (Classless Inter-Domain Routing) with optional port false Blueprint.spec.modules[key].network.egress[index] \u21a9 Parent ModuleDeployment specifies deployment of a Fybrik module Name Type Description Required cluster string Cluster name true release string Release name true urls []string Service URLs, usually represented by hostname + port true Blueprint.spec.modules[key].network.ingress[index] \u21a9 Parent ModuleDeployment specifies deployment of a Fybrik module Name Type Description Required cluster string Cluster name true release string Release name true urls []string Service URLs, usually represented by hostname + port true Blueprint.spec.application \u21a9 Parent ApplicationContext is a context of the origin FybrikApplication (labels, properties, etc.) Name Type Description Required context object Application context such as intent, role, etc. false ipBlocks []object IPBlocks define policy on particular IPBlocks. the structure of the IPBlock is defined at https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#ipblock-v1-networking-k8s-io It is obtained from FybrikApplication spec. false namespaces []string Namespaces where user application might run It is obtained from FybrikApplication spec. false selector object Application selector is used to identify the user workload. It is obtained from FybrikApplication spec. false Blueprint.spec.application.ipBlocks[index] \u21a9 Parent IPBlock describes a particular CIDR (Ex. \"192.168.1.1/24\",\"2001:db9::/64\") that is allowed to the pods matched by a NetworkPolicySpec's podSelector. The except entry describes CIDRs that should not be included within this rule. Name Type Description Required cidr string CIDR is a string representing the IP Block Valid examples are \"192.168.1.1/24\" or \"2001:db9::/64\" true except []string Except is a slice of CIDRs that should not be included within an IP Block Valid examples are \"192.168.1.1/24\" or \"2001:db9::/64\" Except values will be rejected if they are outside the CIDR range false Blueprint.spec.application.selector \u21a9 Parent Application selector is used to identify the user workload. It is obtained from FybrikApplication spec. Name Type Description Required matchExpressions []object matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false Blueprint.spec.application.selector.matchExpressions[index] \u21a9 Parent A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. Name Type Description Required key string key is the label key that the selector applies to. true operator string operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false Blueprint.status \u21a9 Parent BlueprintStatus defines the observed state of Blueprint This includes readiness, error message, and indicators for the Kubernetes resources owned by the Blueprint for cleanup and status monitoring Name Type Description Required modules map[string]object ModulesState is a map which holds the status of each module its key is the moduleInstanceName which is the unique name for the deployed instance related to this workload false observedGeneration integer ObservedGeneration is taken from the Blueprint metadata. This is used to determine during reconcile whether reconcile was called because the desired state changed, or whether status of the allocated resources should be checked. Format : int64 false observedState object ObservedState includes information to be reported back to the FybrikApplication resource It includes readiness and error indications, as well as user instructions false releases map[string]integer Releases map each release to the observed generation of the blueprint containing this release. At the end of reconcile, each release should be mapped to the latest blueprint version or be uninstalled. false Blueprint.status.modules[key] \u21a9 Parent ObservedState represents a part of the generated Blueprint/Plotter resource status that allows update of FybrikApplication status Name Type Description Required error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false Blueprint.status.observedState \u21a9 Parent ObservedState includes information to be reported back to the FybrikApplication resource It includes readiness and error indications, as well as user instructions Name Type Description Required error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false FybrikApplication \u21a9 Parent FybrikApplication provides information about the application whose data is being operated on, the nature of the processing, and the data sets chosen for processing by the application. The FybrikApplication controller obtains instructions regarding any governance related changes that must be performed on the data, identifies the modules capable of performing such changes, and finally generates the Plotter which defines the secure runtime environment and all the components in it. This runtime environment provides the application with access to the data requested in a secure manner and without having to provide any credentials for the data sets. The credentials are obtained automatically by the manager from the credential management system. Name Type Description Required apiVersion string app.fybrik.io/v1beta1 true kind string FybrikApplication true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object FybrikApplicationSpec defines data flows needed by the application, the purpose and other contextual information about the application. true status object FybrikApplicationStatus defines the observed state of FybrikApplication. false FybrikApplication.spec \u21a9 Parent FybrikApplicationSpec defines data flows needed by the application, the purpose and other contextual information about the application. Name Type Description Required appInfo object AppInfo contains information describing the reasons for the processing that will be done by the application. true data []object Data contains the identifiers of the data to be used by the Data Scientist's application, and the protocol used to access it and the format expected. true secretRef string SecretRef points to the secret that holds credentials for each system the user has been authenticated with. The secret is deployed in FybrikApplication namespace. false selector object Selector enables to connect the resource to the application Application labels should match the labels in the selector. false FybrikApplication.spec.data[index] \u21a9 Parent DataContext indicates data set being processed by the workload and includes information about the data format and technologies used to access the data. Name Type Description Required dataSetID string DataSetID is a unique identifier of the dataset chosen from the data catalog. For data catalogs that support multiple sub-catalogs, it includes the catalog id and the dataset id. When writing a new dataset it is the name provided by the user or workload generating it. true requirements object Requirements from the system true flow enum Flows indicates what is being done with the particular dataset - ex: read, write, copy (ingest), delete This is optional for the purpose of backward compatibility. If nothing is provided, read is assumed. Enum : read, write, delete, copy false FybrikApplication.spec.data[index].requirements \u21a9 Parent Requirements from the system Name Type Description Required flowParams object FlowParams include the requirements for particular data flows false interface object Interface indicates the protocol and format expected by the data user false FybrikApplication.spec.data[index].requirements.flowParams \u21a9 Parent FlowParams include the requirements for particular data flows Name Type Description Required catalog string Catalog indicates that the data asset must be cataloged, and in which catalog to register it false isNewDataSet boolean IsNewDataSet if true indicates that the DataContext.DataSetID is user provided and not a full catalog / dataset ID. Relevant when writing. A unique ID from the catalog will be provided in the FybrikApplication Status after a new catalog entry is created. false metadata object Source asset metadata like asset name, owner, geography, etc Relevant when writing new asset. false storageEstimate integer Storage estimate indicates the estimated amount of storage in MB, GB, TB required when writing new data. Format : int64 false FybrikApplication.spec.data[index].requirements.flowParams.metadata \u21a9 Parent Source asset metadata like asset name, owner, geography, etc Relevant when writing new asset. Name Type Description Required columns []object Columns associated with the asset false geography string Geography of the resource false name string Name of the resource false owner string Owner of the resource false tags object Tags associated with the asset false FybrikApplication.spec.data[index].requirements.flowParams.metadata.columns[index] \u21a9 Parent ResourceColumn represents a column in a tabular resource Name Type Description Required name string Name of the column true tags object Tags associated with the column false FybrikApplication.spec.data[index].requirements.interface \u21a9 Parent Interface indicates the protocol and format expected by the data user Name Type Description Required protocol string Connection type, e.g., S3, Kafka, MySQL true dataformat string DataFormat defines the data format type false FybrikApplication.spec.selector \u21a9 Parent Selector enables to connect the resource to the application Application labels should match the labels in the selector. Name Type Description Required workloadSelector object WorkloadSelector enables to connect the resource to a user application. Application labels should match the labels in the selector. true clusterName string Cluster name false ipBlocks []object IPBlocks define policy on particular IPBlocks. the structure of the IPBlock is defined at https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#ipblock-v1-networking-k8s-io false namespaces []string Namespaces where user application might run false FybrikApplication.spec.selector.workloadSelector \u21a9 Parent WorkloadSelector enables to connect the resource to a user application. Application labels should match the labels in the selector. Name Type Description Required matchExpressions []object matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false FybrikApplication.spec.selector.workloadSelector.matchExpressions[index] \u21a9 Parent A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. Name Type Description Required key string key is the label key that the selector applies to. true operator string operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false FybrikApplication.spec.selector.ipBlocks[index] \u21a9 Parent IPBlock describes a particular CIDR (Ex. \"192.168.1.1/24\",\"2001:db9::/64\") that is allowed to the pods matched by a NetworkPolicySpec's podSelector. The except entry describes CIDRs that should not be included within this rule. Name Type Description Required cidr string CIDR is a string representing the IP Block Valid examples are \"192.168.1.1/24\" or \"2001:db9::/64\" true except []string Except is a slice of CIDRs that should not be included within an IP Block Valid examples are \"192.168.1.1/24\" or \"2001:db9::/64\" Except values will be rejected if they are outside the CIDR range false FybrikApplication.status \u21a9 Parent FybrikApplicationStatus defines the observed state of FybrikApplication. Name Type Description Required assetStates map[string]object AssetStates provides a status per asset false errorMessage string ErrorMessage indicates that an error has happened during the reconcile, unrelated to a specific asset false generated object Generated resource identifier false observedGeneration integer ObservedGeneration is taken from the FybrikApplication metadata. This is used to determine during reconcile whether reconcile was called because the desired state changed, or whether the Blueprint status changed. Format : int64 false provisionedStorage map[string]object ProvisionedStorage maps a dataset (identified by AssetID) to the new provisioned bucket. It allows FybrikApplication controller to manage buckets in case the spec has been modified, an error has occurred, or a delete event has been received. ProvisionedStorage has the information required to register the dataset once the owned plotter resource is ready false ready boolean Ready is true if all specified assets are either ready to be used or are denied access. false validApplication string ValidApplication indicates whether the FybrikApplication is valid given the defined taxonomy false validatedGeneration integer ValidatedGeneration is the version of the FyrbikApplication that has been validated with the taxonomy defined. Format : int64 false FybrikApplication.status.assetStates[key] \u21a9 Parent AssetState defines the observed state of an asset Name Type Description Required catalogedAsset string CatalogedAsset provides a new asset identifier after being registered in the enterprise catalog false conditions []object Conditions indicate the asset state (Ready, Deny, Error) false endpoint object Endpoint provides the endpoint spec from which the asset will be served to the application false FybrikApplication.status.assetStates[key].conditions[index] \u21a9 Parent Condition describes the state of a FybrikApplication at a certain point. Name Type Description Required type string Type of the condition true message string Message contains the details of the current condition false observedGeneration integer ObservedGeneration is the version of the resource for which the condition has been evaluated Format : int64 false status enum Status of the condition, one of (`True`, `False`, `Unknown`). Enum : True, False, Unknown Default : Unknown false FybrikApplication.status.assetStates[key].endpoint \u21a9 Parent Endpoint provides the endpoint spec from which the asset will be served to the application Name Type Description Required name string Name of the connection to the data source true FybrikApplication.status.generated \u21a9 Parent Generated resource identifier Name Type Description Required appVersion integer Version of FybrikApplication that has generated this resource Format : int64 true kind string Kind of the resource (Blueprint, Plotter) true name string Resource name true namespace string Resource namespace true FybrikApplication.status.provisionedStorage[key] \u21a9 Parent DatasetDetails holds details of the provisioned storage Name Type Description Required datasetRef string deprecated false details object Dataset information false persistent boolean Persistent storage (not to be removed after FybrikApplication is deleted) false resourceMetadata object Resource Metadata false secretRef object Reference to a secret where the credentials are stored false FybrikApplication.status.provisionedStorage[key].details \u21a9 Parent Dataset information Name Type Description Required connection object Connection has the relevant details for accessing the data (url, table, ssl, etc.) true format string Format represents data format (e.g. parquet) as received from catalog connectors false vault map[string]object Holds details for retrieving credentials by the modules from Vault store. It is a map so that different credentials can be stored for the different DataFlow operations. false FybrikApplication.status.provisionedStorage[key].details.connection \u21a9 Parent Connection has the relevant details for accessing the data (url, table, ssl, etc.) Name Type Description Required name string Name of the connection to the data source true FybrikApplication.status.provisionedStorage[key].details.vault[key] \u21a9 Parent Holds details for retrieving credentials from Vault store. Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true FybrikApplication.status.provisionedStorage[key].resourceMetadata \u21a9 Parent Resource Metadata Name Type Description Required columns []object Columns associated with the asset false geography string Geography of the resource false name string Name of the resource false owner string Owner of the resource false tags object Tags associated with the asset false FybrikApplication.status.provisionedStorage[key].resourceMetadata.columns[index] \u21a9 Parent ResourceColumn represents a column in a tabular resource Name Type Description Required name string Name of the column true tags object Tags associated with the column false FybrikApplication.status.provisionedStorage[key].secretRef \u21a9 Parent Reference to a secret where the credentials are stored Name Type Description Required name string Name true namespace string Namespace true FybrikModule \u21a9 Parent FybrikModule is a description of an injectable component. the parameters it requires, as well as the specification of how to instantiate such a component. It is used as metadata only. There is no status nor reconciliation. Name Type Description Required apiVersion string app.fybrik.io/v1beta1 true kind string FybrikModule true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object FybrikModuleSpec contains the info common to all modules, which are one of the components that process, load, write, audit, monitor the data used by the data scientist's application. true status object FybrikModuleStatus defines the observed state of FybrikModule. false FybrikModule.spec \u21a9 Parent FybrikModuleSpec contains the info common to all modules, which are one of the components that process, load, write, audit, monitor the data used by the data scientist's application. Name Type Description Required capabilities []object Capabilities declares what this module knows how to do and the types of data it knows how to handle The key to the map is a CapabilityType string true chart object Reference to a Helm chart that allows deployment of the resources required for this module true type string May be one of service, config or plugin Service: Means that the control plane deploys the component that performs the capability Config: Another pre-installed service performs the capability and the module deployed configures it for the particular workload or dataset Plugin: Indicates that this module performs a capability as part of another service or module rather than as a stand-alone module true dependencies []object Other components that must be installed in order for this module to work false description string An explanation of what this module does false externalServices []string External services that are required for functionality of the module, format of the strings might be: be a URL (with or without schema) or a host name with or without port, or a CIDR (Classless Inter-Domain Routing) with optional port number separated by a colon false pluginType string Plugin type indicates the plugin technology used to invoke the capabilities Ex: vault, fybrik-wasm... Should be provided if type is plugin false statusIndicators []object StatusIndicators allow checking status of a non-standard resource that can not be computed by helm/kstatus false FybrikModule.spec.capabilities[index] \u21a9 Parent Capability declares what this module knows how to do and the types of data it knows how to handle Name Type Description Required capability string Capability declares what this module knows how to do - ex: read, write, transform... true actions []object Actions are the data transformations that the module supports false api object API indicates to the application how to access the capabilities provided by the module false plugins []object Plugins enable the module to add libraries to perform actions rather than implementing them by itself false scope enum Scope indicates at what level the capability is used: workload, asset, cluster If not indicated it is assumed to be asset Enum : asset, workload, cluster false supportedInterfaces []object Copy should have one or more instances in the list, and its content should have source and sink Read should have one or more instances in the list, each with source populated Write should have one or more instances in the list, each with sink populated This field may not be required if not handling data false FybrikModule.spec.capabilities[index].actions[index] \u21a9 Parent Name Type Description Required name string Unique name of an action supported by the module true FybrikModule.spec.capabilities[index].api \u21a9 Parent API indicates to the application how to access the capabilities provided by the module Name Type Description Required connection object Connection information true dataFormat string Data format false FybrikModule.spec.capabilities[index].api.connection \u21a9 Parent Connection information Name Type Description Required name string Name of the connection to the data source true FybrikModule.spec.capabilities[index].plugins[index] \u21a9 Parent Name Type Description Required dataFormat string DataFormat indicates the format of data the plugin knows how to process true pluginType string PluginType indicates the technology used for the module and the plugin to interact The values supported should come from the module taxonomy Examples of such mechanisms are vault plugins, wasm, etc true FybrikModule.spec.capabilities[index].supportedInterfaces[index] \u21a9 Parent ModuleInOut specifies the protocol and format of the data input and output by the module - if any Name Type Description Required sink object Sink specifies the output data protocol and format false source object Source specifies the input data protocol and format false FybrikModule.spec.capabilities[index].supportedInterfaces[index].sink \u21a9 Parent Sink specifies the output data protocol and format Name Type Description Required protocol string Connection type, e.g., S3, Kafka, MySQL true dataformat string DataFormat defines the data format type false FybrikModule.spec.capabilities[index].supportedInterfaces[index].source \u21a9 Parent Source specifies the input data protocol and format Name Type Description Required protocol string Connection type, e.g., S3, Kafka, MySQL true dataformat string DataFormat defines the data format type false FybrikModule.spec.chart \u21a9 Parent Reference to a Helm chart that allows deployment of the resources required for this module Name Type Description Required name string Name of helm chart true chartPullSecret string Name of secret containing helm registry credentials false values map[string]string Values to pass to helm chart installation false FybrikModule.spec.dependencies[index] \u21a9 Parent Dependency details another component on which this module relies - i.e. a pre-requisit Name Type Description Required name string Name is the name of the dependent component true type enum Type provides information used in determining how to instantiate the component Enum : module, connector, feature true FybrikModule.spec.statusIndicators[index] \u21a9 Parent ResourceStatusIndicator is used to determine the status of an orchestrated resource Name Type Description Required kind string Kind provides information about the resource kind true successCondition string SuccessCondition specifies a condition that indicates that the resource is ready It uses kubernetes label selection syntax (https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/) true errorMessage string ErrorMessage specifies the resource field to check for an error, e.g. status.errorMsg false failureCondition string FailureCondition specifies a condition that indicates the resource failure It uses kubernetes label selection syntax (https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/) false FybrikModule.status \u21a9 Parent FybrikModuleStatus defines the observed state of FybrikModule. Name Type Description Required conditions []object Conditions indicate the module states with respect to validation false FybrikModule.status.conditions[index] \u21a9 Parent Condition describes the state of a FybrikApplication at a certain point. Name Type Description Required type string Type of the condition true message string Message contains the details of the current condition false observedGeneration integer ObservedGeneration is the version of the resource for which the condition has been evaluated Format : int64 false status enum Status of the condition, one of (`True`, `False`, `Unknown`). Enum : True, False, Unknown Default : Unknown false FybrikStorageAccount \u21a9 Parent FybrikStorageAccount defines a storage account used for copying data. Only S3 based storage is supported. It contains endpoint, region and a reference to the credentials a Owner of the asset is responsible to store the credentials Name Type Description Required apiVersion string app.fybrik.io/v1beta1 true kind string FybrikStorageAccount true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object FybrikStorageAccountSpec defines the desired state of FybrikStorageAccount true status object FybrikStorageAccountStatus defines the observed state of FybrikStorageAccount false FybrikStorageAccount.spec \u21a9 Parent FybrikStorageAccountSpec defines the desired state of FybrikStorageAccount Name Type Description Required endpoint string Endpoint for accessing the data true id string Identification of a storage account true region string Storage region true secretRef string A name of k8s secret deployed in the control plane. This secret includes secretKey and accessKey credentials for S3 bucket true Plotter \u21a9 Parent Plotter is the Schema for the plotters API Name Type Description Required apiVersion string app.fybrik.io/v1beta1 true kind string Plotter true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object PlotterSpec defines the desired state of Plotter, which is applied in a multi-clustered environment. Plotter declares what needs to be installed and where (as blueprints running on remote clusters) which provides the Data Scientist's application with secure and governed access to the data requested in the FybrikApplication. true status object PlotterStatus defines the observed state of Plotter This includes readiness, error message, and indicators received from blueprint resources owned by the Plotter for cleanup and status monitoring false Plotter.spec \u21a9 Parent PlotterSpec defines the desired state of Plotter, which is applied in a multi-clustered environment. Plotter declares what needs to be installed and where (as blueprints running on remote clusters) which provides the Data Scientist's application with secure and governed access to the data requested in the FybrikApplication. Name Type Description Required assets map[string]object Assets is a map holding information about the assets The key is the assetID true flows []object true modulesNamespace string ModulesNamespace is the namespace where modules should be allocated true templates map[string]object Templates is a map holding the templates used in this plotter steps The key is the template name true appInfo object Application context to be transferred to the modules false appSelector object Selector enables to connect the resource to the application Application labels should match the labels in the selector. For some flows the selector may not be used. false Plotter.spec.assets[key] \u21a9 Parent AssetDetails is a list of assets used in the fybrikapplication. In addition to assets declared in fybrikapplication, AssetDetails list also contains assets that are allocated by the control-plane in order to serve fybrikapplication Name Type Description Required assetDetails object DataStore contains the details for accessing the data that are sent by catalog connectors Credentials for accessing the data are stored in Vault, in the location represented by Vault property. true advertisedAssetId string AdvertisedAssetID links this asset to asset from fybrikapplication and is used by user facing services false Plotter.spec.assets[key].assetDetails \u21a9 Parent DataStore contains the details for accessing the data that are sent by catalog connectors Credentials for accessing the data are stored in Vault, in the location represented by Vault property. Name Type Description Required connection object Connection has the relevant details for accessing the data (url, table, ssl, etc.) true format string Format represents data format (e.g. parquet) as received from catalog connectors false vault map[string]object Holds details for retrieving credentials by the modules from Vault store. It is a map so that different credentials can be stored for the different DataFlow operations. false Plotter.spec.assets[key].assetDetails.connection \u21a9 Parent Connection has the relevant details for accessing the data (url, table, ssl, etc.) Name Type Description Required name string Name of the connection to the data source true Plotter.spec.assets[key].assetDetails.vault[key] \u21a9 Parent Holds details for retrieving credentials from Vault store. Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true Plotter.spec.flows[index] \u21a9 Parent Flows is the list of data flows driven from fybrikapplication: Each element in the list holds the flow of the data requested in fybrikapplication. Name Type Description Required assetId string AssetID indicates the data set being used in this data flow true flowType enum Type of the flow (e.g. read) Enum : read, write, delete, copy true name string Name of the flow true subFlows []object true Plotter.spec.flows[index].subFlows[index] \u21a9 Parent Subflows is a list of data flows which are originated from the same data asset but are triggered differently (e.g., one upon init trigger and one upon workload trigger) Name Type Description Required flowType enum Type of the flow (e.g. read) Enum : read, write, delete, copy true name string Name of the SubFlow true steps [][]object Steps defines a series of sequential/parallel data flow steps The first dimension represents parallel data flows. The second sequential components within the same parallel data flow. true triggers []enum Triggers true Plotter.spec.flows[index].subFlows[index].steps[index][index] \u21a9 Parent DataFlowStep contains details on a single data flow step Name Type Description Required cluster string Name of the cluster this step is executed on true name string Name of the step true template string Template is the name of the template to execute the step The full details of the template can be extracted from Plotter.spec.templates list field. true parameters object Step parameters TODO why not flatten the parameters into this data flow step false Plotter.spec.flows[index].subFlows[index].steps[index][index].parameters \u21a9 Parent Step parameters TODO why not flatten the parameters into this data flow step Name Type Description Required action []object Actions are the data transformations that the module supports false api object ResourceDetails includes asset connection details false args []object false Plotter.spec.flows[index].subFlows[index].steps[index][index].parameters.action[index] \u21a9 Parent Action to be performed on the data, e.g., masking Name Type Description Required name string Action name true Plotter.spec.flows[index].subFlows[index].steps[index][index].parameters.api \u21a9 Parent ResourceDetails includes asset connection details Name Type Description Required connection object Connection information true dataFormat string Data format false Plotter.spec.flows[index].subFlows[index].steps[index][index].parameters.api.connection \u21a9 Parent Connection information Name Type Description Required name string Name of the connection to the data source true Plotter.spec.flows[index].subFlows[index].steps[index][index].parameters.args[index] \u21a9 Parent StepArgument describes a step: it could be assetID or an endpoint of another step Name Type Description Required api object API holds information for accessing a module instance false assetId string AssetID identifies the source asset of this step false Plotter.spec.flows[index].subFlows[index].steps[index][index].parameters.args[index].api \u21a9 Parent API holds information for accessing a module instance Name Type Description Required connection object Connection information true dataFormat string Data format false Plotter.spec.flows[index].subFlows[index].steps[index][index].parameters.args[index].api.connection \u21a9 Parent Connection information Name Type Description Required name string Name of the connection to the data source true Plotter.spec.templates[key] \u21a9 Parent Template contains basic information about the required modules to serve the fybrikapplication e.g., the module helm chart name. Name Type Description Required modules []object Modules is a list of dependent modules. e.g., if a plugin module is used then the service module is used in should appear first in the modules list of the same template. If the modules list contains more than one module, the first module in the list is referred to as the \"primary module\" of which all the parameters to this template are sent to. true name string Name of the template false Plotter.spec.templates[key].modules[index] \u21a9 Parent ModuleInfo is a copy of FybrikModule Custom Resource. It contains information to instantiate resource of type FybrikModule. Name Type Description Required capability string Module capability true chart object Chart contains the information needed to use helm to install the capability true name string Name of the module true type string May be one of service, config or plugin Service: Means that the control plane deploys the component that performs the capability Config: Another pre-installed service performs the capability and the module deployed configures it for the particular workload or dataset Plugin: Indicates that this module performs a capability as part of another service or module rather than as a stand-alone module true externalServices []string External services that are required for functionality of the module. false scope enum Scope indicates at what level the capability is used: workload, asset, cluster If not indicated it is assumed to be asset Enum : asset, workload, cluster false Plotter.spec.templates[key].modules[index].chart \u21a9 Parent Chart contains the information needed to use helm to install the capability Name Type Description Required name string Name of helm chart true chartPullSecret string Name of secret containing helm registry credentials false values map[string]string Values to pass to helm chart installation false Plotter.spec.appSelector \u21a9 Parent Selector enables to connect the resource to the application Application labels should match the labels in the selector. For some flows the selector may not be used. Name Type Description Required workloadSelector object WorkloadSelector enables to connect the resource to a user application. Application labels should match the labels in the selector. true clusterName string Cluster name false ipBlocks []object IPBlocks define policy on particular IPBlocks. the structure of the IPBlock is defined at https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#ipblock-v1-networking-k8s-io false namespaces []string Namespaces where user application might run false Plotter.spec.appSelector.workloadSelector \u21a9 Parent WorkloadSelector enables to connect the resource to a user application. Application labels should match the labels in the selector. Name Type Description Required matchExpressions []object matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false Plotter.spec.appSelector.workloadSelector.matchExpressions[index] \u21a9 Parent A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. Name Type Description Required key string key is the label key that the selector applies to. true operator string operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false Plotter.spec.appSelector.ipBlocks[index] \u21a9 Parent IPBlock describes a particular CIDR (Ex. \"192.168.1.1/24\",\"2001:db9::/64\") that is allowed to the pods matched by a NetworkPolicySpec's podSelector. The except entry describes CIDRs that should not be included within this rule. Name Type Description Required cidr string CIDR is a string representing the IP Block Valid examples are \"192.168.1.1/24\" or \"2001:db9::/64\" true except []string Except is a slice of CIDRs that should not be included within an IP Block Valid examples are \"192.168.1.1/24\" or \"2001:db9::/64\" Except values will be rejected if they are outside the CIDR range false Plotter.status \u21a9 Parent PlotterStatus defines the observed state of Plotter This includes readiness, error message, and indicators received from blueprint resources owned by the Plotter for cleanup and status monitoring Name Type Description Required assets map[string]object Assets is a map containing the status per asset. The key of this map is assetId false blueprints map[string]object false conditions []object Conditions represent the possible error and failure conditions false flows map[string]object Flows is a map containing the status for each flow the key is the flow name false observedGeneration integer ObservedGeneration is taken from the Plotter metadata. This is used to determine during reconcile whether reconcile was called because the desired state changed, or whether status of the allocated blueprints should be checked. Format : int64 false observedState object ObservedState includes information to be reported back to the FybrikApplication resource It includes readiness and error indications, as well as user instructions false readyTimestamp string Format : date-time false Plotter.status.assets[key] \u21a9 Parent ObservedState represents a part of the generated Blueprint/Plotter resource status that allows update of FybrikApplication status Name Type Description Required error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false Plotter.status.blueprints[key] \u21a9 Parent MetaBlueprint defines blueprint metadata (name, namespace) and status Name Type Description Required name string true namespace string true status object BlueprintStatus defines the observed state of Blueprint This includes readiness, error message, and indicators for the Kubernetes resources owned by the Blueprint for cleanup and status monitoring true Plotter.status.blueprints[key].status \u21a9 Parent BlueprintStatus defines the observed state of Blueprint This includes readiness, error message, and indicators for the Kubernetes resources owned by the Blueprint for cleanup and status monitoring Name Type Description Required modules map[string]object ModulesState is a map which holds the status of each module its key is the moduleInstanceName which is the unique name for the deployed instance related to this workload false observedGeneration integer ObservedGeneration is taken from the Blueprint metadata. This is used to determine during reconcile whether reconcile was called because the desired state changed, or whether status of the allocated resources should be checked. Format : int64 false observedState object ObservedState includes information to be reported back to the FybrikApplication resource It includes readiness and error indications, as well as user instructions false releases map[string]integer Releases map each release to the observed generation of the blueprint containing this release. At the end of reconcile, each release should be mapped to the latest blueprint version or be uninstalled. false Plotter.status.blueprints[key].status.modules[key] \u21a9 Parent ObservedState represents a part of the generated Blueprint/Plotter resource status that allows update of FybrikApplication status Name Type Description Required error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false Plotter.status.blueprints[key].status.observedState \u21a9 Parent ObservedState includes information to be reported back to the FybrikApplication resource It includes readiness and error indications, as well as user instructions Name Type Description Required error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false Plotter.status.conditions[index] \u21a9 Parent Condition describes the state of a FybrikApplication at a certain point. Name Type Description Required type string Type of the condition true message string Message contains the details of the current condition false observedGeneration integer ObservedGeneration is the version of the resource for which the condition has been evaluated Format : int64 false status enum Status of the condition, one of (`True`, `False`, `Unknown`). Enum : True, False, Unknown Default : Unknown false Plotter.status.flows[key] \u21a9 Parent FlowStatus includes information to be reported back to the FybrikApplication resource It holds the status per data flow Name Type Description Required subFlows map[string]object true status object ObservedState includes information about the current flow It includes readiness and error indications, as well as user instructions false Plotter.status.flows[key].subFlows[key] \u21a9 Parent ObservedState represents a part of the generated Blueprint/Plotter resource status that allows update of FybrikApplication status Name Type Description Required error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false Plotter.status.flows[key].status \u21a9 Parent ObservedState includes information about the current flow It includes readiness and error indications, as well as user instructions Name Type Description Required error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false Plotter.status.observedState \u21a9 Parent ObservedState includes information to be reported back to the FybrikApplication resource It includes readiness and error indications, as well as user instructions Name Type Description Required error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false app.fybrik.io/v1beta2 Resource Types: FybrikStorageAccount FybrikStorageAccount \u21a9 Parent FybrikStorageAccount is a storage account Fybrik uses to dynamically allocate space for datasets whose creation or copy it orchestrates. Name Type Description Required apiVersion string app.fybrik.io/v1beta2 true kind string FybrikStorageAccount true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object FybrikStorageAccountSpec defines the desired state of FybrikStorageAccount true status object FybrikStorageAccountStatus defines the observed state of FybrikStorageAccount false FybrikStorageAccount.spec \u21a9 Parent FybrikStorageAccountSpec defines the desired state of FybrikStorageAccount Name Type Description Required geography string Storage geography true id string Identification of a storage account true secretRef string A name of k8s secret deployed in the control plane. true type string Type of the storage, e.g., s3 true katalog.fybrik.io/v1alpha1 Resource Types: Asset Asset \u21a9 Parent Asset defines an asset in the catalog Name Type Description Required apiVersion string katalog.fybrik.io/v1alpha1 true kind string Asset true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object true Asset.spec \u21a9 Parent Name Type Description Required details object Asset details true metadata object Asset metadata true secretRef object Reference to a Secret resource holding credentials for this asset true Asset.spec.details \u21a9 Parent Asset details Name Type Description Required connection object Connection information true dataFormat string Data format false Asset.spec.details.connection \u21a9 Parent Connection information Name Type Description Required name string Name of the connection to the data source true Asset.spec.metadata \u21a9 Parent Asset metadata Name Type Description Required columns []object Columns associated with the asset false geography string Geography of the resource false name string Name of the resource false owner string Owner of the resource false tags object Tags associated with the asset false Asset.spec.metadata.columns[index] \u21a9 Parent ResourceColumn represents a column in a tabular resource Name Type Description Required name string Name of the column true tags object Tags associated with the column false Asset.spec.secretRef \u21a9 Parent Reference to a Secret resource holding credentials for this asset Name Type Description Required name string Name of the Secret resource true namespace string Namespace of the Secret resource. If it is empty then the asset namespace is used. false","title":"API Reference"},{"location":"reference/crds/#api-reference","text":"Packages: app.fybrik.io/v1beta1 app.fybrik.io/v1beta2 katalog.fybrik.io/v1alpha1","title":"API Reference"},{"location":"reference/crds/#appfybrikiov1beta1","text":"Resource Types: Blueprint FybrikApplication FybrikModule FybrikStorageAccount Plotter","title":"app.fybrik.io/v1beta1"},{"location":"reference/crds/#blueprint","text":"\u21a9 Parent Blueprint is the Schema for the blueprints API Name Type Description Required apiVersion string app.fybrik.io/v1beta1 true kind string Blueprint true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object BlueprintSpec defines the desired state of Blueprint, which defines the components of the workload's data path that run in a particular cluster. In a single cluster environment there is one blueprint per workload (FybrikApplication). In a multi-cluster environment there is one Blueprint per cluster per workload (FybrikApplication). true status object BlueprintStatus defines the observed state of Blueprint This includes readiness, error message, and indicators for the Kubernetes resources owned by the Blueprint for cleanup and status monitoring false","title":"Blueprint"},{"location":"reference/crds/#blueprintspec","text":"\u21a9 Parent BlueprintSpec defines the desired state of Blueprint, which defines the components of the workload's data path that run in a particular cluster. In a single cluster environment there is one blueprint per workload (FybrikApplication). In a multi-cluster environment there is one Blueprint per cluster per workload (FybrikApplication). Name Type Description Required cluster string Cluster indicates the cluster on which the Blueprint runs true modules map[string]object Modules is a map which contains modules that indicate the data path components that run in this cluster The map key is moduleInstanceName which is the unique name for the deployed instance related to this workload true modulesNamespace string ModulesNamespace is the namespace where modules should be allocated true application object ApplicationContext is a context of the origin FybrikApplication (labels, properties, etc.) false","title":"Blueprint.spec"},{"location":"reference/crds/#blueprintspecmoduleskey","text":"\u21a9 Parent BlueprintModule is a copy of a FybrikModule Custom Resource. It contains the information necessary to instantiate a datapath component, including the parameters relevant for the particular workload. Name Type Description Required chart object Chart contains the location of the helm chart with info detailing how to deploy true name string Name of the FybrikModule on which this is based true arguments object Arguments are the input parameters for a specific instance of a module. false assetIds []string assetIDs indicate the assets processed by this module. Included so we can track asset status as well as module status in the future. false network object Network specifies the module communication with a workload or other modules false","title":"Blueprint.spec.modules[key]"},{"location":"reference/crds/#blueprintspecmoduleskeychart","text":"\u21a9 Parent Chart contains the location of the helm chart with info detailing how to deploy Name Type Description Required name string Name of helm chart true chartPullSecret string Name of secret containing helm registry credentials false values map[string]string Values to pass to helm chart installation false","title":"Blueprint.spec.modules[key].chart"},{"location":"reference/crds/#blueprintspecmoduleskeyarguments","text":"\u21a9 Parent Arguments are the input parameters for a specific instance of a module. Name Type Description Required assets []object Assets define asset related arguments, such as data source, transformations, etc. false","title":"Blueprint.spec.modules[key].arguments"},{"location":"reference/crds/#blueprintspecmoduleskeyargumentsassetsindex","text":"\u21a9 Parent AssetContext defines the input parameters for modules that access an asset Name Type Description Required assetID string AssetID identifies the asset to be used for accessing the data when it is ready It is copied from the FybrikApplication resource true capability string Capability of the module true args []object List of datastores associated with the asset false transformations []object Transformations are different types of processing that may be done to the data as it is copied. false","title":"Blueprint.spec.modules[key].arguments.assets[index]"},{"location":"reference/crds/#blueprintspecmoduleskeyargumentsassetsindexargsindex","text":"\u21a9 Parent DataStore contains the details for accessing the data that are sent by catalog connectors Credentials for accessing the data are stored in Vault, in the location represented by Vault property. Name Type Description Required connection object Connection has the relevant details for accessing the data (url, table, ssl, etc.) true format string Format represents data format (e.g. parquet) as received from catalog connectors false vault map[string]object Holds details for retrieving credentials by the modules from Vault store. It is a map so that different credentials can be stored for the different DataFlow operations. false","title":"Blueprint.spec.modules[key].arguments.assets[index].args[index]"},{"location":"reference/crds/#blueprintspecmoduleskeyargumentsassetsindexargsindexconnection","text":"\u21a9 Parent Connection has the relevant details for accessing the data (url, table, ssl, etc.) Name Type Description Required name string Name of the connection to the data source true","title":"Blueprint.spec.modules[key].arguments.assets[index].args[index].connection"},{"location":"reference/crds/#blueprintspecmoduleskeyargumentsassetsindexargsindexvaultkey","text":"\u21a9 Parent Holds details for retrieving credentials from Vault store. Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true","title":"Blueprint.spec.modules[key].arguments.assets[index].args[index].vault[key]"},{"location":"reference/crds/#blueprintspecmoduleskeyargumentsassetsindextransformationsindex","text":"\u21a9 Parent Action to be performed on the data, e.g., masking Name Type Description Required name string Action name true","title":"Blueprint.spec.modules[key].arguments.assets[index].transformations[index]"},{"location":"reference/crds/#blueprintspecmoduleskeynetwork","text":"\u21a9 Parent Network specifies the module communication with a workload or other modules Name Type Description Required egress []object Egress (internal modules) false endpoint boolean Endpoint indicates whether the module service is used as an endpoint by the workload application false ingress []object Ingress (internal modules) false urls []string External services and datasets in the form of hostname + port or a hostname only (e.g., s3 endpoint), or a CIDR (Classless Inter-Domain Routing) with optional port false","title":"Blueprint.spec.modules[key].network"},{"location":"reference/crds/#blueprintspecmoduleskeynetworkegressindex","text":"\u21a9 Parent ModuleDeployment specifies deployment of a Fybrik module Name Type Description Required cluster string Cluster name true release string Release name true urls []string Service URLs, usually represented by hostname + port true","title":"Blueprint.spec.modules[key].network.egress[index]"},{"location":"reference/crds/#blueprintspecmoduleskeynetworkingressindex","text":"\u21a9 Parent ModuleDeployment specifies deployment of a Fybrik module Name Type Description Required cluster string Cluster name true release string Release name true urls []string Service URLs, usually represented by hostname + port true","title":"Blueprint.spec.modules[key].network.ingress[index]"},{"location":"reference/crds/#blueprintspecapplication","text":"\u21a9 Parent ApplicationContext is a context of the origin FybrikApplication (labels, properties, etc.) Name Type Description Required context object Application context such as intent, role, etc. false ipBlocks []object IPBlocks define policy on particular IPBlocks. the structure of the IPBlock is defined at https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#ipblock-v1-networking-k8s-io It is obtained from FybrikApplication spec. false namespaces []string Namespaces where user application might run It is obtained from FybrikApplication spec. false selector object Application selector is used to identify the user workload. It is obtained from FybrikApplication spec. false","title":"Blueprint.spec.application"},{"location":"reference/crds/#blueprintspecapplicationipblocksindex","text":"\u21a9 Parent IPBlock describes a particular CIDR (Ex. \"192.168.1.1/24\",\"2001:db9::/64\") that is allowed to the pods matched by a NetworkPolicySpec's podSelector. The except entry describes CIDRs that should not be included within this rule. Name Type Description Required cidr string CIDR is a string representing the IP Block Valid examples are \"192.168.1.1/24\" or \"2001:db9::/64\" true except []string Except is a slice of CIDRs that should not be included within an IP Block Valid examples are \"192.168.1.1/24\" or \"2001:db9::/64\" Except values will be rejected if they are outside the CIDR range false","title":"Blueprint.spec.application.ipBlocks[index]"},{"location":"reference/crds/#blueprintspecapplicationselector","text":"\u21a9 Parent Application selector is used to identify the user workload. It is obtained from FybrikApplication spec. Name Type Description Required matchExpressions []object matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false","title":"Blueprint.spec.application.selector"},{"location":"reference/crds/#blueprintspecapplicationselectormatchexpressionsindex","text":"\u21a9 Parent A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. Name Type Description Required key string key is the label key that the selector applies to. true operator string operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false","title":"Blueprint.spec.application.selector.matchExpressions[index]"},{"location":"reference/crds/#blueprintstatus","text":"\u21a9 Parent BlueprintStatus defines the observed state of Blueprint This includes readiness, error message, and indicators for the Kubernetes resources owned by the Blueprint for cleanup and status monitoring Name Type Description Required modules map[string]object ModulesState is a map which holds the status of each module its key is the moduleInstanceName which is the unique name for the deployed instance related to this workload false observedGeneration integer ObservedGeneration is taken from the Blueprint metadata. This is used to determine during reconcile whether reconcile was called because the desired state changed, or whether status of the allocated resources should be checked. Format : int64 false observedState object ObservedState includes information to be reported back to the FybrikApplication resource It includes readiness and error indications, as well as user instructions false releases map[string]integer Releases map each release to the observed generation of the blueprint containing this release. At the end of reconcile, each release should be mapped to the latest blueprint version or be uninstalled. false","title":"Blueprint.status"},{"location":"reference/crds/#blueprintstatusmoduleskey","text":"\u21a9 Parent ObservedState represents a part of the generated Blueprint/Plotter resource status that allows update of FybrikApplication status Name Type Description Required error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false","title":"Blueprint.status.modules[key]"},{"location":"reference/crds/#blueprintstatusobservedstate","text":"\u21a9 Parent ObservedState includes information to be reported back to the FybrikApplication resource It includes readiness and error indications, as well as user instructions Name Type Description Required error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false","title":"Blueprint.status.observedState"},{"location":"reference/crds/#fybrikapplication","text":"\u21a9 Parent FybrikApplication provides information about the application whose data is being operated on, the nature of the processing, and the data sets chosen for processing by the application. The FybrikApplication controller obtains instructions regarding any governance related changes that must be performed on the data, identifies the modules capable of performing such changes, and finally generates the Plotter which defines the secure runtime environment and all the components in it. This runtime environment provides the application with access to the data requested in a secure manner and without having to provide any credentials for the data sets. The credentials are obtained automatically by the manager from the credential management system. Name Type Description Required apiVersion string app.fybrik.io/v1beta1 true kind string FybrikApplication true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object FybrikApplicationSpec defines data flows needed by the application, the purpose and other contextual information about the application. true status object FybrikApplicationStatus defines the observed state of FybrikApplication. false","title":"FybrikApplication"},{"location":"reference/crds/#fybrikapplicationspec","text":"\u21a9 Parent FybrikApplicationSpec defines data flows needed by the application, the purpose and other contextual information about the application. Name Type Description Required appInfo object AppInfo contains information describing the reasons for the processing that will be done by the application. true data []object Data contains the identifiers of the data to be used by the Data Scientist's application, and the protocol used to access it and the format expected. true secretRef string SecretRef points to the secret that holds credentials for each system the user has been authenticated with. The secret is deployed in FybrikApplication namespace. false selector object Selector enables to connect the resource to the application Application labels should match the labels in the selector. false","title":"FybrikApplication.spec"},{"location":"reference/crds/#fybrikapplicationspecdataindex","text":"\u21a9 Parent DataContext indicates data set being processed by the workload and includes information about the data format and technologies used to access the data. Name Type Description Required dataSetID string DataSetID is a unique identifier of the dataset chosen from the data catalog. For data catalogs that support multiple sub-catalogs, it includes the catalog id and the dataset id. When writing a new dataset it is the name provided by the user or workload generating it. true requirements object Requirements from the system true flow enum Flows indicates what is being done with the particular dataset - ex: read, write, copy (ingest), delete This is optional for the purpose of backward compatibility. If nothing is provided, read is assumed. Enum : read, write, delete, copy false","title":"FybrikApplication.spec.data[index]"},{"location":"reference/crds/#fybrikapplicationspecdataindexrequirements","text":"\u21a9 Parent Requirements from the system Name Type Description Required flowParams object FlowParams include the requirements for particular data flows false interface object Interface indicates the protocol and format expected by the data user false","title":"FybrikApplication.spec.data[index].requirements"},{"location":"reference/crds/#fybrikapplicationspecdataindexrequirementsflowparams","text":"\u21a9 Parent FlowParams include the requirements for particular data flows Name Type Description Required catalog string Catalog indicates that the data asset must be cataloged, and in which catalog to register it false isNewDataSet boolean IsNewDataSet if true indicates that the DataContext.DataSetID is user provided and not a full catalog / dataset ID. Relevant when writing. A unique ID from the catalog will be provided in the FybrikApplication Status after a new catalog entry is created. false metadata object Source asset metadata like asset name, owner, geography, etc Relevant when writing new asset. false storageEstimate integer Storage estimate indicates the estimated amount of storage in MB, GB, TB required when writing new data. Format : int64 false","title":"FybrikApplication.spec.data[index].requirements.flowParams"},{"location":"reference/crds/#fybrikapplicationspecdataindexrequirementsflowparamsmetadata","text":"\u21a9 Parent Source asset metadata like asset name, owner, geography, etc Relevant when writing new asset. Name Type Description Required columns []object Columns associated with the asset false geography string Geography of the resource false name string Name of the resource false owner string Owner of the resource false tags object Tags associated with the asset false","title":"FybrikApplication.spec.data[index].requirements.flowParams.metadata"},{"location":"reference/crds/#fybrikapplicationspecdataindexrequirementsflowparamsmetadatacolumnsindex","text":"\u21a9 Parent ResourceColumn represents a column in a tabular resource Name Type Description Required name string Name of the column true tags object Tags associated with the column false","title":"FybrikApplication.spec.data[index].requirements.flowParams.metadata.columns[index]"},{"location":"reference/crds/#fybrikapplicationspecdataindexrequirementsinterface","text":"\u21a9 Parent Interface indicates the protocol and format expected by the data user Name Type Description Required protocol string Connection type, e.g., S3, Kafka, MySQL true dataformat string DataFormat defines the data format type false","title":"FybrikApplication.spec.data[index].requirements.interface"},{"location":"reference/crds/#fybrikapplicationspecselector","text":"\u21a9 Parent Selector enables to connect the resource to the application Application labels should match the labels in the selector. Name Type Description Required workloadSelector object WorkloadSelector enables to connect the resource to a user application. Application labels should match the labels in the selector. true clusterName string Cluster name false ipBlocks []object IPBlocks define policy on particular IPBlocks. the structure of the IPBlock is defined at https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#ipblock-v1-networking-k8s-io false namespaces []string Namespaces where user application might run false","title":"FybrikApplication.spec.selector"},{"location":"reference/crds/#fybrikapplicationspecselectorworkloadselector","text":"\u21a9 Parent WorkloadSelector enables to connect the resource to a user application. Application labels should match the labels in the selector. Name Type Description Required matchExpressions []object matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false","title":"FybrikApplication.spec.selector.workloadSelector"},{"location":"reference/crds/#fybrikapplicationspecselectorworkloadselectormatchexpressionsindex","text":"\u21a9 Parent A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. Name Type Description Required key string key is the label key that the selector applies to. true operator string operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false","title":"FybrikApplication.spec.selector.workloadSelector.matchExpressions[index]"},{"location":"reference/crds/#fybrikapplicationspecselectoripblocksindex","text":"\u21a9 Parent IPBlock describes a particular CIDR (Ex. \"192.168.1.1/24\",\"2001:db9::/64\") that is allowed to the pods matched by a NetworkPolicySpec's podSelector. The except entry describes CIDRs that should not be included within this rule. Name Type Description Required cidr string CIDR is a string representing the IP Block Valid examples are \"192.168.1.1/24\" or \"2001:db9::/64\" true except []string Except is a slice of CIDRs that should not be included within an IP Block Valid examples are \"192.168.1.1/24\" or \"2001:db9::/64\" Except values will be rejected if they are outside the CIDR range false","title":"FybrikApplication.spec.selector.ipBlocks[index]"},{"location":"reference/crds/#fybrikapplicationstatus","text":"\u21a9 Parent FybrikApplicationStatus defines the observed state of FybrikApplication. Name Type Description Required assetStates map[string]object AssetStates provides a status per asset false errorMessage string ErrorMessage indicates that an error has happened during the reconcile, unrelated to a specific asset false generated object Generated resource identifier false observedGeneration integer ObservedGeneration is taken from the FybrikApplication metadata. This is used to determine during reconcile whether reconcile was called because the desired state changed, or whether the Blueprint status changed. Format : int64 false provisionedStorage map[string]object ProvisionedStorage maps a dataset (identified by AssetID) to the new provisioned bucket. It allows FybrikApplication controller to manage buckets in case the spec has been modified, an error has occurred, or a delete event has been received. ProvisionedStorage has the information required to register the dataset once the owned plotter resource is ready false ready boolean Ready is true if all specified assets are either ready to be used or are denied access. false validApplication string ValidApplication indicates whether the FybrikApplication is valid given the defined taxonomy false validatedGeneration integer ValidatedGeneration is the version of the FyrbikApplication that has been validated with the taxonomy defined. Format : int64 false","title":"FybrikApplication.status"},{"location":"reference/crds/#fybrikapplicationstatusassetstateskey","text":"\u21a9 Parent AssetState defines the observed state of an asset Name Type Description Required catalogedAsset string CatalogedAsset provides a new asset identifier after being registered in the enterprise catalog false conditions []object Conditions indicate the asset state (Ready, Deny, Error) false endpoint object Endpoint provides the endpoint spec from which the asset will be served to the application false","title":"FybrikApplication.status.assetStates[key]"},{"location":"reference/crds/#fybrikapplicationstatusassetstateskeyconditionsindex","text":"\u21a9 Parent Condition describes the state of a FybrikApplication at a certain point. Name Type Description Required type string Type of the condition true message string Message contains the details of the current condition false observedGeneration integer ObservedGeneration is the version of the resource for which the condition has been evaluated Format : int64 false status enum Status of the condition, one of (`True`, `False`, `Unknown`). Enum : True, False, Unknown Default : Unknown false","title":"FybrikApplication.status.assetStates[key].conditions[index]"},{"location":"reference/crds/#fybrikapplicationstatusassetstateskeyendpoint","text":"\u21a9 Parent Endpoint provides the endpoint spec from which the asset will be served to the application Name Type Description Required name string Name of the connection to the data source true","title":"FybrikApplication.status.assetStates[key].endpoint"},{"location":"reference/crds/#fybrikapplicationstatusgenerated","text":"\u21a9 Parent Generated resource identifier Name Type Description Required appVersion integer Version of FybrikApplication that has generated this resource Format : int64 true kind string Kind of the resource (Blueprint, Plotter) true name string Resource name true namespace string Resource namespace true","title":"FybrikApplication.status.generated"},{"location":"reference/crds/#fybrikapplicationstatusprovisionedstoragekey","text":"\u21a9 Parent DatasetDetails holds details of the provisioned storage Name Type Description Required datasetRef string deprecated false details object Dataset information false persistent boolean Persistent storage (not to be removed after FybrikApplication is deleted) false resourceMetadata object Resource Metadata false secretRef object Reference to a secret where the credentials are stored false","title":"FybrikApplication.status.provisionedStorage[key]"},{"location":"reference/crds/#fybrikapplicationstatusprovisionedstoragekeydetails","text":"\u21a9 Parent Dataset information Name Type Description Required connection object Connection has the relevant details for accessing the data (url, table, ssl, etc.) true format string Format represents data format (e.g. parquet) as received from catalog connectors false vault map[string]object Holds details for retrieving credentials by the modules from Vault store. It is a map so that different credentials can be stored for the different DataFlow operations. false","title":"FybrikApplication.status.provisionedStorage[key].details"},{"location":"reference/crds/#fybrikapplicationstatusprovisionedstoragekeydetailsconnection","text":"\u21a9 Parent Connection has the relevant details for accessing the data (url, table, ssl, etc.) Name Type Description Required name string Name of the connection to the data source true","title":"FybrikApplication.status.provisionedStorage[key].details.connection"},{"location":"reference/crds/#fybrikapplicationstatusprovisionedstoragekeydetailsvaultkey","text":"\u21a9 Parent Holds details for retrieving credentials from Vault store. Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true","title":"FybrikApplication.status.provisionedStorage[key].details.vault[key]"},{"location":"reference/crds/#fybrikapplicationstatusprovisionedstoragekeyresourcemetadata","text":"\u21a9 Parent Resource Metadata Name Type Description Required columns []object Columns associated with the asset false geography string Geography of the resource false name string Name of the resource false owner string Owner of the resource false tags object Tags associated with the asset false","title":"FybrikApplication.status.provisionedStorage[key].resourceMetadata"},{"location":"reference/crds/#fybrikapplicationstatusprovisionedstoragekeyresourcemetadatacolumnsindex","text":"\u21a9 Parent ResourceColumn represents a column in a tabular resource Name Type Description Required name string Name of the column true tags object Tags associated with the column false","title":"FybrikApplication.status.provisionedStorage[key].resourceMetadata.columns[index]"},{"location":"reference/crds/#fybrikapplicationstatusprovisionedstoragekeysecretref","text":"\u21a9 Parent Reference to a secret where the credentials are stored Name Type Description Required name string Name true namespace string Namespace true","title":"FybrikApplication.status.provisionedStorage[key].secretRef"},{"location":"reference/crds/#fybrikmodule","text":"\u21a9 Parent FybrikModule is a description of an injectable component. the parameters it requires, as well as the specification of how to instantiate such a component. It is used as metadata only. There is no status nor reconciliation. Name Type Description Required apiVersion string app.fybrik.io/v1beta1 true kind string FybrikModule true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object FybrikModuleSpec contains the info common to all modules, which are one of the components that process, load, write, audit, monitor the data used by the data scientist's application. true status object FybrikModuleStatus defines the observed state of FybrikModule. false","title":"FybrikModule"},{"location":"reference/crds/#fybrikmodulespec","text":"\u21a9 Parent FybrikModuleSpec contains the info common to all modules, which are one of the components that process, load, write, audit, monitor the data used by the data scientist's application. Name Type Description Required capabilities []object Capabilities declares what this module knows how to do and the types of data it knows how to handle The key to the map is a CapabilityType string true chart object Reference to a Helm chart that allows deployment of the resources required for this module true type string May be one of service, config or plugin Service: Means that the control plane deploys the component that performs the capability Config: Another pre-installed service performs the capability and the module deployed configures it for the particular workload or dataset Plugin: Indicates that this module performs a capability as part of another service or module rather than as a stand-alone module true dependencies []object Other components that must be installed in order for this module to work false description string An explanation of what this module does false externalServices []string External services that are required for functionality of the module, format of the strings might be: be a URL (with or without schema) or a host name with or without port, or a CIDR (Classless Inter-Domain Routing) with optional port number separated by a colon false pluginType string Plugin type indicates the plugin technology used to invoke the capabilities Ex: vault, fybrik-wasm... Should be provided if type is plugin false statusIndicators []object StatusIndicators allow checking status of a non-standard resource that can not be computed by helm/kstatus false","title":"FybrikModule.spec"},{"location":"reference/crds/#fybrikmodulespeccapabilitiesindex","text":"\u21a9 Parent Capability declares what this module knows how to do and the types of data it knows how to handle Name Type Description Required capability string Capability declares what this module knows how to do - ex: read, write, transform... true actions []object Actions are the data transformations that the module supports false api object API indicates to the application how to access the capabilities provided by the module false plugins []object Plugins enable the module to add libraries to perform actions rather than implementing them by itself false scope enum Scope indicates at what level the capability is used: workload, asset, cluster If not indicated it is assumed to be asset Enum : asset, workload, cluster false supportedInterfaces []object Copy should have one or more instances in the list, and its content should have source and sink Read should have one or more instances in the list, each with source populated Write should have one or more instances in the list, each with sink populated This field may not be required if not handling data false","title":"FybrikModule.spec.capabilities[index]"},{"location":"reference/crds/#fybrikmodulespeccapabilitiesindexactionsindex","text":"\u21a9 Parent Name Type Description Required name string Unique name of an action supported by the module true","title":"FybrikModule.spec.capabilities[index].actions[index]"},{"location":"reference/crds/#fybrikmodulespeccapabilitiesindexapi","text":"\u21a9 Parent API indicates to the application how to access the capabilities provided by the module Name Type Description Required connection object Connection information true dataFormat string Data format false","title":"FybrikModule.spec.capabilities[index].api"},{"location":"reference/crds/#fybrikmodulespeccapabilitiesindexapiconnection","text":"\u21a9 Parent Connection information Name Type Description Required name string Name of the connection to the data source true","title":"FybrikModule.spec.capabilities[index].api.connection"},{"location":"reference/crds/#fybrikmodulespeccapabilitiesindexpluginsindex","text":"\u21a9 Parent Name Type Description Required dataFormat string DataFormat indicates the format of data the plugin knows how to process true pluginType string PluginType indicates the technology used for the module and the plugin to interact The values supported should come from the module taxonomy Examples of such mechanisms are vault plugins, wasm, etc true","title":"FybrikModule.spec.capabilities[index].plugins[index]"},{"location":"reference/crds/#fybrikmodulespeccapabilitiesindexsupportedinterfacesindex","text":"\u21a9 Parent ModuleInOut specifies the protocol and format of the data input and output by the module - if any Name Type Description Required sink object Sink specifies the output data protocol and format false source object Source specifies the input data protocol and format false","title":"FybrikModule.spec.capabilities[index].supportedInterfaces[index]"},{"location":"reference/crds/#fybrikmodulespeccapabilitiesindexsupportedinterfacesindexsink","text":"\u21a9 Parent Sink specifies the output data protocol and format Name Type Description Required protocol string Connection type, e.g., S3, Kafka, MySQL true dataformat string DataFormat defines the data format type false","title":"FybrikModule.spec.capabilities[index].supportedInterfaces[index].sink"},{"location":"reference/crds/#fybrikmodulespeccapabilitiesindexsupportedinterfacesindexsource","text":"\u21a9 Parent Source specifies the input data protocol and format Name Type Description Required protocol string Connection type, e.g., S3, Kafka, MySQL true dataformat string DataFormat defines the data format type false","title":"FybrikModule.spec.capabilities[index].supportedInterfaces[index].source"},{"location":"reference/crds/#fybrikmodulespecchart","text":"\u21a9 Parent Reference to a Helm chart that allows deployment of the resources required for this module Name Type Description Required name string Name of helm chart true chartPullSecret string Name of secret containing helm registry credentials false values map[string]string Values to pass to helm chart installation false","title":"FybrikModule.spec.chart"},{"location":"reference/crds/#fybrikmodulespecdependenciesindex","text":"\u21a9 Parent Dependency details another component on which this module relies - i.e. a pre-requisit Name Type Description Required name string Name is the name of the dependent component true type enum Type provides information used in determining how to instantiate the component Enum : module, connector, feature true","title":"FybrikModule.spec.dependencies[index]"},{"location":"reference/crds/#fybrikmodulespecstatusindicatorsindex","text":"\u21a9 Parent ResourceStatusIndicator is used to determine the status of an orchestrated resource Name Type Description Required kind string Kind provides information about the resource kind true successCondition string SuccessCondition specifies a condition that indicates that the resource is ready It uses kubernetes label selection syntax (https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/) true errorMessage string ErrorMessage specifies the resource field to check for an error, e.g. status.errorMsg false failureCondition string FailureCondition specifies a condition that indicates the resource failure It uses kubernetes label selection syntax (https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/) false","title":"FybrikModule.spec.statusIndicators[index]"},{"location":"reference/crds/#fybrikmodulestatus","text":"\u21a9 Parent FybrikModuleStatus defines the observed state of FybrikModule. Name Type Description Required conditions []object Conditions indicate the module states with respect to validation false","title":"FybrikModule.status"},{"location":"reference/crds/#fybrikmodulestatusconditionsindex","text":"\u21a9 Parent Condition describes the state of a FybrikApplication at a certain point. Name Type Description Required type string Type of the condition true message string Message contains the details of the current condition false observedGeneration integer ObservedGeneration is the version of the resource for which the condition has been evaluated Format : int64 false status enum Status of the condition, one of (`True`, `False`, `Unknown`). Enum : True, False, Unknown Default : Unknown false","title":"FybrikModule.status.conditions[index]"},{"location":"reference/crds/#fybrikstorageaccount","text":"\u21a9 Parent FybrikStorageAccount defines a storage account used for copying data. Only S3 based storage is supported. It contains endpoint, region and a reference to the credentials a Owner of the asset is responsible to store the credentials Name Type Description Required apiVersion string app.fybrik.io/v1beta1 true kind string FybrikStorageAccount true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object FybrikStorageAccountSpec defines the desired state of FybrikStorageAccount true status object FybrikStorageAccountStatus defines the observed state of FybrikStorageAccount false","title":"FybrikStorageAccount"},{"location":"reference/crds/#fybrikstorageaccountspec","text":"\u21a9 Parent FybrikStorageAccountSpec defines the desired state of FybrikStorageAccount Name Type Description Required endpoint string Endpoint for accessing the data true id string Identification of a storage account true region string Storage region true secretRef string A name of k8s secret deployed in the control plane. This secret includes secretKey and accessKey credentials for S3 bucket true","title":"FybrikStorageAccount.spec"},{"location":"reference/crds/#plotter","text":"\u21a9 Parent Plotter is the Schema for the plotters API Name Type Description Required apiVersion string app.fybrik.io/v1beta1 true kind string Plotter true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object PlotterSpec defines the desired state of Plotter, which is applied in a multi-clustered environment. Plotter declares what needs to be installed and where (as blueprints running on remote clusters) which provides the Data Scientist's application with secure and governed access to the data requested in the FybrikApplication. true status object PlotterStatus defines the observed state of Plotter This includes readiness, error message, and indicators received from blueprint resources owned by the Plotter for cleanup and status monitoring false","title":"Plotter"},{"location":"reference/crds/#plotterspec","text":"\u21a9 Parent PlotterSpec defines the desired state of Plotter, which is applied in a multi-clustered environment. Plotter declares what needs to be installed and where (as blueprints running on remote clusters) which provides the Data Scientist's application with secure and governed access to the data requested in the FybrikApplication. Name Type Description Required assets map[string]object Assets is a map holding information about the assets The key is the assetID true flows []object true modulesNamespace string ModulesNamespace is the namespace where modules should be allocated true templates map[string]object Templates is a map holding the templates used in this plotter steps The key is the template name true appInfo object Application context to be transferred to the modules false appSelector object Selector enables to connect the resource to the application Application labels should match the labels in the selector. For some flows the selector may not be used. false","title":"Plotter.spec"},{"location":"reference/crds/#plotterspecassetskey","text":"\u21a9 Parent AssetDetails is a list of assets used in the fybrikapplication. In addition to assets declared in fybrikapplication, AssetDetails list also contains assets that are allocated by the control-plane in order to serve fybrikapplication Name Type Description Required assetDetails object DataStore contains the details for accessing the data that are sent by catalog connectors Credentials for accessing the data are stored in Vault, in the location represented by Vault property. true advertisedAssetId string AdvertisedAssetID links this asset to asset from fybrikapplication and is used by user facing services false","title":"Plotter.spec.assets[key]"},{"location":"reference/crds/#plotterspecassetskeyassetdetails","text":"\u21a9 Parent DataStore contains the details for accessing the data that are sent by catalog connectors Credentials for accessing the data are stored in Vault, in the location represented by Vault property. Name Type Description Required connection object Connection has the relevant details for accessing the data (url, table, ssl, etc.) true format string Format represents data format (e.g. parquet) as received from catalog connectors false vault map[string]object Holds details for retrieving credentials by the modules from Vault store. It is a map so that different credentials can be stored for the different DataFlow operations. false","title":"Plotter.spec.assets[key].assetDetails"},{"location":"reference/crds/#plotterspecassetskeyassetdetailsconnection","text":"\u21a9 Parent Connection has the relevant details for accessing the data (url, table, ssl, etc.) Name Type Description Required name string Name of the connection to the data source true","title":"Plotter.spec.assets[key].assetDetails.connection"},{"location":"reference/crds/#plotterspecassetskeyassetdetailsvaultkey","text":"\u21a9 Parent Holds details for retrieving credentials from Vault store. Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true","title":"Plotter.spec.assets[key].assetDetails.vault[key]"},{"location":"reference/crds/#plotterspecflowsindex","text":"\u21a9 Parent Flows is the list of data flows driven from fybrikapplication: Each element in the list holds the flow of the data requested in fybrikapplication. Name Type Description Required assetId string AssetID indicates the data set being used in this data flow true flowType enum Type of the flow (e.g. read) Enum : read, write, delete, copy true name string Name of the flow true subFlows []object true","title":"Plotter.spec.flows[index]"},{"location":"reference/crds/#plotterspecflowsindexsubflowsindex","text":"\u21a9 Parent Subflows is a list of data flows which are originated from the same data asset but are triggered differently (e.g., one upon init trigger and one upon workload trigger) Name Type Description Required flowType enum Type of the flow (e.g. read) Enum : read, write, delete, copy true name string Name of the SubFlow true steps [][]object Steps defines a series of sequential/parallel data flow steps The first dimension represents parallel data flows. The second sequential components within the same parallel data flow. true triggers []enum Triggers true","title":"Plotter.spec.flows[index].subFlows[index]"},{"location":"reference/crds/#plotterspecflowsindexsubflowsindexstepsindexindex","text":"\u21a9 Parent DataFlowStep contains details on a single data flow step Name Type Description Required cluster string Name of the cluster this step is executed on true name string Name of the step true template string Template is the name of the template to execute the step The full details of the template can be extracted from Plotter.spec.templates list field. true parameters object Step parameters TODO why not flatten the parameters into this data flow step false","title":"Plotter.spec.flows[index].subFlows[index].steps[index][index]"},{"location":"reference/crds/#plotterspecflowsindexsubflowsindexstepsindexindexparameters","text":"\u21a9 Parent Step parameters TODO why not flatten the parameters into this data flow step Name Type Description Required action []object Actions are the data transformations that the module supports false api object ResourceDetails includes asset connection details false args []object false","title":"Plotter.spec.flows[index].subFlows[index].steps[index][index].parameters"},{"location":"reference/crds/#plotterspecflowsindexsubflowsindexstepsindexindexparametersactionindex","text":"\u21a9 Parent Action to be performed on the data, e.g., masking Name Type Description Required name string Action name true","title":"Plotter.spec.flows[index].subFlows[index].steps[index][index].parameters.action[index]"},{"location":"reference/crds/#plotterspecflowsindexsubflowsindexstepsindexindexparametersapi","text":"\u21a9 Parent ResourceDetails includes asset connection details Name Type Description Required connection object Connection information true dataFormat string Data format false","title":"Plotter.spec.flows[index].subFlows[index].steps[index][index].parameters.api"},{"location":"reference/crds/#plotterspecflowsindexsubflowsindexstepsindexindexparametersapiconnection","text":"\u21a9 Parent Connection information Name Type Description Required name string Name of the connection to the data source true","title":"Plotter.spec.flows[index].subFlows[index].steps[index][index].parameters.api.connection"},{"location":"reference/crds/#plotterspecflowsindexsubflowsindexstepsindexindexparametersargsindex","text":"\u21a9 Parent StepArgument describes a step: it could be assetID or an endpoint of another step Name Type Description Required api object API holds information for accessing a module instance false assetId string AssetID identifies the source asset of this step false","title":"Plotter.spec.flows[index].subFlows[index].steps[index][index].parameters.args[index]"},{"location":"reference/crds/#plotterspecflowsindexsubflowsindexstepsindexindexparametersargsindexapi","text":"\u21a9 Parent API holds information for accessing a module instance Name Type Description Required connection object Connection information true dataFormat string Data format false","title":"Plotter.spec.flows[index].subFlows[index].steps[index][index].parameters.args[index].api"},{"location":"reference/crds/#plotterspecflowsindexsubflowsindexstepsindexindexparametersargsindexapiconnection","text":"\u21a9 Parent Connection information Name Type Description Required name string Name of the connection to the data source true","title":"Plotter.spec.flows[index].subFlows[index].steps[index][index].parameters.args[index].api.connection"},{"location":"reference/crds/#plotterspectemplateskey","text":"\u21a9 Parent Template contains basic information about the required modules to serve the fybrikapplication e.g., the module helm chart name. Name Type Description Required modules []object Modules is a list of dependent modules. e.g., if a plugin module is used then the service module is used in should appear first in the modules list of the same template. If the modules list contains more than one module, the first module in the list is referred to as the \"primary module\" of which all the parameters to this template are sent to. true name string Name of the template false","title":"Plotter.spec.templates[key]"},{"location":"reference/crds/#plotterspectemplateskeymodulesindex","text":"\u21a9 Parent ModuleInfo is a copy of FybrikModule Custom Resource. It contains information to instantiate resource of type FybrikModule. Name Type Description Required capability string Module capability true chart object Chart contains the information needed to use helm to install the capability true name string Name of the module true type string May be one of service, config or plugin Service: Means that the control plane deploys the component that performs the capability Config: Another pre-installed service performs the capability and the module deployed configures it for the particular workload or dataset Plugin: Indicates that this module performs a capability as part of another service or module rather than as a stand-alone module true externalServices []string External services that are required for functionality of the module. false scope enum Scope indicates at what level the capability is used: workload, asset, cluster If not indicated it is assumed to be asset Enum : asset, workload, cluster false","title":"Plotter.spec.templates[key].modules[index]"},{"location":"reference/crds/#plotterspectemplateskeymodulesindexchart","text":"\u21a9 Parent Chart contains the information needed to use helm to install the capability Name Type Description Required name string Name of helm chart true chartPullSecret string Name of secret containing helm registry credentials false values map[string]string Values to pass to helm chart installation false","title":"Plotter.spec.templates[key].modules[index].chart"},{"location":"reference/crds/#plotterspecappselector","text":"\u21a9 Parent Selector enables to connect the resource to the application Application labels should match the labels in the selector. For some flows the selector may not be used. Name Type Description Required workloadSelector object WorkloadSelector enables to connect the resource to a user application. Application labels should match the labels in the selector. true clusterName string Cluster name false ipBlocks []object IPBlocks define policy on particular IPBlocks. the structure of the IPBlock is defined at https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#ipblock-v1-networking-k8s-io false namespaces []string Namespaces where user application might run false","title":"Plotter.spec.appSelector"},{"location":"reference/crds/#plotterspecappselectorworkloadselector","text":"\u21a9 Parent WorkloadSelector enables to connect the resource to a user application. Application labels should match the labels in the selector. Name Type Description Required matchExpressions []object matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false","title":"Plotter.spec.appSelector.workloadSelector"},{"location":"reference/crds/#plotterspecappselectorworkloadselectormatchexpressionsindex","text":"\u21a9 Parent A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. Name Type Description Required key string key is the label key that the selector applies to. true operator string operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false","title":"Plotter.spec.appSelector.workloadSelector.matchExpressions[index]"},{"location":"reference/crds/#plotterspecappselectoripblocksindex","text":"\u21a9 Parent IPBlock describes a particular CIDR (Ex. \"192.168.1.1/24\",\"2001:db9::/64\") that is allowed to the pods matched by a NetworkPolicySpec's podSelector. The except entry describes CIDRs that should not be included within this rule. Name Type Description Required cidr string CIDR is a string representing the IP Block Valid examples are \"192.168.1.1/24\" or \"2001:db9::/64\" true except []string Except is a slice of CIDRs that should not be included within an IP Block Valid examples are \"192.168.1.1/24\" or \"2001:db9::/64\" Except values will be rejected if they are outside the CIDR range false","title":"Plotter.spec.appSelector.ipBlocks[index]"},{"location":"reference/crds/#plotterstatus","text":"\u21a9 Parent PlotterStatus defines the observed state of Plotter This includes readiness, error message, and indicators received from blueprint resources owned by the Plotter for cleanup and status monitoring Name Type Description Required assets map[string]object Assets is a map containing the status per asset. The key of this map is assetId false blueprints map[string]object false conditions []object Conditions represent the possible error and failure conditions false flows map[string]object Flows is a map containing the status for each flow the key is the flow name false observedGeneration integer ObservedGeneration is taken from the Plotter metadata. This is used to determine during reconcile whether reconcile was called because the desired state changed, or whether status of the allocated blueprints should be checked. Format : int64 false observedState object ObservedState includes information to be reported back to the FybrikApplication resource It includes readiness and error indications, as well as user instructions false readyTimestamp string Format : date-time false","title":"Plotter.status"},{"location":"reference/crds/#plotterstatusassetskey","text":"\u21a9 Parent ObservedState represents a part of the generated Blueprint/Plotter resource status that allows update of FybrikApplication status Name Type Description Required error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false","title":"Plotter.status.assets[key]"},{"location":"reference/crds/#plotterstatusblueprintskey","text":"\u21a9 Parent MetaBlueprint defines blueprint metadata (name, namespace) and status Name Type Description Required name string true namespace string true status object BlueprintStatus defines the observed state of Blueprint This includes readiness, error message, and indicators for the Kubernetes resources owned by the Blueprint for cleanup and status monitoring true","title":"Plotter.status.blueprints[key]"},{"location":"reference/crds/#plotterstatusblueprintskeystatus","text":"\u21a9 Parent BlueprintStatus defines the observed state of Blueprint This includes readiness, error message, and indicators for the Kubernetes resources owned by the Blueprint for cleanup and status monitoring Name Type Description Required modules map[string]object ModulesState is a map which holds the status of each module its key is the moduleInstanceName which is the unique name for the deployed instance related to this workload false observedGeneration integer ObservedGeneration is taken from the Blueprint metadata. This is used to determine during reconcile whether reconcile was called because the desired state changed, or whether status of the allocated resources should be checked. Format : int64 false observedState object ObservedState includes information to be reported back to the FybrikApplication resource It includes readiness and error indications, as well as user instructions false releases map[string]integer Releases map each release to the observed generation of the blueprint containing this release. At the end of reconcile, each release should be mapped to the latest blueprint version or be uninstalled. false","title":"Plotter.status.blueprints[key].status"},{"location":"reference/crds/#plotterstatusblueprintskeystatusmoduleskey","text":"\u21a9 Parent ObservedState represents a part of the generated Blueprint/Plotter resource status that allows update of FybrikApplication status Name Type Description Required error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false","title":"Plotter.status.blueprints[key].status.modules[key]"},{"location":"reference/crds/#plotterstatusblueprintskeystatusobservedstate","text":"\u21a9 Parent ObservedState includes information to be reported back to the FybrikApplication resource It includes readiness and error indications, as well as user instructions Name Type Description Required error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false","title":"Plotter.status.blueprints[key].status.observedState"},{"location":"reference/crds/#plotterstatusconditionsindex","text":"\u21a9 Parent Condition describes the state of a FybrikApplication at a certain point. Name Type Description Required type string Type of the condition true message string Message contains the details of the current condition false observedGeneration integer ObservedGeneration is the version of the resource for which the condition has been evaluated Format : int64 false status enum Status of the condition, one of (`True`, `False`, `Unknown`). Enum : True, False, Unknown Default : Unknown false","title":"Plotter.status.conditions[index]"},{"location":"reference/crds/#plotterstatusflowskey","text":"\u21a9 Parent FlowStatus includes information to be reported back to the FybrikApplication resource It holds the status per data flow Name Type Description Required subFlows map[string]object true status object ObservedState includes information about the current flow It includes readiness and error indications, as well as user instructions false","title":"Plotter.status.flows[key]"},{"location":"reference/crds/#plotterstatusflowskeysubflowskey","text":"\u21a9 Parent ObservedState represents a part of the generated Blueprint/Plotter resource status that allows update of FybrikApplication status Name Type Description Required error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false","title":"Plotter.status.flows[key].subFlows[key]"},{"location":"reference/crds/#plotterstatusflowskeystatus","text":"\u21a9 Parent ObservedState includes information about the current flow It includes readiness and error indications, as well as user instructions Name Type Description Required error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false","title":"Plotter.status.flows[key].status"},{"location":"reference/crds/#plotterstatusobservedstate","text":"\u21a9 Parent ObservedState includes information to be reported back to the FybrikApplication resource It includes readiness and error indications, as well as user instructions Name Type Description Required error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false","title":"Plotter.status.observedState"},{"location":"reference/crds/#appfybrikiov1beta2","text":"Resource Types: FybrikStorageAccount","title":"app.fybrik.io/v1beta2"},{"location":"reference/crds/#fybrikstorageaccount_1","text":"\u21a9 Parent FybrikStorageAccount is a storage account Fybrik uses to dynamically allocate space for datasets whose creation or copy it orchestrates. Name Type Description Required apiVersion string app.fybrik.io/v1beta2 true kind string FybrikStorageAccount true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object FybrikStorageAccountSpec defines the desired state of FybrikStorageAccount true status object FybrikStorageAccountStatus defines the observed state of FybrikStorageAccount false","title":"FybrikStorageAccount"},{"location":"reference/crds/#fybrikstorageaccountspec_1","text":"\u21a9 Parent FybrikStorageAccountSpec defines the desired state of FybrikStorageAccount Name Type Description Required geography string Storage geography true id string Identification of a storage account true secretRef string A name of k8s secret deployed in the control plane. true type string Type of the storage, e.g., s3 true","title":"FybrikStorageAccount.spec"},{"location":"reference/crds/#katalogfybrikiov1alpha1","text":"Resource Types: Asset","title":"katalog.fybrik.io/v1alpha1"},{"location":"reference/crds/#asset","text":"\u21a9 Parent Asset defines an asset in the catalog Name Type Description Required apiVersion string katalog.fybrik.io/v1alpha1 true kind string Asset true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object true","title":"Asset"},{"location":"reference/crds/#assetspec","text":"\u21a9 Parent Name Type Description Required details object Asset details true metadata object Asset metadata true secretRef object Reference to a Secret resource holding credentials for this asset true","title":"Asset.spec"},{"location":"reference/crds/#assetspecdetails","text":"\u21a9 Parent Asset details Name Type Description Required connection object Connection information true dataFormat string Data format false","title":"Asset.spec.details"},{"location":"reference/crds/#assetspecdetailsconnection","text":"\u21a9 Parent Connection information Name Type Description Required name string Name of the connection to the data source true","title":"Asset.spec.details.connection"},{"location":"reference/crds/#assetspecmetadata","text":"\u21a9 Parent Asset metadata Name Type Description Required columns []object Columns associated with the asset false geography string Geography of the resource false name string Name of the resource false owner string Owner of the resource false tags object Tags associated with the asset false","title":"Asset.spec.metadata"},{"location":"reference/crds/#assetspecmetadatacolumnsindex","text":"\u21a9 Parent ResourceColumn represents a column in a tabular resource Name Type Description Required name string Name of the column true tags object Tags associated with the column false","title":"Asset.spec.metadata.columns[index]"},{"location":"reference/crds/#assetspecsecretref","text":"\u21a9 Parent Reference to a Secret resource holding credentials for this asset Name Type Description Required name string Name of the Secret resource true namespace string Namespace of the Secret resource. If it is empty then the asset namespace is used. false","title":"Asset.spec.secretRef"},{"location":"reference/ddc/","text":"Data Distribution Controller Overview of Requirements and Functionality The Data Distribution Controller (DDC) handles the movement of data between data stores. Fybrik uses the DDC to perform an action called \"implicit copy\", i.e. the movement of a data set from one data store to another with possibly some unitary transform applied to that data set. It corresponds to Step 8 in the Architecture . Data can be copied from data store to data store in a large variety of different ways, depending on the types of the data store (e.g. COS, Relational DB) and nature of the data capture (Streamed, Snapshot). This document defines the functionality as well as the boundary conditions of the data distribution controller. Goals This document introduces fundamental concepts of the data distribution component and describes a high-level API for invoking data distributions. The initial focus is on structured (tabular) data. One goal of the data distribution component is to maximize congruence across different data stores and formats by preserving not only the data content but also the structure of the data as faithfully as possible. Fully unstructured data such (e.g. \"binary content\") will also be supported but that is not the focus of the initial version. Semi-structured data will be supported on a case-by-case basis. Non-Goals The focus is on how to invoke data distribution and not the if and when. This document doesn't describe the control component that is required to decide whether, when and how often data should be copied across storage systems. Neither does the data distribution perform any policy enforcement. This is done by the component that controls the data distribution system. High-Level Design Before providing an outline of the API functionality, some fundamental concepts are defined. Data Sets and Data Assets The following definition is aligned with the terminology used in the Watson Knowledge Catalog. Data is organized into data sets and data assets. A data set is a collection of data assets that is administered by a single body using a set of policies. Both data sets and data assets are uniquely identifiable. A data set is a collection of data assets with the same structure. Some examples: A data set is data that resides in a relational database where the database tables or views form the data asset. A data set consisting of objects that reside in a COS bucket where object prefix paths that have a common format are data assets. E.g. a set of partitioned parquet files with the same schema. A data set may be formed by a set of Kafka topics where each topic contains messages in compatible format. A data asset is represented by the content of the topic. The unit of data distribution is the data asset. Data Stores A data store allows access to data sets and data assets. Each store allows to individually access data through a data store specific API, e.g. S3 API or JDBC. Additional properties that are relevant for data distribution: Granularity of data access for reading: Some systems provide access to entire data assets only. (e.g. single unpartitioned files on COS). Other storage systems support queries to retrieve a sub-set (a selection and/or projection) of an individual data assets. (e.g. queries on Db2 or partitioned/bucketed prefixes on COS) Granularity of data access for writing: Fine-granular write access is required to apply delta-updates of individual data assets, i.e. update and insert ( upsert ) operations as well as deletes on record level are needed to process streams of changes. Systems that support fine-granular updates are relational database systems, elastic search indexes, and in-memory caches. Other systems such as traditional parquet files stored on COS or HDFS only allow data assets to be updated in their entirety. More sophisticated storage formats such as Delta Lake , Apache Iceberg or Hudi extend the capabilities of parquet. Fidelity of the type system: Data stores use various different typing systems and have different data models that require type conversions as data is distributed between these systems. For example, when moving the content of an elastic search index into a relational database we are moving between two entirely different data models. In order to minimize loss of information, type specific metadata (technical metadata) may need to be preserved as separate entities. In addition, schema inference might be needed to support certain data distributions. The invoker of the DDC is assumed to have knowledge of the technical metadata present at the source data asset and of the desired technical metadata of that data asset at the target. If the invoker does not specify this the DDC will attempt to infer it where possible. In both cases the source and target technical metadata are returned as part of the result of the data distribution. If the passed source or target technical metadata is inconsistent with the data asset at the source, then the data distribution fails. The version 1.0 of the DDC supports the following data stores: Db2 LUW v10.5 or newer Apache Kafka v2.2 or newer (Raw + Confluent KTable format serialized in JSON or Avro) IBM COS with Parquet, JSON and ORC (using a Stocator based approach) Transformations The data distribution supports simple transformations and filtering such as: - Filtering of individual rows or columns based on condition clauses. - Masking of specific columns. - Encrypting/hashing of specific columns. - Sampling of a subset of rows. This is specifically for creating a derived version of a specific data asset and is NOT to enrich or combine data assets, i.e. this is a not a general purpose computation environment. Data Life-cycle The DDC moves a data asset from a source to a target data store. The copy of the data asset will be retained at the target until explicitly removed by the invoker via the DDC API. API High-level Description The API follows the custom resource definition approach (CRD) for Kubernetes. The following basic CRD types exist: - BatchTransfer : One-time or periodic transfer of a single data asset from a source data store to a destination data store. This is also called snapshotting. This is similar to a job in K8s and will inherit many features from it, e.g. the state is kept in K8s after the batch transfer has completed and must be deleted manually. - SyncTransfer : Continuous synchronization of a single data asset from a source data store to a destination data store. The main use-case is to continuously update a destination data asset as it is typically used in a streaming or change-data-capture scenario. This CRD is similar to a stateful set in K8s. Both transfer types will have the same API concerning the core transfer definitions such as: - The source data store including connection details and data asset. - The path (in Vault) to the credentials required to access the source data store. - The destination data store including connection details and data asset. - The path (in Vault) to the credentials required to access the destination data store. - Transfer properties that define parameters such as schedule, retries, transformations etc. The difference is that SyncTransfer is running continuously, BatchTransfer requires a schedule or is a one-time transfer. Initially we will limit SyncTransfer to the movement of data from Kafka to COS or from Kafka to Db2. The status of the CRD is continuously updated with the state of the data distribution. It is used to detect both success or error situations as well as freshness. It also provides transfer statistics. Using the status of the CRD a user may examine: - where data assets have been moved - when this was last successfully completed (for _BatchTransfer_s) - statistics, i.e. how long this took, how many bytes, rows etc. were transferred - what technical metadata about the data was used at the source/destination Other K8s controllers can watch the objects and subscribe to statistics or technical metadata updates and forward these changes e.g. in dashboards or WKT. Secret management The data distribution API should not define any secrets in the CRD spec in a production environment. For development and testing direct definitions can be used but in a production environment credentials shall be retrieved from the secret provider. The secret provider can be accessed via a REST API using a role and a secret name. This secret name refers to a path in vault. At the movement operator shall not create any secrets in Kubernetes that contain any credentials and credentials shall only be maintained in memory. The fetching of secrets will be executed by the datamover component. The datamover component retrieves configuration from a JSON file that is passed on as a Kubernetes secret. The goal is that vault paths can be specified in this JSON configuration file and will be substituted by values retrieved from the secret provider. The following example illustrates this mechanism: Given the example configuration file: { \"db2URL\": \"jdbc:db2//host1:1234/MYDB\", \"user\": \"myuser\" \"vaultPath\": \"/myvault/db2Password\" } and the following string in vault: {\"password\": \"mypassword\"} The substitution in the datamover will find a JSON field called vaultPath and look up the value using the secret provider. The substitution happens at the same level as the vaultPath field was found. This works whenever the data that is stored in vault is a JSON object itself. The advantage is that the in-memory configuration will be the same as in a dev/test environment after the substitution. The result of the given example after substitution will be: { \"db2URL\": \"jdbc:db2//host1:1234/MYDB\", \"user\": \"myuser\" \"password\": \"mypassword\" } This credential substitution can also be used in the options field of transformations. Error handling The data distribution API is using validation hooks to do simple checks when a CRD is created or updated. This is a first kind of error that will result in an error when creating/updating the CRD. It will specify an error message about which fields are not valid. (e.g. an invalid cron pattern for the schedule property) As validation errors are checked before objects are created they return an error via the Kubernetes API. If an error occurred during a BatchTransfer the status of the CRD will be set to FAILED and a possible error reason will show in the error field. The error messages will differ depending on the type of exception that is thrown in the internal datamover process. The internal datamover process will communicate errors to Kubernetes via a termination message . The content of the termination message will be written into the error field of the BatchTransfer . The error message shall describe the error as good as possible without any stack traces to keep it readable and displayable in a short form. Actions for possible error states: * Pending - Nothing to do. Normal process * Running - Nothing to do. Normal process * Succeeded - Possibly execute on succeeded actions (e.g. updating a catalog, ...) * Failed - Operator will try to recover. * Fatal - Operator could not recover. Possibly recreate CRD to resolve and investigate error further. Events In addition to errors the datamover application that is called by the data distribution api will publish Kubernetes events for the CRD in order to give feedback for errors and successes. Errors will contain the error message. Successful messages will contain additional metrics such as number of transferred rows or technical metadata information. API Specification The formalism to use to describe this is to be decided, possibilities are Go using kubebuilder OR CRD directly. As the definition of transfer specific parameters is the same for BatchTransfer kind and SyncTransfer kind the definition below focusses on the BatchTransfer kind. (Think of it like a pod template definition that is the same for a job or a deployment) A possible but not complete list of Go structs using kubebuilder is: // BatchTransferSpec defines the desired state of BatchTransfer type BatchTransferSpec struct { Source DataStore `json:\"source\"` Destination DataStore `json:\"destination\"` Transformation []Transformation `json:\"transformation,omitempty\"` Schedule string `json:\"schedule,omitempty\"` Image string `json:\"image\"` // Has default value from webhook ImagePullPolicy corev1.PullPolicy `json:\"imagePullPolicy\"` // Has default value from webhook SecretProviderURL string `json:\"secretProviderURL\"` // Has default value from webhook SecretProviderRole string `json:\"secretProviderRole\"` // Has default value from webhook Suspend bool `json:\"suspend,omitempty\"` // Has default value from webhook MaxFailedRetries int `json:\"maxFailedRetries,omitempty\"` // Has default value from webhook SuccessfulJobHistoryLimit int `json:\"successfulJobHistoryLimit,omitempty\"` // Has default value from webhook FailedJobHistoryLimit int `json:\"failedJobHistoryLimit,omitempty\"` // Has default value from webhook } type DataStore struct { DataAsset string `json:\"dataAsset\"` Database *Database `json:\"database,omitempty\"` S3 *S3 `json:\"s3,omitempty\"` Kafka *Kafka `json:\"kafka,omitempty\"` } type Database struct { Db2URL string `json:\"db2URL\"` User string `json:\"user\"` Password *string `json:\"password,omitempty\"` // Please use for dev/test only! VaultPath *string `json:\"vaultPath,omitempty\"` } type S3 struct { Endpoint string `json:\"endpoint\"` Region string `json:\"region,omitempty\"` Bucket string `json:\"bucket\"` AccessKey *string `json:\"accessKey,omitempty\"` // Please use for dev/test only! SecretKey *string `json:\"secretKey,omitempty\"` // Please use for dev/test only! VaultPath *string `json:\"vaultPath,omitempty\"` ObjectKey string `json:\"objectKey\"` DataFormat string `json:\"dataFormat,omitempty\"` } type Kafka struct { KafkaBrokers string `json:\"kafkaBrokers\"` SchemaRegistryURL string `json:\"schemaRegistryURL\"` User string `json:\"user\"` Password *string `json:\"password,omitempty\"` // Please use for dev/test only! VaultPath *string `json:\"vaultPath,omitempty\"` SslTruststoreLocation string `json:\"sslTruststoreLocation,omitempty\"` SslTruststorePassword string `json:\"sslTruststorePassword,omitempty\"` KafkaTopic string `json:\"kafkaTopic\"` CreateSnapshot bool `json:\"createSnapshot,omitempty\"` } type Transformation struct { Name string `json:\"name,omitempty\"` Action Action `json:\"action,omitempty\"` Columns []string `json:\"columns,omitempty\"` Options map[string]string `json:\"options,omitempty\"` } type Action string const ( RemoveColumn Action = \"RemoveColumn\" Filter Action = \"Filter\" Encrypt Action = \"Encrypt\" Sample Action = \"Sample\" Digest Action = \"Digest\" // md5, sha1, crc32, sha256, sha512, xxhash32, xxhash64, murmur32 Redact Action = \"Redact\" // random, fixed, formatted, etc ) // BatchTransferStatus defines the observed state of BatchTransfer type BatchTransferStatus struct { Active *corev1.ObjectReference `json:\"active,omitempty\"` Status Status `json:\"status,omitempty\"` Error string `json:\"status,omitempty\"` LastCompleted *corev1.ObjectReference `json:\"lastCompleted,omitempty\"` LastFailed *corev1.ObjectReference `json:\"lastFailed,omitempty\"` LastSuccessTime *metav1.Time `json:\"lastSuccessTime,omitempty\"` LastRecordTime *metav1.Time `json:\"lastRecordTime,omitempty\"` NumRecords int64 `json:\"numRecords,omitempty\"` LastScheduleTime *metav1.Time `json:\"lastScheduleTime,omitempty\"` } // +kubebuilder:validation:Enum=Pending;Running;Succeeded;Failed;Fatal;ConfigurationError type Status string const ( Pending Status = \"Pending\" // Starting up transfers Running Status = \"Running\" // Transfers are running Succeeded Status = \"Succeeded\" // Transfers succeeded Failed Status = \"Failed\" // Transfers failed (Maybe recoverable (e.g. temporary connection issues)) Fatal Status = \"Fatal\" // Fatal. Cannot recover. Manual intervention needed ) // +kubebuilder:object:root=true // BatchTransfer is the Schema for the batchtransfers API type BatchTransfer struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` Spec BatchTransferSpec `json:\"spec,omitempty\"` Status BatchTransferStatus `json:\"status,omitempty\"` } // +kubebuilder:object:root=true // BatchTransferList contains a list of BatchTransfer type BatchTransferList struct { metav1.TypeMeta `json:\",inline\"` metav1.ListMeta `json:\"metadata,omitempty\"` Assets []BatchTransfer `json:\"assets\"` } Examples --- apiVersion: \"fybrik.io/v1\" kind: BatchTransfer metadata: name: copy1 namespace: myNamespace spec: source: db: db2URL: \"jdbc:db2://db1.psvc-dev.zc2.ibm.com:50602/DHUBS:sslConnection=true;\" user: myuser password: \"mypassword\" destination: cos: endpoint: s3... bucket: myBucket accessKey: 0123 secretKey: 0123 transformation: - name: \"Remove column A\" action: \"RemoveColumn\" columns: [\"A\"] - name: \"Digest column B\" action: \"Digest\" columns: [\"B\"] options: algo: \"md5\" schedule: null # Cron schedule definition if needed maxFailedRetries: 3 # Maximum retries if failed suspend: false successfulJobsHistoryLimit: 2 failedJobsHistoryLimit: 5 status: lastCompleted: corev1.ObjectReference # Reference to child K8s objects lastScheduledTime: 2018-01-01T00:00:00Z lastSuccessTime: 2018-01-01T00:00:00Z lastRecordTime: 2018-01-01T00:00:00Z # inspect data? numRecords: 23113 External Dependencies Data distribution will be implemented in different ways, depending on the distribution kind, on the source and destination data store technologies as well as depending on the requested transformations. The control layer of the data distribution is implemented following the operator pattern of Kubernetes. In addition, the following technologies are relevant for specific distribution scenarios: - Redhat Debezium for Change Data Capture - IBM Event Streams (Apache Kafka) for SyncTransfer - Apache Spark - Db2 client - COS client - Reference to IBM Specific JDBC driver for streaming into a relation database. Relevant Code Repositories The data distribution core libraries that are Scala/Spark based The data distribution operator that is the operator of data distribution and will be integrated into Fybrik code as part of the manager in future. Roadmap Integration with Parquet Encryption + KeyProtect (As Target) Integration with Iceberg (As Target) Integration with Relational Databases (As Target) Integration with KTables (As Source)","title":"Data Distribution Controller"},{"location":"reference/ddc/#data-distribution-controller","text":"","title":"Data Distribution Controller"},{"location":"reference/ddc/#overview-of-requirements-and-functionality","text":"The Data Distribution Controller (DDC) handles the movement of data between data stores. Fybrik uses the DDC to perform an action called \"implicit copy\", i.e. the movement of a data set from one data store to another with possibly some unitary transform applied to that data set. It corresponds to Step 8 in the Architecture . Data can be copied from data store to data store in a large variety of different ways, depending on the types of the data store (e.g. COS, Relational DB) and nature of the data capture (Streamed, Snapshot). This document defines the functionality as well as the boundary conditions of the data distribution controller.","title":"Overview of Requirements and Functionality"},{"location":"reference/ddc/#goals","text":"This document introduces fundamental concepts of the data distribution component and describes a high-level API for invoking data distributions. The initial focus is on structured (tabular) data. One goal of the data distribution component is to maximize congruence across different data stores and formats by preserving not only the data content but also the structure of the data as faithfully as possible. Fully unstructured data such (e.g. \"binary content\") will also be supported but that is not the focus of the initial version. Semi-structured data will be supported on a case-by-case basis.","title":"Goals"},{"location":"reference/ddc/#non-goals","text":"The focus is on how to invoke data distribution and not the if and when. This document doesn't describe the control component that is required to decide whether, when and how often data should be copied across storage systems. Neither does the data distribution perform any policy enforcement. This is done by the component that controls the data distribution system.","title":"Non-Goals"},{"location":"reference/ddc/#high-level-design","text":"Before providing an outline of the API functionality, some fundamental concepts are defined.","title":"High-Level Design"},{"location":"reference/ddc/#data-sets-and-data-assets","text":"The following definition is aligned with the terminology used in the Watson Knowledge Catalog. Data is organized into data sets and data assets. A data set is a collection of data assets that is administered by a single body using a set of policies. Both data sets and data assets are uniquely identifiable. A data set is a collection of data assets with the same structure. Some examples: A data set is data that resides in a relational database where the database tables or views form the data asset. A data set consisting of objects that reside in a COS bucket where object prefix paths that have a common format are data assets. E.g. a set of partitioned parquet files with the same schema. A data set may be formed by a set of Kafka topics where each topic contains messages in compatible format. A data asset is represented by the content of the topic. The unit of data distribution is the data asset.","title":"Data Sets and Data Assets"},{"location":"reference/ddc/#data-stores","text":"A data store allows access to data sets and data assets. Each store allows to individually access data through a data store specific API, e.g. S3 API or JDBC. Additional properties that are relevant for data distribution: Granularity of data access for reading: Some systems provide access to entire data assets only. (e.g. single unpartitioned files on COS). Other storage systems support queries to retrieve a sub-set (a selection and/or projection) of an individual data assets. (e.g. queries on Db2 or partitioned/bucketed prefixes on COS) Granularity of data access for writing: Fine-granular write access is required to apply delta-updates of individual data assets, i.e. update and insert ( upsert ) operations as well as deletes on record level are needed to process streams of changes. Systems that support fine-granular updates are relational database systems, elastic search indexes, and in-memory caches. Other systems such as traditional parquet files stored on COS or HDFS only allow data assets to be updated in their entirety. More sophisticated storage formats such as Delta Lake , Apache Iceberg or Hudi extend the capabilities of parquet. Fidelity of the type system: Data stores use various different typing systems and have different data models that require type conversions as data is distributed between these systems. For example, when moving the content of an elastic search index into a relational database we are moving between two entirely different data models. In order to minimize loss of information, type specific metadata (technical metadata) may need to be preserved as separate entities. In addition, schema inference might be needed to support certain data distributions. The invoker of the DDC is assumed to have knowledge of the technical metadata present at the source data asset and of the desired technical metadata of that data asset at the target. If the invoker does not specify this the DDC will attempt to infer it where possible. In both cases the source and target technical metadata are returned as part of the result of the data distribution. If the passed source or target technical metadata is inconsistent with the data asset at the source, then the data distribution fails. The version 1.0 of the DDC supports the following data stores: Db2 LUW v10.5 or newer Apache Kafka v2.2 or newer (Raw + Confluent KTable format serialized in JSON or Avro) IBM COS with Parquet, JSON and ORC (using a Stocator based approach)","title":"Data Stores"},{"location":"reference/ddc/#transformations","text":"The data distribution supports simple transformations and filtering such as: - Filtering of individual rows or columns based on condition clauses. - Masking of specific columns. - Encrypting/hashing of specific columns. - Sampling of a subset of rows. This is specifically for creating a derived version of a specific data asset and is NOT to enrich or combine data assets, i.e. this is a not a general purpose computation environment.","title":"Transformations"},{"location":"reference/ddc/#data-life-cycle","text":"The DDC moves a data asset from a source to a target data store. The copy of the data asset will be retained at the target until explicitly removed by the invoker via the DDC API.","title":"Data Life-cycle"},{"location":"reference/ddc/#api-high-level-description","text":"The API follows the custom resource definition approach (CRD) for Kubernetes. The following basic CRD types exist: - BatchTransfer : One-time or periodic transfer of a single data asset from a source data store to a destination data store. This is also called snapshotting. This is similar to a job in K8s and will inherit many features from it, e.g. the state is kept in K8s after the batch transfer has completed and must be deleted manually. - SyncTransfer : Continuous synchronization of a single data asset from a source data store to a destination data store. The main use-case is to continuously update a destination data asset as it is typically used in a streaming or change-data-capture scenario. This CRD is similar to a stateful set in K8s. Both transfer types will have the same API concerning the core transfer definitions such as: - The source data store including connection details and data asset. - The path (in Vault) to the credentials required to access the source data store. - The destination data store including connection details and data asset. - The path (in Vault) to the credentials required to access the destination data store. - Transfer properties that define parameters such as schedule, retries, transformations etc. The difference is that SyncTransfer is running continuously, BatchTransfer requires a schedule or is a one-time transfer. Initially we will limit SyncTransfer to the movement of data from Kafka to COS or from Kafka to Db2. The status of the CRD is continuously updated with the state of the data distribution. It is used to detect both success or error situations as well as freshness. It also provides transfer statistics. Using the status of the CRD a user may examine: - where data assets have been moved - when this was last successfully completed (for _BatchTransfer_s) - statistics, i.e. how long this took, how many bytes, rows etc. were transferred - what technical metadata about the data was used at the source/destination Other K8s controllers can watch the objects and subscribe to statistics or technical metadata updates and forward these changes e.g. in dashboards or WKT.","title":"API High-level Description"},{"location":"reference/ddc/#secret-management","text":"The data distribution API should not define any secrets in the CRD spec in a production environment. For development and testing direct definitions can be used but in a production environment credentials shall be retrieved from the secret provider. The secret provider can be accessed via a REST API using a role and a secret name. This secret name refers to a path in vault. At the movement operator shall not create any secrets in Kubernetes that contain any credentials and credentials shall only be maintained in memory. The fetching of secrets will be executed by the datamover component. The datamover component retrieves configuration from a JSON file that is passed on as a Kubernetes secret. The goal is that vault paths can be specified in this JSON configuration file and will be substituted by values retrieved from the secret provider. The following example illustrates this mechanism: Given the example configuration file: { \"db2URL\": \"jdbc:db2//host1:1234/MYDB\", \"user\": \"myuser\" \"vaultPath\": \"/myvault/db2Password\" } and the following string in vault: {\"password\": \"mypassword\"} The substitution in the datamover will find a JSON field called vaultPath and look up the value using the secret provider. The substitution happens at the same level as the vaultPath field was found. This works whenever the data that is stored in vault is a JSON object itself. The advantage is that the in-memory configuration will be the same as in a dev/test environment after the substitution. The result of the given example after substitution will be: { \"db2URL\": \"jdbc:db2//host1:1234/MYDB\", \"user\": \"myuser\" \"password\": \"mypassword\" } This credential substitution can also be used in the options field of transformations.","title":"Secret management"},{"location":"reference/ddc/#error-handling","text":"The data distribution API is using validation hooks to do simple checks when a CRD is created or updated. This is a first kind of error that will result in an error when creating/updating the CRD. It will specify an error message about which fields are not valid. (e.g. an invalid cron pattern for the schedule property) As validation errors are checked before objects are created they return an error via the Kubernetes API. If an error occurred during a BatchTransfer the status of the CRD will be set to FAILED and a possible error reason will show in the error field. The error messages will differ depending on the type of exception that is thrown in the internal datamover process. The internal datamover process will communicate errors to Kubernetes via a termination message . The content of the termination message will be written into the error field of the BatchTransfer . The error message shall describe the error as good as possible without any stack traces to keep it readable and displayable in a short form. Actions for possible error states: * Pending - Nothing to do. Normal process * Running - Nothing to do. Normal process * Succeeded - Possibly execute on succeeded actions (e.g. updating a catalog, ...) * Failed - Operator will try to recover. * Fatal - Operator could not recover. Possibly recreate CRD to resolve and investigate error further.","title":"Error handling"},{"location":"reference/ddc/#events","text":"In addition to errors the datamover application that is called by the data distribution api will publish Kubernetes events for the CRD in order to give feedback for errors and successes. Errors will contain the error message. Successful messages will contain additional metrics such as number of transferred rows or technical metadata information.","title":"Events"},{"location":"reference/ddc/#api-specification","text":"The formalism to use to describe this is to be decided, possibilities are Go using kubebuilder OR CRD directly. As the definition of transfer specific parameters is the same for BatchTransfer kind and SyncTransfer kind the definition below focusses on the BatchTransfer kind. (Think of it like a pod template definition that is the same for a job or a deployment) A possible but not complete list of Go structs using kubebuilder is: // BatchTransferSpec defines the desired state of BatchTransfer type BatchTransferSpec struct { Source DataStore `json:\"source\"` Destination DataStore `json:\"destination\"` Transformation []Transformation `json:\"transformation,omitempty\"` Schedule string `json:\"schedule,omitempty\"` Image string `json:\"image\"` // Has default value from webhook ImagePullPolicy corev1.PullPolicy `json:\"imagePullPolicy\"` // Has default value from webhook SecretProviderURL string `json:\"secretProviderURL\"` // Has default value from webhook SecretProviderRole string `json:\"secretProviderRole\"` // Has default value from webhook Suspend bool `json:\"suspend,omitempty\"` // Has default value from webhook MaxFailedRetries int `json:\"maxFailedRetries,omitempty\"` // Has default value from webhook SuccessfulJobHistoryLimit int `json:\"successfulJobHistoryLimit,omitempty\"` // Has default value from webhook FailedJobHistoryLimit int `json:\"failedJobHistoryLimit,omitempty\"` // Has default value from webhook } type DataStore struct { DataAsset string `json:\"dataAsset\"` Database *Database `json:\"database,omitempty\"` S3 *S3 `json:\"s3,omitempty\"` Kafka *Kafka `json:\"kafka,omitempty\"` } type Database struct { Db2URL string `json:\"db2URL\"` User string `json:\"user\"` Password *string `json:\"password,omitempty\"` // Please use for dev/test only! VaultPath *string `json:\"vaultPath,omitempty\"` } type S3 struct { Endpoint string `json:\"endpoint\"` Region string `json:\"region,omitempty\"` Bucket string `json:\"bucket\"` AccessKey *string `json:\"accessKey,omitempty\"` // Please use for dev/test only! SecretKey *string `json:\"secretKey,omitempty\"` // Please use for dev/test only! VaultPath *string `json:\"vaultPath,omitempty\"` ObjectKey string `json:\"objectKey\"` DataFormat string `json:\"dataFormat,omitempty\"` } type Kafka struct { KafkaBrokers string `json:\"kafkaBrokers\"` SchemaRegistryURL string `json:\"schemaRegistryURL\"` User string `json:\"user\"` Password *string `json:\"password,omitempty\"` // Please use for dev/test only! VaultPath *string `json:\"vaultPath,omitempty\"` SslTruststoreLocation string `json:\"sslTruststoreLocation,omitempty\"` SslTruststorePassword string `json:\"sslTruststorePassword,omitempty\"` KafkaTopic string `json:\"kafkaTopic\"` CreateSnapshot bool `json:\"createSnapshot,omitempty\"` } type Transformation struct { Name string `json:\"name,omitempty\"` Action Action `json:\"action,omitempty\"` Columns []string `json:\"columns,omitempty\"` Options map[string]string `json:\"options,omitempty\"` } type Action string const ( RemoveColumn Action = \"RemoveColumn\" Filter Action = \"Filter\" Encrypt Action = \"Encrypt\" Sample Action = \"Sample\" Digest Action = \"Digest\" // md5, sha1, crc32, sha256, sha512, xxhash32, xxhash64, murmur32 Redact Action = \"Redact\" // random, fixed, formatted, etc ) // BatchTransferStatus defines the observed state of BatchTransfer type BatchTransferStatus struct { Active *corev1.ObjectReference `json:\"active,omitempty\"` Status Status `json:\"status,omitempty\"` Error string `json:\"status,omitempty\"` LastCompleted *corev1.ObjectReference `json:\"lastCompleted,omitempty\"` LastFailed *corev1.ObjectReference `json:\"lastFailed,omitempty\"` LastSuccessTime *metav1.Time `json:\"lastSuccessTime,omitempty\"` LastRecordTime *metav1.Time `json:\"lastRecordTime,omitempty\"` NumRecords int64 `json:\"numRecords,omitempty\"` LastScheduleTime *metav1.Time `json:\"lastScheduleTime,omitempty\"` } // +kubebuilder:validation:Enum=Pending;Running;Succeeded;Failed;Fatal;ConfigurationError type Status string const ( Pending Status = \"Pending\" // Starting up transfers Running Status = \"Running\" // Transfers are running Succeeded Status = \"Succeeded\" // Transfers succeeded Failed Status = \"Failed\" // Transfers failed (Maybe recoverable (e.g. temporary connection issues)) Fatal Status = \"Fatal\" // Fatal. Cannot recover. Manual intervention needed ) // +kubebuilder:object:root=true // BatchTransfer is the Schema for the batchtransfers API type BatchTransfer struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` Spec BatchTransferSpec `json:\"spec,omitempty\"` Status BatchTransferStatus `json:\"status,omitempty\"` } // +kubebuilder:object:root=true // BatchTransferList contains a list of BatchTransfer type BatchTransferList struct { metav1.TypeMeta `json:\",inline\"` metav1.ListMeta `json:\"metadata,omitempty\"` Assets []BatchTransfer `json:\"assets\"` }","title":"API Specification"},{"location":"reference/ddc/#examples","text":"--- apiVersion: \"fybrik.io/v1\" kind: BatchTransfer metadata: name: copy1 namespace: myNamespace spec: source: db: db2URL: \"jdbc:db2://db1.psvc-dev.zc2.ibm.com:50602/DHUBS:sslConnection=true;\" user: myuser password: \"mypassword\" destination: cos: endpoint: s3... bucket: myBucket accessKey: 0123 secretKey: 0123 transformation: - name: \"Remove column A\" action: \"RemoveColumn\" columns: [\"A\"] - name: \"Digest column B\" action: \"Digest\" columns: [\"B\"] options: algo: \"md5\" schedule: null # Cron schedule definition if needed maxFailedRetries: 3 # Maximum retries if failed suspend: false successfulJobsHistoryLimit: 2 failedJobsHistoryLimit: 5 status: lastCompleted: corev1.ObjectReference # Reference to child K8s objects lastScheduledTime: 2018-01-01T00:00:00Z lastSuccessTime: 2018-01-01T00:00:00Z lastRecordTime: 2018-01-01T00:00:00Z # inspect data? numRecords: 23113","title":"Examples"},{"location":"reference/ddc/#external-dependencies","text":"Data distribution will be implemented in different ways, depending on the distribution kind, on the source and destination data store technologies as well as depending on the requested transformations. The control layer of the data distribution is implemented following the operator pattern of Kubernetes. In addition, the following technologies are relevant for specific distribution scenarios: - Redhat Debezium for Change Data Capture - IBM Event Streams (Apache Kafka) for SyncTransfer - Apache Spark - Db2 client - COS client - Reference to IBM Specific JDBC driver for streaming into a relation database.","title":"External Dependencies"},{"location":"reference/ddc/#relevant-code-repositories","text":"The data distribution core libraries that are Scala/Spark based The data distribution operator that is the operator of data distribution and will be integrated into Fybrik code as part of the manager in future.","title":"Relevant Code Repositories"},{"location":"reference/ddc/#roadmap","text":"Integration with Parquet Encryption + KeyProtect (As Target) Integration with Iceberg (As Target) Integration with Relational Databases (As Target) Integration with KTables (As Source)","title":"Roadmap"},{"location":"reference/katalog/","text":"Katalog Katalog is a data catalog that is included in Fybrik for evaluation purposes. It is powered by Kubernetes resources: Asset CRD for managing data assets Secret resources for managing data access credentials Usage An Asset CRD includes a reference to a credentials Secret , connection information, and other metadata such as columns and associated security tags. Apply it like any other Kubernetes resource. Access credenditals are stored in Kubernetes Secret resources. You can use Basic authentication secrets or Opaque secrets with the following keys: Name Type Description Required access_key string Access key also known as AccessKeyId false secret_key string Secret key also known as SecretAccessKey false api_key string API key used in various IAM enabled services false password string Password for basic authentication false username string Username for basic authentication false Manage users Kubernetes RBAC is used for user management: To view Asset resources a Kubernetes user must be granted the katalog-viewer cluster role. To manage Asset resources a Kubernetes user must be granted the katalog-editor cluster role. As always, create a RoleBinding to grant these permissions to assets in a specific namespace and a ClusterRoleBinding to grant these premissions cluster wide.","title":"Katalog"},{"location":"reference/katalog/#katalog","text":"Katalog is a data catalog that is included in Fybrik for evaluation purposes. It is powered by Kubernetes resources: Asset CRD for managing data assets Secret resources for managing data access credentials","title":"Katalog"},{"location":"reference/katalog/#usage","text":"An Asset CRD includes a reference to a credentials Secret , connection information, and other metadata such as columns and associated security tags. Apply it like any other Kubernetes resource. Access credenditals are stored in Kubernetes Secret resources. You can use Basic authentication secrets or Opaque secrets with the following keys: Name Type Description Required access_key string Access key also known as AccessKeyId false secret_key string Secret key also known as SecretAccessKey false api_key string API key used in various IAM enabled services false password string Password for basic authentication false username string Username for basic authentication false","title":"Usage"},{"location":"reference/katalog/#manage-users","text":"Kubernetes RBAC is used for user management: To view Asset resources a Kubernetes user must be granted the katalog-viewer cluster role. To manage Asset resources a Kubernetes user must be granted the katalog-editor cluster role. As always, create a RoleBinding to grant these permissions to assets in a specific namespace and a ClusterRoleBinding to grant these premissions cluster wide.","title":"Manage users"},{"location":"reference/connectors-datacatalog/","text":"Data Catalog Service - Asset Details Documentation for API Endpoints All URIs are relative to https://localhost:8080 Class Method HTTP request Description DefaultApi createAsset POST /createAsset This REST API writes data asset information to the data catalog configured in fybrik DefaultApi deleteAsset DELETE /deleteAsset This REST API deletes data asset DefaultApi getAssetInfo POST /getAssetInfo This REST API gets data asset information from the data catalog configured in fybrik for the data sets indicated in FybrikApplication yaml DefaultApi updateAsset PATCH /updateAsset This REST API updates data asset information in the data catalog configured in fybrik Documentation for Models Connection CreateAssetRequest CreateAssetResponse DeleteAssetRequest DeleteAssetResponse GetAssetRequest GetAssetResponse ResourceColumn ResourceDetails ResourceMetadata UpdateAssetRequest UpdateAssetResponse db2 fybrik-arrow-flight google-sheets https kafka mysql postgres s3 Documentation for Authorization All endpoints do not require authorization.","title":"Data catalog"},{"location":"reference/connectors-datacatalog/#data-catalog-service-asset-details","text":"","title":"Data Catalog Service - Asset Details"},{"location":"reference/connectors-datacatalog/#documentation-for-api-endpoints","text":"All URIs are relative to https://localhost:8080 Class Method HTTP request Description DefaultApi createAsset POST /createAsset This REST API writes data asset information to the data catalog configured in fybrik DefaultApi deleteAsset DELETE /deleteAsset This REST API deletes data asset DefaultApi getAssetInfo POST /getAssetInfo This REST API gets data asset information from the data catalog configured in fybrik for the data sets indicated in FybrikApplication yaml DefaultApi updateAsset PATCH /updateAsset This REST API updates data asset information in the data catalog configured in fybrik","title":"Documentation for API Endpoints"},{"location":"reference/connectors-datacatalog/#documentation-for-models","text":"Connection CreateAssetRequest CreateAssetResponse DeleteAssetRequest DeleteAssetResponse GetAssetRequest GetAssetResponse ResourceColumn ResourceDetails ResourceMetadata UpdateAssetRequest UpdateAssetResponse db2 fybrik-arrow-flight google-sheets https kafka mysql postgres s3","title":"Documentation for Models"},{"location":"reference/connectors-datacatalog/#documentation-for-authorization","text":"All endpoints do not require authorization.","title":"Documentation for Authorization"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/","text":"DefaultApi All URIs are relative to https://localhost:8080 Method HTTP request Description createAsset POST /createAsset This REST API writes data asset information to the data catalog configured in fybrik deleteAsset DELETE /deleteAsset This REST API deletes data asset getAssetInfo POST /getAssetInfo This REST API gets data asset information from the data catalog configured in fybrik for the data sets indicated in FybrikApplication yaml updateAsset PATCH /updateAsset This REST API updates data asset information in the data catalog configured in fybrik createAsset CreateAssetResponse createAsset(X-Request-Datacatalog-Write-CredCreateAssetRequest) This REST API writes data asset information to the data catalog configured in fybrik Parameters Name Type Description Notes X-Request-Datacatalog-Write-Cred String This header carries credential information related to accessing the relevant destination catalog. [default to null] CreateAssetRequest CreateAssetRequest Write Asset Request Return type CreateAssetResponse Authorization No authorization required HTTP request headers Content-Type : application/json Accept : application/json [Back to API-Specification] deleteAsset DeleteAssetResponse deleteAsset(X-Request-Datacatalog-CredDeleteAssetRequest) This REST API deletes data asset Parameters Name Type Description Notes X-Request-Datacatalog-Cred String This header carries credential information related to relevant catalog from which the asset information needs to be retrieved. [default to null] DeleteAssetRequest DeleteAssetRequest Delete Asset Request Return type DeleteAssetResponse Authorization No authorization required HTTP request headers Content-Type : application/json Accept : application/json [Back to API-Specification] getAssetInfo GetAssetResponse getAssetInfo(X-Request-Datacatalog-CredGetAssetRequest) This REST API gets data asset information from the data catalog configured in fybrik for the data sets indicated in FybrikApplication yaml Parameters Name Type Description Notes X-Request-Datacatalog-Cred String This header carries credential information related to relevant catalog from which the asset information needs to be retrieved. [default to null] GetAssetRequest GetAssetRequest Data Catalog Request Object. Return type GetAssetResponse Authorization No authorization required HTTP request headers Content-Type : application/json Accept : application/json [Back to API-Specification] updateAsset UpdateAssetResponse updateAsset(X-Request-Datacatalog-Update-CredUpdateAssetRequest) This REST API updates data asset information in the data catalog configured in fybrik Parameters Name Type Description Notes X-Request-Datacatalog-Update-Cred String This header carries credential information related to accessing the relevant destination catalog. [default to null] UpdateAssetRequest UpdateAssetRequest Update Asset Request Return type UpdateAssetResponse Authorization No authorization required HTTP request headers Content-Type : application/json Accept : application/json [Back to API-Specification]","title":"DefaultApi"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#defaultapi","text":"All URIs are relative to https://localhost:8080 Method HTTP request Description createAsset POST /createAsset This REST API writes data asset information to the data catalog configured in fybrik deleteAsset DELETE /deleteAsset This REST API deletes data asset getAssetInfo POST /getAssetInfo This REST API gets data asset information from the data catalog configured in fybrik for the data sets indicated in FybrikApplication yaml updateAsset PATCH /updateAsset This REST API updates data asset information in the data catalog configured in fybrik","title":"DefaultApi"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#createasset","text":"CreateAssetResponse createAsset(X-Request-Datacatalog-Write-CredCreateAssetRequest) This REST API writes data asset information to the data catalog configured in fybrik","title":"createAsset"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#parameters","text":"Name Type Description Notes X-Request-Datacatalog-Write-Cred String This header carries credential information related to accessing the relevant destination catalog. [default to null] CreateAssetRequest CreateAssetRequest Write Asset Request","title":"Parameters"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#return-type","text":"CreateAssetResponse","title":"Return type"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#authorization","text":"No authorization required","title":"Authorization"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#http-request-headers","text":"Content-Type : application/json Accept : application/json [Back to API-Specification]","title":"HTTP request headers"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#deleteasset","text":"DeleteAssetResponse deleteAsset(X-Request-Datacatalog-CredDeleteAssetRequest) This REST API deletes data asset","title":"deleteAsset"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#parameters_1","text":"Name Type Description Notes X-Request-Datacatalog-Cred String This header carries credential information related to relevant catalog from which the asset information needs to be retrieved. [default to null] DeleteAssetRequest DeleteAssetRequest Delete Asset Request","title":"Parameters"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#return-type_1","text":"DeleteAssetResponse","title":"Return type"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#authorization_1","text":"No authorization required","title":"Authorization"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#http-request-headers_1","text":"Content-Type : application/json Accept : application/json [Back to API-Specification]","title":"HTTP request headers"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#getassetinfo","text":"GetAssetResponse getAssetInfo(X-Request-Datacatalog-CredGetAssetRequest) This REST API gets data asset information from the data catalog configured in fybrik for the data sets indicated in FybrikApplication yaml","title":"getAssetInfo"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#parameters_2","text":"Name Type Description Notes X-Request-Datacatalog-Cred String This header carries credential information related to relevant catalog from which the asset information needs to be retrieved. [default to null] GetAssetRequest GetAssetRequest Data Catalog Request Object.","title":"Parameters"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#return-type_2","text":"GetAssetResponse","title":"Return type"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#authorization_2","text":"No authorization required","title":"Authorization"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#http-request-headers_2","text":"Content-Type : application/json Accept : application/json [Back to API-Specification]","title":"HTTP request headers"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#updateasset","text":"UpdateAssetResponse updateAsset(X-Request-Datacatalog-Update-CredUpdateAssetRequest) This REST API updates data asset information in the data catalog configured in fybrik","title":"updateAsset"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#parameters_3","text":"Name Type Description Notes X-Request-Datacatalog-Update-Cred String This header carries credential information related to accessing the relevant destination catalog. [default to null] UpdateAssetRequest UpdateAssetRequest Update Asset Request","title":"Parameters"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#return-type_3","text":"UpdateAssetResponse","title":"Return type"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#authorization_3","text":"No authorization required","title":"Authorization"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#http-request-headers_3","text":"Content-Type : application/json Accept : application/json [Back to API-Specification]","title":"HTTP request headers"},{"location":"reference/connectors-datacatalog/Models/Connection/","text":"Connection Details of connection types supported for accessing data stores. Not all are necessarily supported by fybrik storage allocation mechanism used to store temporary/persistent datasets. Properties Name Type Description Notes db2 db2 [optional] [default: null] fybrik-arrow-flight fybrik-arrow-flight [optional] [default: null] google-sheets google-sheets [optional] [default: null] https https [optional] [default: null] kafka kafka [optional] [default: null] mysql mysql [optional] [default: null] name String Name of the connection type to the data source [default: null] postgres postgres [optional] [default: null] s3 s3 [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Connection"},{"location":"reference/connectors-datacatalog/Models/Connection/#connection","text":"Details of connection types supported for accessing data stores. Not all are necessarily supported by fybrik storage allocation mechanism used to store temporary/persistent datasets.","title":"Connection"},{"location":"reference/connectors-datacatalog/Models/Connection/#properties","text":"Name Type Description Notes db2 db2 [optional] [default: null] fybrik-arrow-flight fybrik-arrow-flight [optional] [default: null] google-sheets google-sheets [optional] [default: null] https https [optional] [default: null] kafka kafka [optional] [default: null] mysql mysql [optional] [default: null] name String Name of the connection type to the data source [default: null] postgres postgres [optional] [default: null] s3 s3 [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-datacatalog/Models/CreateAssetRequest/","text":"CreateAssetRequest Properties Name Type Description Notes credentials String The vault plugin path where the destination data credentials will be stored as kubernetes secrets [optional] [default: null] destinationAssetID String Asset ID to be used for the created asset [optional] [default: null] destinationCatalogID String The destination catalog id in which the new asset will be created based on the information provided in ResourceMetadata and ResourceDetails field [default: null] details ResourceDetails [default: null] resourceMetadata ResourceMetadata [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"CreateAssetRequest"},{"location":"reference/connectors-datacatalog/Models/CreateAssetRequest/#createassetrequest","text":"","title":"CreateAssetRequest"},{"location":"reference/connectors-datacatalog/Models/CreateAssetRequest/#properties","text":"Name Type Description Notes credentials String The vault plugin path where the destination data credentials will be stored as kubernetes secrets [optional] [default: null] destinationAssetID String Asset ID to be used for the created asset [optional] [default: null] destinationCatalogID String The destination catalog id in which the new asset will be created based on the information provided in ResourceMetadata and ResourceDetails field [default: null] details ResourceDetails [default: null] resourceMetadata ResourceMetadata [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-datacatalog/Models/CreateAssetResponse/","text":"CreateAssetResponse Properties Name Type Description Notes assetID String The ID of the created asset based on the source asset information given in CreateAssetRequest object [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"CreateAssetResponse"},{"location":"reference/connectors-datacatalog/Models/CreateAssetResponse/#createassetresponse","text":"","title":"CreateAssetResponse"},{"location":"reference/connectors-datacatalog/Models/CreateAssetResponse/#properties","text":"Name Type Description Notes assetID String The ID of the created asset based on the source asset information given in CreateAssetRequest object [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-datacatalog/Models/DeleteAssetRequest/","text":"DeleteAssetRequest Properties Name Type Description Notes assetID String Asset ID of the registered asset to be queried in the catalog, or a name of the new asset to be created and registered by Fybrik [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"DeleteAssetRequest"},{"location":"reference/connectors-datacatalog/Models/DeleteAssetRequest/#deleteassetrequest","text":"","title":"DeleteAssetRequest"},{"location":"reference/connectors-datacatalog/Models/DeleteAssetRequest/#properties","text":"Name Type Description Notes assetID String Asset ID of the registered asset to be queried in the catalog, or a name of the new asset to be created and registered by Fybrik [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-datacatalog/Models/DeleteAssetResponse/","text":"DeleteAssetResponse Properties Name Type Description Notes status String The deletion status [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"DeleteAssetResponse"},{"location":"reference/connectors-datacatalog/Models/DeleteAssetResponse/#deleteassetresponse","text":"","title":"DeleteAssetResponse"},{"location":"reference/connectors-datacatalog/Models/DeleteAssetResponse/#properties","text":"Name Type Description Notes status String The deletion status [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-datacatalog/Models/GetAssetRequest/","text":"GetAssetRequest Properties Name Type Description Notes assetID String Asset ID of the registered asset to be queried in the catalog, or a name of the new asset to be created and registered by Fybrik [default: null] operationType String Type of operation requested for the asset [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"GetAssetRequest"},{"location":"reference/connectors-datacatalog/Models/GetAssetRequest/#getassetrequest","text":"","title":"GetAssetRequest"},{"location":"reference/connectors-datacatalog/Models/GetAssetRequest/#properties","text":"Name Type Description Notes assetID String Asset ID of the registered asset to be queried in the catalog, or a name of the new asset to be created and registered by Fybrik [default: null] operationType String Type of operation requested for the asset [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-datacatalog/Models/GetAssetResponse/","text":"GetAssetResponse Properties Name Type Description Notes credentials String Vault plugin path where the data credentials will be stored as kubernetes secrets This value is assumed to be known to the catalog connector. [default: null] details ResourceDetails [default: null] message String Additional message to be reported to the user [optional] [default: null] resourceMetadata ResourceMetadata [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"GetAssetResponse"},{"location":"reference/connectors-datacatalog/Models/GetAssetResponse/#getassetresponse","text":"","title":"GetAssetResponse"},{"location":"reference/connectors-datacatalog/Models/GetAssetResponse/#properties","text":"Name Type Description Notes credentials String Vault plugin path where the data credentials will be stored as kubernetes secrets This value is assumed to be known to the catalog connector. [default: null] details ResourceDetails [default: null] message String Additional message to be reported to the user [optional] [default: null] resourceMetadata ResourceMetadata [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-datacatalog/Models/ResourceColumn/","text":"ResourceColumn ResourceColumn represents a column in a tabular resource Properties Name Type Description Notes name String Name of the column [default: null] tags Map Additional metadata for the asset/field [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"ResourceColumn"},{"location":"reference/connectors-datacatalog/Models/ResourceColumn/#resourcecolumn","text":"ResourceColumn represents a column in a tabular resource","title":"ResourceColumn"},{"location":"reference/connectors-datacatalog/Models/ResourceColumn/#properties","text":"Name Type Description Notes name String Name of the column [default: null] tags Map Additional metadata for the asset/field [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-datacatalog/Models/ResourceDetails/","text":"ResourceDetails ResourceDetails includes asset connection details Properties Name Type Description Notes connection Connection [default: null] dataFormat String Format in which the data is being read/written by the workload [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"ResourceDetails"},{"location":"reference/connectors-datacatalog/Models/ResourceDetails/#resourcedetails","text":"ResourceDetails includes asset connection details","title":"ResourceDetails"},{"location":"reference/connectors-datacatalog/Models/ResourceDetails/#properties","text":"Name Type Description Notes connection Connection [default: null] dataFormat String Format in which the data is being read/written by the workload [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-datacatalog/Models/ResourceMetadata/","text":"ResourceMetadata ResourceMetadata defines model for resource metadata Properties Name Type Description Notes columns List Columns associated with the asset [optional] [default: null] geography String Geography of the resource [optional] [default: null] name String Name of the resource [optional] [default: null] owner String Owner of the resource [optional] [default: null] tags Map Additional metadata for the asset/field [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"ResourceMetadata"},{"location":"reference/connectors-datacatalog/Models/ResourceMetadata/#resourcemetadata","text":"ResourceMetadata defines model for resource metadata","title":"ResourceMetadata"},{"location":"reference/connectors-datacatalog/Models/ResourceMetadata/#properties","text":"Name Type Description Notes columns List Columns associated with the asset [optional] [default: null] geography String Geography of the resource [optional] [default: null] name String Name of the resource [optional] [default: null] owner String Owner of the resource [optional] [default: null] tags Map Additional metadata for the asset/field [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-datacatalog/Models/UpdateAssetRequest/","text":"UpdateAssetRequest Properties Name Type Description Notes assetID String Asset ID of the registered asset to be queried in the catalog, or a name of the new asset to be created and registered by Fybrik [default: null] columns List New columns associated with the asset [optional] [default: null] name String New name of the resource [optional] [default: null] owner String New owner of the resource [optional] [default: null] tags Map Additional metadata for the asset/field [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"UpdateAssetRequest"},{"location":"reference/connectors-datacatalog/Models/UpdateAssetRequest/#updateassetrequest","text":"","title":"UpdateAssetRequest"},{"location":"reference/connectors-datacatalog/Models/UpdateAssetRequest/#properties","text":"Name Type Description Notes assetID String Asset ID of the registered asset to be queried in the catalog, or a name of the new asset to be created and registered by Fybrik [default: null] columns List New columns associated with the asset [optional] [default: null] name String New name of the resource [optional] [default: null] owner String New owner of the resource [optional] [default: null] tags Map Additional metadata for the asset/field [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-datacatalog/Models/UpdateAssetResponse/","text":"UpdateAssetResponse Properties Name Type Description Notes status String The updation status [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"UpdateAssetResponse"},{"location":"reference/connectors-datacatalog/Models/UpdateAssetResponse/#updateassetresponse","text":"","title":"UpdateAssetResponse"},{"location":"reference/connectors-datacatalog/Models/UpdateAssetResponse/#properties","text":"Name Type Description Notes status String The updation status [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-datacatalog/Models/db2/","text":"db2 Connection information for accessing a table in a db2 database Properties Name Type Description Notes database String Database name [default: null] port Integer Database port [default: null] ssl Boolean SSL indicates whether the website is secured by an SSL certificate [optional] [default: false] table String Table name [default: null] url String URL of the database server [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"db2"},{"location":"reference/connectors-datacatalog/Models/db2/#db2","text":"Connection information for accessing a table in a db2 database","title":"db2"},{"location":"reference/connectors-datacatalog/Models/db2/#properties","text":"Name Type Description Notes database String Database name [default: null] port Integer Database port [default: null] ssl Boolean SSL indicates whether the website is secured by an SSL certificate [optional] [default: false] table String Table name [default: null] url String URL of the database server [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-datacatalog/Models/fybrik-arrow-flight/","text":"fybrik-arrow-flight Connection information for accessing data in-memory using API of the Fybrik Arrow Flight server Properties Name Type Description Notes hostname String Server host [default: null] port Integer Server port [default: null] scheme String Scheme (grpc, http, https) [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"fybrik-arrow-flight"},{"location":"reference/connectors-datacatalog/Models/fybrik-arrow-flight/#fybrik-arrow-flight","text":"Connection information for accessing data in-memory using API of the Fybrik Arrow Flight server","title":"fybrik-arrow-flight"},{"location":"reference/connectors-datacatalog/Models/fybrik-arrow-flight/#properties","text":"Name Type Description Notes hostname String Server host [default: null] port Integer Server port [default: null] scheme String Scheme (grpc, http, https) [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-datacatalog/Models/google-sheets/","text":"google-sheets Connection information for accessing data in google-sheets Properties Name Type Description Notes spreadsheet_id String The link to the Google spreadsheet [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"google-sheets"},{"location":"reference/connectors-datacatalog/Models/google-sheets/#google-sheets","text":"Connection information for accessing data in google-sheets","title":"google-sheets"},{"location":"reference/connectors-datacatalog/Models/google-sheets/#properties","text":"Name Type Description Notes spreadsheet_id String The link to the Google spreadsheet [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-datacatalog/Models/https/","text":"https Connection information for accessing data via https Properties Name Type Description Notes url String The URL path to access the file. [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"https"},{"location":"reference/connectors-datacatalog/Models/https/#https","text":"Connection information for accessing data via https","title":"https"},{"location":"reference/connectors-datacatalog/Models/https/#properties","text":"Name Type Description Notes url String The URL path to access the file. [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-datacatalog/Models/kafka/","text":"kafka Connection information for accessing a kafka topic Properties Name Type Description Notes bootstrap_servers String A comma-separated list of host/port pairs to use for establishing the initial connection to the Kafka cluster [default: null] key_deserializer String Deserializer to be used for the keys of the topic [optional] [default: null] sasl_mechanism String SASL Mechanism to be used (e.g. PLAIN or SCRAM-SHA-512) [optional] [default: SCRAM-SHA-512] schema_registry String Host/port to connect the schema registry server [optional] [default: null] security_protocol String Kafka security protocol one of (PLAINTEXT, SASL_PLAINTEXT, SASL_SSL, SSL) [optional] [default: SASL_SSL] ssl_truststore String A truststore or certificate encoded as base64. The format can be JKS or PKCS12. [optional] [default: null] topic_name String Name of the Kafka topic [default: null] value_deserializer String Deserializer to be used for the values of the topic [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"kafka"},{"location":"reference/connectors-datacatalog/Models/kafka/#kafka","text":"Connection information for accessing a kafka topic","title":"kafka"},{"location":"reference/connectors-datacatalog/Models/kafka/#properties","text":"Name Type Description Notes bootstrap_servers String A comma-separated list of host/port pairs to use for establishing the initial connection to the Kafka cluster [default: null] key_deserializer String Deserializer to be used for the keys of the topic [optional] [default: null] sasl_mechanism String SASL Mechanism to be used (e.g. PLAIN or SCRAM-SHA-512) [optional] [default: SCRAM-SHA-512] schema_registry String Host/port to connect the schema registry server [optional] [default: null] security_protocol String Kafka security protocol one of (PLAINTEXT, SASL_PLAINTEXT, SASL_SSL, SSL) [optional] [default: SASL_SSL] ssl_truststore String A truststore or certificate encoded as base64. The format can be JKS or PKCS12. [optional] [default: null] topic_name String Name of the Kafka topic [default: null] value_deserializer String Deserializer to be used for the values of the topic [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-datacatalog/Models/mysql/","text":"mysql Connection information for accessing a table in a mysql database Properties Name Type Description Notes database String Database name [default: null] host String Server host [default: null] port Integer Server port [default: null] ssl Boolean SSL indicates whether to encrypt data using SSL [optional] [default: false] table String Table name [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"mysql"},{"location":"reference/connectors-datacatalog/Models/mysql/#mysql","text":"Connection information for accessing a table in a mysql database","title":"mysql"},{"location":"reference/connectors-datacatalog/Models/mysql/#properties","text":"Name Type Description Notes database String Database name [default: null] host String Server host [default: null] port Integer Server port [default: null] ssl Boolean SSL indicates whether to encrypt data using SSL [optional] [default: false] table String Table name [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-datacatalog/Models/postgres/","text":"postgres Connection information for accessing a table in a postgres database Properties Name Type Description Notes database String Database name [default: null] host String Server host [default: null] port Integer Server port [default: null] ssl Boolean SSL indicates whether to encrypt data using SSL [optional] [default: false] table String Table name [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"postgres"},{"location":"reference/connectors-datacatalog/Models/postgres/#postgres","text":"Connection information for accessing a table in a postgres database","title":"postgres"},{"location":"reference/connectors-datacatalog/Models/postgres/#properties","text":"Name Type Description Notes database String Database name [default: null] host String Server host [default: null] port Integer Server port [default: null] ssl Boolean SSL indicates whether to encrypt data using SSL [optional] [default: false] table String Table name [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-datacatalog/Models/s3/","text":"s3 Connection information for S3 compatible object store Properties Name Type Description Notes bucket String S3 bucket name [default: null] endpoint String S3 endpoint URL [default: null] object_key String File name or a prefix (for a partitioned asset) [default: null] region String S3 region, e.g., us-south [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"s3"},{"location":"reference/connectors-datacatalog/Models/s3/#s3","text":"Connection information for S3 compatible object store","title":"s3"},{"location":"reference/connectors-datacatalog/Models/s3/#properties","text":"Name Type Description Notes bucket String S3 bucket name [default: null] endpoint String S3 endpoint URL [default: null] object_key String File name or a prefix (for a partitioned asset) [default: null] region String S3 region, e.g., us-south [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-policymanager/","text":"Policy Manager Service Documentation for API Endpoints All URIs are relative to https://localhost:8080 Class Method HTTP request Description DefaultApi getPoliciesDecisions POST /getPoliciesDecisions This REST API gets data governance decisions for the data sets indicated in FybrikApplication yaml based on the context indicated Documentation for Models Action DataFlow GetPolicyDecisionsRequest GetPolicyDecisionsResponse RequestAction Resource ResourceColumn ResourceMetadata ResultItem Documentation for Authorization All endpoints do not require authorization.","title":"Policy manager"},{"location":"reference/connectors-policymanager/#policy-manager-service","text":"","title":"Policy Manager Service"},{"location":"reference/connectors-policymanager/#documentation-for-api-endpoints","text":"All URIs are relative to https://localhost:8080 Class Method HTTP request Description DefaultApi getPoliciesDecisions POST /getPoliciesDecisions This REST API gets data governance decisions for the data sets indicated in FybrikApplication yaml based on the context indicated","title":"Documentation for API Endpoints"},{"location":"reference/connectors-policymanager/#documentation-for-models","text":"Action DataFlow GetPolicyDecisionsRequest GetPolicyDecisionsResponse RequestAction Resource ResourceColumn ResourceMetadata ResultItem","title":"Documentation for Models"},{"location":"reference/connectors-policymanager/#documentation-for-authorization","text":"All endpoints do not require authorization.","title":"Documentation for Authorization"},{"location":"reference/connectors-policymanager/Apis/DefaultApi/","text":"DefaultApi All URIs are relative to https://localhost:8080 Method HTTP request Description getPoliciesDecisions POST /getPoliciesDecisions This REST API gets data governance decisions for the data sets indicated in FybrikApplication yaml based on the context indicated getPoliciesDecisions GetPolicyDecisionsResponse getPoliciesDecisions(X-Request-CredGetPolicyDecisionsRequest) This REST API gets data governance decisions for the data sets indicated in FybrikApplication yaml based on the context indicated Parameters Name Type Description Notes X-Request-Cred String [default to null] GetPolicyDecisionsRequest GetPolicyDecisionsRequest Policy Manager Request Object. Return type GetPolicyDecisionsResponse Authorization No authorization required HTTP request headers Content-Type : application/json Accept : application/json [Back to API-Specification]","title":"DefaultApi"},{"location":"reference/connectors-policymanager/Apis/DefaultApi/#defaultapi","text":"All URIs are relative to https://localhost:8080 Method HTTP request Description getPoliciesDecisions POST /getPoliciesDecisions This REST API gets data governance decisions for the data sets indicated in FybrikApplication yaml based on the context indicated","title":"DefaultApi"},{"location":"reference/connectors-policymanager/Apis/DefaultApi/#getpoliciesdecisions","text":"GetPolicyDecisionsResponse getPoliciesDecisions(X-Request-CredGetPolicyDecisionsRequest) This REST API gets data governance decisions for the data sets indicated in FybrikApplication yaml based on the context indicated","title":"getPoliciesDecisions"},{"location":"reference/connectors-policymanager/Apis/DefaultApi/#parameters","text":"Name Type Description Notes X-Request-Cred String [default to null] GetPolicyDecisionsRequest GetPolicyDecisionsRequest Policy Manager Request Object.","title":"Parameters"},{"location":"reference/connectors-policymanager/Apis/DefaultApi/#return-type","text":"GetPolicyDecisionsResponse","title":"Return type"},{"location":"reference/connectors-policymanager/Apis/DefaultApi/#authorization","text":"No authorization required","title":"Authorization"},{"location":"reference/connectors-policymanager/Apis/DefaultApi/#http-request-headers","text":"Content-Type : application/json Accept : application/json [Back to API-Specification]","title":"HTTP request headers"},{"location":"reference/connectors-policymanager/Models/Action/","text":"Action Action to be performed on the data, e.g., masking Properties Name Type Description Notes name String Name of the action to be performed, or Deny if access to the data is forbidden Action names should be defined in additional taxonomy layers [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Action"},{"location":"reference/connectors-policymanager/Models/Action/#action","text":"Action to be performed on the data, e.g., masking","title":"Action"},{"location":"reference/connectors-policymanager/Models/Action/#properties","text":"Name Type Description Notes name String Name of the action to be performed, or Deny if access to the data is forbidden Action names should be defined in additional taxonomy layers [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-policymanager/Models/DataFlow/","text":"DataFlow DataFlow indicates how the data is used by the workload, e.g., it is being read, copied, written or deleted Properties Name Type Description Notes [Back to Model list] [Back to API list] [Back to API-Specification]","title":"DataFlow"},{"location":"reference/connectors-policymanager/Models/DataFlow/#dataflow","text":"DataFlow indicates how the data is used by the workload, e.g., it is being read, copied, written or deleted","title":"DataFlow"},{"location":"reference/connectors-policymanager/Models/DataFlow/#properties","text":"Name Type Description Notes [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-policymanager/Models/GetPolicyDecisionsRequest/","text":"GetPolicyDecisionsRequest Properties Name Type Description Notes action RequestAction [default: null] context Map Context in which a policy is evaluated, e.g., details of the data user such as role and intent [optional] [default: null] resource Resource [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"GetPolicyDecisionsRequest"},{"location":"reference/connectors-policymanager/Models/GetPolicyDecisionsRequest/#getpolicydecisionsrequest","text":"","title":"GetPolicyDecisionsRequest"},{"location":"reference/connectors-policymanager/Models/GetPolicyDecisionsRequest/#properties","text":"Name Type Description Notes action RequestAction [default: null] context Map Context in which a policy is evaluated, e.g., details of the data user such as role and intent [optional] [default: null] resource Resource [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-policymanager/Models/GetPolicyDecisionsResponse/","text":"GetPolicyDecisionsResponse Properties Name Type Description Notes decision_id String [optional] [default: null] message String Additional message to be reported to the user [optional] [default: null] result List Result of policy evaluation [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"GetPolicyDecisionsResponse"},{"location":"reference/connectors-policymanager/Models/GetPolicyDecisionsResponse/#getpolicydecisionsresponse","text":"","title":"GetPolicyDecisionsResponse"},{"location":"reference/connectors-policymanager/Models/GetPolicyDecisionsResponse/#properties","text":"Name Type Description Notes decision_id String [optional] [default: null] message String Additional message to be reported to the user [optional] [default: null] result List Result of policy evaluation [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-policymanager/Models/RequestAction/","text":"RequestAction RequestAction describes the reason for accessing the data, e.g., read/write/delete, where the data is processed or written to Properties Name Type Description Notes actionType DataFlow [default: null] destination String [optional] [default: null] processingLocation String location information [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"RequestAction"},{"location":"reference/connectors-policymanager/Models/RequestAction/#requestaction","text":"RequestAction describes the reason for accessing the data, e.g., read/write/delete, where the data is processed or written to","title":"RequestAction"},{"location":"reference/connectors-policymanager/Models/RequestAction/#properties","text":"Name Type Description Notes actionType DataFlow [default: null] destination String [optional] [default: null] processingLocation String location information [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-policymanager/Models/Resource/","text":"Resource Asset metadata Properties Name Type Description Notes id String Asset ID of the registered asset to be queried in the catalog, or a name of the new asset to be created and registered by Fybrik [default: null] metadata ResourceMetadata [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Resource"},{"location":"reference/connectors-policymanager/Models/Resource/#resource","text":"Asset metadata","title":"Resource"},{"location":"reference/connectors-policymanager/Models/Resource/#properties","text":"Name Type Description Notes id String Asset ID of the registered asset to be queried in the catalog, or a name of the new asset to be created and registered by Fybrik [default: null] metadata ResourceMetadata [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-policymanager/Models/ResourceColumn/","text":"ResourceColumn ResourceColumn represents a column in a tabular resource Properties Name Type Description Notes name String Name of the column [default: null] tags Map Additional metadata for the asset/field [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"ResourceColumn"},{"location":"reference/connectors-policymanager/Models/ResourceColumn/#resourcecolumn","text":"ResourceColumn represents a column in a tabular resource","title":"ResourceColumn"},{"location":"reference/connectors-policymanager/Models/ResourceColumn/#properties","text":"Name Type Description Notes name String Name of the column [default: null] tags Map Additional metadata for the asset/field [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-policymanager/Models/ResourceMetadata/","text":"ResourceMetadata ResourceMetadata defines model for resource metadata Properties Name Type Description Notes columns List Columns associated with the asset [optional] [default: null] geography String Geography of the resource [optional] [default: null] name String Name of the resource [optional] [default: null] owner String Owner of the resource [optional] [default: null] tags Map Additional metadata for the asset/field [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"ResourceMetadata"},{"location":"reference/connectors-policymanager/Models/ResourceMetadata/#resourcemetadata","text":"ResourceMetadata defines model for resource metadata","title":"ResourceMetadata"},{"location":"reference/connectors-policymanager/Models/ResourceMetadata/#properties","text":"Name Type Description Notes columns List Columns associated with the asset [optional] [default: null] geography String Geography of the resource [optional] [default: null] name String Name of the resource [optional] [default: null] owner String Owner of the resource [optional] [default: null] tags Map Additional metadata for the asset/field [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-policymanager/Models/ResultItem/","text":"ResultItem Result of policy evaluation Properties Name Type Description Notes action Action [default: null] policy String The policy on which the decision was based [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"ResultItem"},{"location":"reference/connectors-policymanager/Models/ResultItem/#resultitem","text":"Result of policy evaluation","title":"ResultItem"},{"location":"reference/connectors-policymanager/Models/ResultItem/#properties","text":"Name Type Description Notes action Action [default: null] policy String The policy on which the decision was based [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-storagemanager/","text":"Storage Manager Service Documentation for API Endpoints All URIs are relative to https://localhost:8082 Class Method HTTP request Description DefaultApi allocateStorage POST /allocateStorage This REST API allocates storage based on the storage account selected by Fybrik DefaultApi deleteStorage DELETE /deleteStorage This REST API deletes allocated storage DefaultApi getSupportedStorageTypes POST /getSupportedStorageTypes This REST API returns a list of supported storage types Documentation for Models AllocateStorageRequest AllocateStorageResponse ApplicationDetails ConfigOptions Connection DatasetDetails DeleteStorageRequest GetSupportedStorageTypesResponse Options SecretRef db2 fybrik-arrow-flight google-sheets https kafka mysql postgres s3 Documentation for Authorization All endpoints do not require authorization.","title":"Storage manager API"},{"location":"reference/connectors-storagemanager/#storage-manager-service","text":"","title":"Storage Manager Service"},{"location":"reference/connectors-storagemanager/#documentation-for-api-endpoints","text":"All URIs are relative to https://localhost:8082 Class Method HTTP request Description DefaultApi allocateStorage POST /allocateStorage This REST API allocates storage based on the storage account selected by Fybrik DefaultApi deleteStorage DELETE /deleteStorage This REST API deletes allocated storage DefaultApi getSupportedStorageTypes POST /getSupportedStorageTypes This REST API returns a list of supported storage types","title":"Documentation for API Endpoints"},{"location":"reference/connectors-storagemanager/#documentation-for-models","text":"AllocateStorageRequest AllocateStorageResponse ApplicationDetails ConfigOptions Connection DatasetDetails DeleteStorageRequest GetSupportedStorageTypesResponse Options SecretRef db2 fybrik-arrow-flight google-sheets https kafka mysql postgres s3","title":"Documentation for Models"},{"location":"reference/connectors-storagemanager/#documentation-for-authorization","text":"All endpoints do not require authorization.","title":"Documentation for Authorization"},{"location":"reference/connectors-storagemanager/Apis/DefaultApi/","text":"DefaultApi All URIs are relative to https://localhost:8082 Method HTTP request Description allocateStorage POST /allocateStorage This REST API allocates storage based on the storage account selected by Fybrik deleteStorage DELETE /deleteStorage This REST API deletes allocated storage getSupportedStorageTypes POST /getSupportedStorageTypes This REST API returns a list of supported storage types allocateStorage AllocateStorageResponse allocateStorage(AllocateStorageRequest) This REST API allocates storage based on the storage account selected by Fybrik Parameters Name Type Description Notes AllocateStorageRequest AllocateStorageRequest Allocate Storage Request Return type AllocateStorageResponse Authorization No authorization required HTTP request headers Content-Type : application/json Accept : application/json [Back to API-Specification] deleteStorage deleteStorage(DeleteStorageRequest) This REST API deletes allocated storage Parameters Name Type Description Notes DeleteStorageRequest DeleteStorageRequest Delete Storage Request Return type null (empty response body) Authorization No authorization required HTTP request headers Content-Type : application/json Accept : Not defined [Back to API-Specification] getSupportedStorageTypes GetSupportedStorageTypesResponse getSupportedStorageTypes() This REST API returns a list of supported storage types Parameters This endpoint does not need any parameter. Return type GetSupportedStorageTypesResponse Authorization No authorization required HTTP request headers Content-Type : Not defined Accept : application/json [Back to API-Specification]","title":"DefaultApi"},{"location":"reference/connectors-storagemanager/Apis/DefaultApi/#defaultapi","text":"All URIs are relative to https://localhost:8082 Method HTTP request Description allocateStorage POST /allocateStorage This REST API allocates storage based on the storage account selected by Fybrik deleteStorage DELETE /deleteStorage This REST API deletes allocated storage getSupportedStorageTypes POST /getSupportedStorageTypes This REST API returns a list of supported storage types","title":"DefaultApi"},{"location":"reference/connectors-storagemanager/Apis/DefaultApi/#allocatestorage","text":"AllocateStorageResponse allocateStorage(AllocateStorageRequest) This REST API allocates storage based on the storage account selected by Fybrik","title":"allocateStorage"},{"location":"reference/connectors-storagemanager/Apis/DefaultApi/#parameters","text":"Name Type Description Notes AllocateStorageRequest AllocateStorageRequest Allocate Storage Request","title":"Parameters"},{"location":"reference/connectors-storagemanager/Apis/DefaultApi/#return-type","text":"AllocateStorageResponse","title":"Return type"},{"location":"reference/connectors-storagemanager/Apis/DefaultApi/#authorization","text":"No authorization required","title":"Authorization"},{"location":"reference/connectors-storagemanager/Apis/DefaultApi/#http-request-headers","text":"Content-Type : application/json Accept : application/json [Back to API-Specification]","title":"HTTP request headers"},{"location":"reference/connectors-storagemanager/Apis/DefaultApi/#deletestorage","text":"deleteStorage(DeleteStorageRequest) This REST API deletes allocated storage","title":"deleteStorage"},{"location":"reference/connectors-storagemanager/Apis/DefaultApi/#parameters_1","text":"Name Type Description Notes DeleteStorageRequest DeleteStorageRequest Delete Storage Request","title":"Parameters"},{"location":"reference/connectors-storagemanager/Apis/DefaultApi/#return-type_1","text":"null (empty response body)","title":"Return type"},{"location":"reference/connectors-storagemanager/Apis/DefaultApi/#authorization_1","text":"No authorization required","title":"Authorization"},{"location":"reference/connectors-storagemanager/Apis/DefaultApi/#http-request-headers_1","text":"Content-Type : application/json Accept : Not defined [Back to API-Specification]","title":"HTTP request headers"},{"location":"reference/connectors-storagemanager/Apis/DefaultApi/#getsupportedstoragetypes","text":"GetSupportedStorageTypesResponse getSupportedStorageTypes() This REST API returns a list of supported storage types","title":"getSupportedStorageTypes"},{"location":"reference/connectors-storagemanager/Apis/DefaultApi/#parameters_2","text":"This endpoint does not need any parameter.","title":"Parameters"},{"location":"reference/connectors-storagemanager/Apis/DefaultApi/#return-type_2","text":"GetSupportedStorageTypesResponse","title":"Return type"},{"location":"reference/connectors-storagemanager/Apis/DefaultApi/#authorization_2","text":"No authorization required","title":"Authorization"},{"location":"reference/connectors-storagemanager/Apis/DefaultApi/#http-request-headers_2","text":"Content-Type : Not defined Accept : application/json [Back to API-Specification]","title":"HTTP request headers"},{"location":"reference/connectors-storagemanager/Models/AllocateStorageRequest/","text":"AllocateStorageRequest Properties Name Type Description Notes accountProperties Map Properties of a shared storage account, e.g., endpoint [default: null] accountType String Name of the connection type to the data source [default: null] options Options [default: null] secret SecretRef [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"AllocateStorageRequest"},{"location":"reference/connectors-storagemanager/Models/AllocateStorageRequest/#allocatestoragerequest","text":"","title":"AllocateStorageRequest"},{"location":"reference/connectors-storagemanager/Models/AllocateStorageRequest/#properties","text":"Name Type Description Notes accountProperties Map Properties of a shared storage account, e.g., endpoint [default: null] accountType String Name of the connection type to the data source [default: null] options Options [default: null] secret SecretRef [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-storagemanager/Models/AllocateStorageResponse/","text":"AllocateStorageResponse Properties Name Type Description Notes connection Connection [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"AllocateStorageResponse"},{"location":"reference/connectors-storagemanager/Models/AllocateStorageResponse/#allocatestorageresponse","text":"","title":"AllocateStorageResponse"},{"location":"reference/connectors-storagemanager/Models/AllocateStorageResponse/#properties","text":"Name Type Description Notes connection Connection [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-storagemanager/Models/ApplicationDetails/","text":"ApplicationDetails Details of the owner application Properties Name Type Description Notes name String Application name [default: null] namespace String Application namespace [default: null] uuid String uuid [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"ApplicationDetails"},{"location":"reference/connectors-storagemanager/Models/ApplicationDetails/#applicationdetails","text":"Details of the owner application","title":"ApplicationDetails"},{"location":"reference/connectors-storagemanager/Models/ApplicationDetails/#properties","text":"Name Type Description Notes name String Application name [default: null] namespace String Application namespace [default: null] uuid String uuid [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-storagemanager/Models/ConfigOptions/","text":"ConfigOptions Configuration options TODO: extend IT config policies to return options for storage management Properties Name Type Description Notes deleteEmptyFolder Boolean Delete an empty folder/bucket when the allocated storage is deleted [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"ConfigOptions"},{"location":"reference/connectors-storagemanager/Models/ConfigOptions/#configoptions","text":"Configuration options TODO: extend IT config policies to return options for storage management","title":"ConfigOptions"},{"location":"reference/connectors-storagemanager/Models/ConfigOptions/#properties","text":"Name Type Description Notes deleteEmptyFolder Boolean Delete an empty folder/bucket when the allocated storage is deleted [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-storagemanager/Models/Connection/","text":"Connection Details of connection types supported for accessing data stores. Not all are necessarily supported by fybrik storage allocation mechanism used to store temporary/persistent datasets. Properties Name Type Description Notes db2 db2 [optional] [default: null] fybrik-arrow-flight fybrik-arrow-flight [optional] [default: null] google-sheets google-sheets [optional] [default: null] https https [optional] [default: null] kafka kafka [optional] [default: null] mysql mysql [optional] [default: null] name String Name of the connection type to the data source [default: null] postgres postgres [optional] [default: null] s3 s3 [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Connection"},{"location":"reference/connectors-storagemanager/Models/Connection/#connection","text":"Details of connection types supported for accessing data stores. Not all are necessarily supported by fybrik storage allocation mechanism used to store temporary/persistent datasets.","title":"Connection"},{"location":"reference/connectors-storagemanager/Models/Connection/#properties","text":"Name Type Description Notes db2 db2 [optional] [default: null] fybrik-arrow-flight fybrik-arrow-flight [optional] [default: null] google-sheets google-sheets [optional] [default: null] https https [optional] [default: null] kafka kafka [optional] [default: null] mysql mysql [optional] [default: null] name String Name of the connection type to the data source [default: null] postgres postgres [optional] [default: null] s3 s3 [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-storagemanager/Models/DatasetDetails/","text":"DatasetDetails Details of the new asset The current implementation includes only a name provided in the write flow for a new asset Properties Name Type Description Notes name String [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"DatasetDetails"},{"location":"reference/connectors-storagemanager/Models/DatasetDetails/#datasetdetails","text":"Details of the new asset The current implementation includes only a name provided in the write flow for a new asset","title":"DatasetDetails"},{"location":"reference/connectors-storagemanager/Models/DatasetDetails/#properties","text":"Name Type Description Notes name String [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-storagemanager/Models/DeleteStorageRequest/","text":"DeleteStorageRequest Properties Name Type Description Notes connection Connection [default: null] options Options [default: null] secret SecretRef [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"DeleteStorageRequest"},{"location":"reference/connectors-storagemanager/Models/DeleteStorageRequest/#deletestoragerequest","text":"","title":"DeleteStorageRequest"},{"location":"reference/connectors-storagemanager/Models/DeleteStorageRequest/#properties","text":"Name Type Description Notes connection Connection [default: null] options Options [default: null] secret SecretRef [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-storagemanager/Models/GetSupportedStorageTypesResponse/","text":"GetSupportedStorageTypesResponse Properties Name Type Description Notes connectionTypes List connection types supported by StorageManager for storage allocation/deletion [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"GetSupportedStorageTypesResponse"},{"location":"reference/connectors-storagemanager/Models/GetSupportedStorageTypesResponse/#getsupportedstoragetypesresponse","text":"","title":"GetSupportedStorageTypesResponse"},{"location":"reference/connectors-storagemanager/Models/GetSupportedStorageTypesResponse/#properties","text":"Name Type Description Notes connectionTypes List connection types supported by StorageManager for storage allocation/deletion [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-storagemanager/Models/Options/","text":"Options Additional options provided for storage allocation/deletion Properties Name Type Description Notes appDetails ApplicationDetails [default: null] configurationOpts ConfigOptions [default: null] datasetProperties DatasetDetails [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Options"},{"location":"reference/connectors-storagemanager/Models/Options/#options","text":"Additional options provided for storage allocation/deletion","title":"Options"},{"location":"reference/connectors-storagemanager/Models/Options/#properties","text":"Name Type Description Notes appDetails ApplicationDetails [default: null] configurationOpts ConfigOptions [default: null] datasetProperties DatasetDetails [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-storagemanager/Models/SecretRef/","text":"SecretRef Reference to k8s secret holding credentials for storage access Properties Name Type Description Notes name String Name [default: null] namespace String Namespace [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"SecretRef"},{"location":"reference/connectors-storagemanager/Models/SecretRef/#secretref","text":"Reference to k8s secret holding credentials for storage access","title":"SecretRef"},{"location":"reference/connectors-storagemanager/Models/SecretRef/#properties","text":"Name Type Description Notes name String Name [default: null] namespace String Namespace [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-storagemanager/Models/db2/","text":"db2 Connection information for accessing a table in a db2 database Properties Name Type Description Notes database String Database name [default: null] port Integer Database port [default: null] ssl Boolean SSL indicates whether the website is secured by an SSL certificate [optional] [default: false] table String Table name [default: null] url String URL of the database server [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"db2"},{"location":"reference/connectors-storagemanager/Models/db2/#db2","text":"Connection information for accessing a table in a db2 database","title":"db2"},{"location":"reference/connectors-storagemanager/Models/db2/#properties","text":"Name Type Description Notes database String Database name [default: null] port Integer Database port [default: null] ssl Boolean SSL indicates whether the website is secured by an SSL certificate [optional] [default: false] table String Table name [default: null] url String URL of the database server [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-storagemanager/Models/fybrik-arrow-flight/","text":"fybrik-arrow-flight Connection information for accessing data in-memory using API of the Fybrik Arrow Flight server Properties Name Type Description Notes hostname String Server host [default: null] port Integer Server port [default: null] scheme String Scheme (grpc, http, https) [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"fybrik-arrow-flight"},{"location":"reference/connectors-storagemanager/Models/fybrik-arrow-flight/#fybrik-arrow-flight","text":"Connection information for accessing data in-memory using API of the Fybrik Arrow Flight server","title":"fybrik-arrow-flight"},{"location":"reference/connectors-storagemanager/Models/fybrik-arrow-flight/#properties","text":"Name Type Description Notes hostname String Server host [default: null] port Integer Server port [default: null] scheme String Scheme (grpc, http, https) [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-storagemanager/Models/google-sheets/","text":"google-sheets Connection information for accessing data in google-sheets Properties Name Type Description Notes spreadsheet_id String The link to the Google spreadsheet [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"google-sheets"},{"location":"reference/connectors-storagemanager/Models/google-sheets/#google-sheets","text":"Connection information for accessing data in google-sheets","title":"google-sheets"},{"location":"reference/connectors-storagemanager/Models/google-sheets/#properties","text":"Name Type Description Notes spreadsheet_id String The link to the Google spreadsheet [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-storagemanager/Models/https/","text":"https Connection information for accessing data via https Properties Name Type Description Notes url String The URL path to access the file. [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"https"},{"location":"reference/connectors-storagemanager/Models/https/#https","text":"Connection information for accessing data via https","title":"https"},{"location":"reference/connectors-storagemanager/Models/https/#properties","text":"Name Type Description Notes url String The URL path to access the file. [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-storagemanager/Models/kafka/","text":"kafka Connection information for accessing a kafka topic Properties Name Type Description Notes bootstrap_servers String A comma-separated list of host/port pairs to use for establishing the initial connection to the Kafka cluster [default: null] key_deserializer String Deserializer to be used for the keys of the topic [optional] [default: null] sasl_mechanism String SASL Mechanism to be used (e.g. PLAIN or SCRAM-SHA-512) [optional] [default: SCRAM-SHA-512] schema_registry String Host/port to connect the schema registry server [optional] [default: null] security_protocol String Kafka security protocol one of (PLAINTEXT, SASL_PLAINTEXT, SASL_SSL, SSL) [optional] [default: SASL_SSL] ssl_truststore String A truststore or certificate encoded as base64. The format can be JKS or PKCS12. [optional] [default: null] topic_name String Name of the Kafka topic [default: null] value_deserializer String Deserializer to be used for the values of the topic [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"kafka"},{"location":"reference/connectors-storagemanager/Models/kafka/#kafka","text":"Connection information for accessing a kafka topic","title":"kafka"},{"location":"reference/connectors-storagemanager/Models/kafka/#properties","text":"Name Type Description Notes bootstrap_servers String A comma-separated list of host/port pairs to use for establishing the initial connection to the Kafka cluster [default: null] key_deserializer String Deserializer to be used for the keys of the topic [optional] [default: null] sasl_mechanism String SASL Mechanism to be used (e.g. PLAIN or SCRAM-SHA-512) [optional] [default: SCRAM-SHA-512] schema_registry String Host/port to connect the schema registry server [optional] [default: null] security_protocol String Kafka security protocol one of (PLAINTEXT, SASL_PLAINTEXT, SASL_SSL, SSL) [optional] [default: SASL_SSL] ssl_truststore String A truststore or certificate encoded as base64. The format can be JKS or PKCS12. [optional] [default: null] topic_name String Name of the Kafka topic [default: null] value_deserializer String Deserializer to be used for the values of the topic [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-storagemanager/Models/mysql/","text":"mysql Connection information for accessing a table in a mysql database Properties Name Type Description Notes database String Database name [default: null] host String Server host [default: null] port Integer Server port [default: null] ssl Boolean SSL indicates whether to encrypt data using SSL [optional] [default: false] table String Table name [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"mysql"},{"location":"reference/connectors-storagemanager/Models/mysql/#mysql","text":"Connection information for accessing a table in a mysql database","title":"mysql"},{"location":"reference/connectors-storagemanager/Models/mysql/#properties","text":"Name Type Description Notes database String Database name [default: null] host String Server host [default: null] port Integer Server port [default: null] ssl Boolean SSL indicates whether to encrypt data using SSL [optional] [default: false] table String Table name [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-storagemanager/Models/postgres/","text":"postgres Connection information for accessing a table in a postgres database Properties Name Type Description Notes database String Database name [default: null] host String Server host [default: null] port Integer Server port [default: null] ssl Boolean SSL indicates whether to encrypt data using SSL [optional] [default: false] table String Table name [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"postgres"},{"location":"reference/connectors-storagemanager/Models/postgres/#postgres","text":"Connection information for accessing a table in a postgres database","title":"postgres"},{"location":"reference/connectors-storagemanager/Models/postgres/#properties","text":"Name Type Description Notes database String Database name [default: null] host String Server host [default: null] port Integer Server port [default: null] ssl Boolean SSL indicates whether to encrypt data using SSL [optional] [default: false] table String Table name [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-storagemanager/Models/s3/","text":"s3 Connection information for S3 compatible object store Properties Name Type Description Notes bucket String S3 bucket name [default: null] endpoint String S3 endpoint URL [default: null] object_key String File name or a prefix (for a partitioned asset) [default: null] region String S3 region, e.g., us-south [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"s3"},{"location":"reference/connectors-storagemanager/Models/s3/#s3","text":"Connection information for S3 compatible object store","title":"s3"},{"location":"reference/connectors-storagemanager/Models/s3/#properties","text":"Name Type Description Notes bucket String S3 bucket name [default: null] endpoint String S3 endpoint URL [default: null] object_key String File name or a prefix (for a partitioned asset) [default: null] region String S3 region, e.g., us-south [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"samples/chaining-sample/","text":"FybrikModule chaining sample This sample shows how to implement a use case where, based on the data source and governance policies, the Fybrik manager determines that it must deploy two FybrikModules to allow a workload access to a dataset. One FybrikModule handles reading the data and the second does the data transformation. Data is passed between the FybrikModules without writing to intermediate storage. The data read in this example is the userdata dataset, a Parquet file found in https://github.com/Teradata/kylo/blob/master/samples/sample-data/parquet/userdata2.parquet. Two FybrikModules are available for use by the Fybrik control plane: the arrow-flight-module and the airbyte-module . Only the airbyte-module can give read access to the dataset. However, it does not have any data transformation capabilities. Therefore, to satisfy constraints, the Fybrik manager must deploy both modules: the airbyte module for reading the dataset, and the arrow-flight-module for transforming the dataset based on the governance policies. To recreate this scenario, you will need a copy of the airbyte-module repository. Set the AIRBYTE_MODULE_DIR environment variable to be the path of the airbyte-module directory: cd /tmp git clone https://github.com/fybrik/airbyte-module.git cd airbyte-module export AIRBYTE_MODULE_DIR = ${ PWD } Install the Airbyte module: kubectl apply -f https://github.com/fybrik/airbyte-module/releases/latest/download/module.yaml -n fybrik-system Install the arrow-flight module for transformations: kubectl apply -f https://github.com/fybrik/arrow-flight-module/releases/latest/download/module.yaml -n fybrik-system Next, register the data asset itself in the data catalog. The way to do it depends on the data catalog with which you are working: With OpenMetadata With Katalog We use port-forwarding to send asset creation requests to the OpenMetadata connector. kubectl port-forward svc/openmetadata-connector -n fybrik-system 8081 :8080 & cat << EOF | curl -X POST localhost:8081/createAsset -d @- { \"destinationCatalogID\": \"openmetadata\", \"destinationAssetID\": \"userdata\", \"details\": { \"dataFormat\": \"parquet\", \"connection\": { \"name\": \"https\", \"https\": { \"url\": \"https://github.com/Teradata/kylo/raw/master/samples/sample-data/parquet/userdata2.parquet\" } } }, \"resourceMetadata\": { \"name\": \"test data\", \"geography\": \"theshire \", \"tags\": { \"Purpose.finance\": \"true\" }, \"columns\": [ { \"name\": \"first_name\", \"tags\": { \"PII.Sensitive\": \"true\" } }, { \"name\": \"last_name\", \"tags\": { \"PII.Sensitive\": \"true\" } }, { \"name\": \"birthdate\", \"tags\": { \"PII.Sensitive\": \"true\" } } ] } } EOF The response from the OpenMetadata connector should look like this: { \"assetID\" : \"openmetadata-https.default.openmetadata.userdata\" } The asset is now registered in the catalog. Store the asset ID in a CATALOGED_ASSET variable: CATALOGED_ASSET = \"openmetadata-https.default.openmetadata.userdata\" kubectl apply -f $AIRBYTE_MODULE_DIR /fybrik/read-flow/asset.yaml The asset is now registered in the catalog. Store the asset ID in a CATALOGED_ASSET variable: CATALOGED_ASSET = fybrik-notebook-sample/userdata Before creating a policy for accessing the asset, make sure that you clean up previously existing access policies: NS = \"fybrik-system\" ; kubectl -n $NS get configmap | awk '/sample/{print $1}' | xargs kubectl delete -n $NS configmap Create the policy to access the asset (we use a policy that requires redactions of PII.Sensitive columns): kubectl -n fybrik-system create configmap sample-policy --from-file = $AIRBYTE_MODULE_DIR /fybrik/sample-policy-restrictive.rego kubectl -n fybrik-system label configmap sample-policy openpolicyagent.org/policy = rego while [[ $( kubectl get cm sample-policy -n fybrik-system -o 'jsonpath={.metadata.annotations.openpolicyagent\\.org/policy-status}' ) ! = '{\"status\":\"ok\"}' ]] ; do echo \"waiting for policy to be applied\" && sleep 5 ; done Create a FybrikApplication resource to register the workload to the control plane of Fybrik. The value you place in the dataSetID field is your asset ID, as explained above. cat <<EOF | kubectl apply -f - apiVersion: app.fybrik.io/v1beta1 kind: FybrikApplication metadata: name: my-app labels: app: my-app spec: selector: workloadSelector: matchLabels: app: my-app appInfo: intent: Fraud Detection data: - dataSetID: ${CATALOGED_ASSET} requirements: interface: protocol: fybrik-arrow-flight EOF After the FybrikApplication is applied, the Fybrik control plane attempts to create the data path for the application. Fybrik realizes that the Airbyte module can give the application access to the userdata dataset, and that the arrow-flight module could provide the redaction transformation. Fybrik deploys both modules in the fybrik-blueprints namespace. To verify that the Airbyte module and the arrow-flight module were indeed deployed, run: kubectl get pods -n fybrik-blueprints NOTE: If you are using OpenShift cluster you will see that the deployment fails because OpenShift doesn't allow privileged: true value in securityContext field by default. Thus, you should add the service account of the module's deployment to the privileged SCC using the following command: oc adm policy add-scc-to-user privileged system:serviceaccount:fybrik-blueprints:<SERVICE_ACCOUNT_NAME> Then, the deployment will restart the failed pods and the pods in fybrik-blueprints namespace should start successfully. You should see pods with names similar to: NAME READY STATUS RESTARTS AGE my-app60cf6ba8-a767-4313-aa39-b78495b625c7-airb-95c0f-airbz4rh8 2 /2 Running 0 88s my-app60cf6ba8-a767-4313-aa39-b78495b625c7-arro-6f3f8-arro82dcb 1 /1 Running 0 87s Wait for the FybrikModule pods to be ready by running: kubectl wait pod --all --for = condition = ready -n fybrik-blueprints --timeout 10m Run the following commands to set the CATALOGED_ASSET_MODIFIED and the ENDPOINT_HOSTNAME environment variables: CATALOGED_ASSET_MODIFIED = $( echo $CATALOGED_ASSET | sed 's/\\./\\\\\\./g' ) export ENDPOINT_HOSTNAME = $( kubectl get fybrikapplication my-app -n fybrik-notebook-sample -o \"jsonpath={.status.assetStates. ${ CATALOGED_ASSET_MODIFIED } .endpoint.fybrik-arrow-flight.hostname}\" ) To verify that the Airbyte module gives access to the userdata dataset, run: cd $AIRBYTE_MODULE_DIR /helm/client ./deploy_airbyte_module_client_pod.sh kubectl exec -it my-shell -n default -- python3 /root/client.py --host ${ ENDPOINT_HOSTNAME } --port 80 --asset ${ CATALOGED_ASSET } You should see the following output: registration_dttm id first_name last_name email ... country birthdate salary title comments 0 2016 -02-03T13:36:39 1 .0 XXXXX XXXXX XXXXX ... Indonesia XXXXX 140249 .37 Senior Financial Analyst 1 2016 -02-03T00:22:28 2 .0 XXXXX XXXXX XXXXX ... China XXXXX NaN 2 2016 -02-03T18:29:04 3 .0 XXXXX XXXXX XXXXX ... France XXXXX 236219 .26 Teacher 3 2016 -02-03T13:42:19 4 .0 XXXXX XXXXX XXXXX ... Russia XXXXX NaN Nuclear Power Engineer 4 2016 -02-03T00:15:29 5 .0 XXXXX XXXXX XXXXX ... France XXXXX 50210 .02 Senior Editor .. ... ... ... ... ... ... ... ... ... ... ... 995 2016 -02-03T13:36:49 996 .0 XXXXX XXXXX XXXXX ... China XXXXX 185421 .82 \" 996 2016-02-03T04:39:01 997.0 XXXXX XXXXX XXXXX ... Malaysia XXXXX 279671.68 997 2016-02-03T00:33:54 998.0 XXXXX XXXXX XXXXX ... Poland XXXXX 112275.78 998 2016-02-03T00:15:08 999.0 XXXXX XXXXX XXXXX ... Kazakhstan XXXXX 53564.76 Speech Pathologist 999 2016-02-03T00:53:53 1000.0 XXXXX XXXXX XXXXX ... Nigeria XXXXX 239858.70 [1000 rows x 13 columns] Alternatively, one can access the userdata dataset from a Jupyter notebook, as described in the notebook sample . To determine the virtual endpoint from which to access the data set, run: CATALOGED_ASSET_MODIFIED = $( echo $CATALOGED_ASSET | sed 's/\\./\\\\\\./g' ) ENDPOINT_SCHEME = $( kubectl get fybrikapplication my-app -o jsonpath ={ .status.assetStates. ${ CATALOGED_ASSET_MODIFIED } .endpoint.fybrik-arrow-flight.scheme } ) ENDPOINT_HOSTNAME = $( kubectl get fybrikapplication my-app -o jsonpath ={ .status.assetStates. ${ CATALOGED_ASSET_MODIFIED } .endpoint.fybrik-arrow-flight.hostname } ) ENDPOINT_PORT = $( kubectl get fybrikapplication my-app -o jsonpath ={ .status.assetStates. ${ CATALOGED_ASSET_MODIFIED } .endpoint.fybrik-arrow-flight.port } ) printf \" ${ ENDPOINT_SCHEME } :// ${ ENDPOINT_HOSTNAME } : ${ ENDPOINT_PORT } \" Note that the virtual endpoint determined from the FybrikApplication status points to the arrow-flight transform module, although this is transparent to the user. Insert a new notebook cell to install pandas and pyarrow packages: %pip install pandas pyarrow Finally, given the endpoint value determined above, insert the following to a new notebook cell: import json import pyarrow.flight as fl # Create a Flight client client = fl.connect ( '<ENDPOINT>' ) # Prepare the request request = { \"asset\" : \"openmetadata-https.default.openmetadata.userdata\" , } # Send request and fetch result as a pandas DataFrame info = client.get_flight_info ( fl.FlightDescriptor.for_command ( json.dumps ( request ))) reader: fl.FlightStreamReader = client.do_get ( info.endpoints [ 0 ] .ticket ) print ( reader.read_pandas ())","title":"Chaining sample"},{"location":"samples/chaining-sample/#fybrikmodule-chaining-sample","text":"This sample shows how to implement a use case where, based on the data source and governance policies, the Fybrik manager determines that it must deploy two FybrikModules to allow a workload access to a dataset. One FybrikModule handles reading the data and the second does the data transformation. Data is passed between the FybrikModules without writing to intermediate storage. The data read in this example is the userdata dataset, a Parquet file found in https://github.com/Teradata/kylo/blob/master/samples/sample-data/parquet/userdata2.parquet. Two FybrikModules are available for use by the Fybrik control plane: the arrow-flight-module and the airbyte-module . Only the airbyte-module can give read access to the dataset. However, it does not have any data transformation capabilities. Therefore, to satisfy constraints, the Fybrik manager must deploy both modules: the airbyte module for reading the dataset, and the arrow-flight-module for transforming the dataset based on the governance policies. To recreate this scenario, you will need a copy of the airbyte-module repository. Set the AIRBYTE_MODULE_DIR environment variable to be the path of the airbyte-module directory: cd /tmp git clone https://github.com/fybrik/airbyte-module.git cd airbyte-module export AIRBYTE_MODULE_DIR = ${ PWD } Install the Airbyte module: kubectl apply -f https://github.com/fybrik/airbyte-module/releases/latest/download/module.yaml -n fybrik-system Install the arrow-flight module for transformations: kubectl apply -f https://github.com/fybrik/arrow-flight-module/releases/latest/download/module.yaml -n fybrik-system Next, register the data asset itself in the data catalog. The way to do it depends on the data catalog with which you are working: With OpenMetadata With Katalog We use port-forwarding to send asset creation requests to the OpenMetadata connector. kubectl port-forward svc/openmetadata-connector -n fybrik-system 8081 :8080 & cat << EOF | curl -X POST localhost:8081/createAsset -d @- { \"destinationCatalogID\": \"openmetadata\", \"destinationAssetID\": \"userdata\", \"details\": { \"dataFormat\": \"parquet\", \"connection\": { \"name\": \"https\", \"https\": { \"url\": \"https://github.com/Teradata/kylo/raw/master/samples/sample-data/parquet/userdata2.parquet\" } } }, \"resourceMetadata\": { \"name\": \"test data\", \"geography\": \"theshire \", \"tags\": { \"Purpose.finance\": \"true\" }, \"columns\": [ { \"name\": \"first_name\", \"tags\": { \"PII.Sensitive\": \"true\" } }, { \"name\": \"last_name\", \"tags\": { \"PII.Sensitive\": \"true\" } }, { \"name\": \"birthdate\", \"tags\": { \"PII.Sensitive\": \"true\" } } ] } } EOF The response from the OpenMetadata connector should look like this: { \"assetID\" : \"openmetadata-https.default.openmetadata.userdata\" } The asset is now registered in the catalog. Store the asset ID in a CATALOGED_ASSET variable: CATALOGED_ASSET = \"openmetadata-https.default.openmetadata.userdata\" kubectl apply -f $AIRBYTE_MODULE_DIR /fybrik/read-flow/asset.yaml The asset is now registered in the catalog. Store the asset ID in a CATALOGED_ASSET variable: CATALOGED_ASSET = fybrik-notebook-sample/userdata Before creating a policy for accessing the asset, make sure that you clean up previously existing access policies: NS = \"fybrik-system\" ; kubectl -n $NS get configmap | awk '/sample/{print $1}' | xargs kubectl delete -n $NS configmap Create the policy to access the asset (we use a policy that requires redactions of PII.Sensitive columns): kubectl -n fybrik-system create configmap sample-policy --from-file = $AIRBYTE_MODULE_DIR /fybrik/sample-policy-restrictive.rego kubectl -n fybrik-system label configmap sample-policy openpolicyagent.org/policy = rego while [[ $( kubectl get cm sample-policy -n fybrik-system -o 'jsonpath={.metadata.annotations.openpolicyagent\\.org/policy-status}' ) ! = '{\"status\":\"ok\"}' ]] ; do echo \"waiting for policy to be applied\" && sleep 5 ; done Create a FybrikApplication resource to register the workload to the control plane of Fybrik. The value you place in the dataSetID field is your asset ID, as explained above. cat <<EOF | kubectl apply -f - apiVersion: app.fybrik.io/v1beta1 kind: FybrikApplication metadata: name: my-app labels: app: my-app spec: selector: workloadSelector: matchLabels: app: my-app appInfo: intent: Fraud Detection data: - dataSetID: ${CATALOGED_ASSET} requirements: interface: protocol: fybrik-arrow-flight EOF After the FybrikApplication is applied, the Fybrik control plane attempts to create the data path for the application. Fybrik realizes that the Airbyte module can give the application access to the userdata dataset, and that the arrow-flight module could provide the redaction transformation. Fybrik deploys both modules in the fybrik-blueprints namespace. To verify that the Airbyte module and the arrow-flight module were indeed deployed, run: kubectl get pods -n fybrik-blueprints NOTE: If you are using OpenShift cluster you will see that the deployment fails because OpenShift doesn't allow privileged: true value in securityContext field by default. Thus, you should add the service account of the module's deployment to the privileged SCC using the following command: oc adm policy add-scc-to-user privileged system:serviceaccount:fybrik-blueprints:<SERVICE_ACCOUNT_NAME> Then, the deployment will restart the failed pods and the pods in fybrik-blueprints namespace should start successfully. You should see pods with names similar to: NAME READY STATUS RESTARTS AGE my-app60cf6ba8-a767-4313-aa39-b78495b625c7-airb-95c0f-airbz4rh8 2 /2 Running 0 88s my-app60cf6ba8-a767-4313-aa39-b78495b625c7-arro-6f3f8-arro82dcb 1 /1 Running 0 87s Wait for the FybrikModule pods to be ready by running: kubectl wait pod --all --for = condition = ready -n fybrik-blueprints --timeout 10m Run the following commands to set the CATALOGED_ASSET_MODIFIED and the ENDPOINT_HOSTNAME environment variables: CATALOGED_ASSET_MODIFIED = $( echo $CATALOGED_ASSET | sed 's/\\./\\\\\\./g' ) export ENDPOINT_HOSTNAME = $( kubectl get fybrikapplication my-app -n fybrik-notebook-sample -o \"jsonpath={.status.assetStates. ${ CATALOGED_ASSET_MODIFIED } .endpoint.fybrik-arrow-flight.hostname}\" ) To verify that the Airbyte module gives access to the userdata dataset, run: cd $AIRBYTE_MODULE_DIR /helm/client ./deploy_airbyte_module_client_pod.sh kubectl exec -it my-shell -n default -- python3 /root/client.py --host ${ ENDPOINT_HOSTNAME } --port 80 --asset ${ CATALOGED_ASSET } You should see the following output: registration_dttm id first_name last_name email ... country birthdate salary title comments 0 2016 -02-03T13:36:39 1 .0 XXXXX XXXXX XXXXX ... Indonesia XXXXX 140249 .37 Senior Financial Analyst 1 2016 -02-03T00:22:28 2 .0 XXXXX XXXXX XXXXX ... China XXXXX NaN 2 2016 -02-03T18:29:04 3 .0 XXXXX XXXXX XXXXX ... France XXXXX 236219 .26 Teacher 3 2016 -02-03T13:42:19 4 .0 XXXXX XXXXX XXXXX ... Russia XXXXX NaN Nuclear Power Engineer 4 2016 -02-03T00:15:29 5 .0 XXXXX XXXXX XXXXX ... France XXXXX 50210 .02 Senior Editor .. ... ... ... ... ... ... ... ... ... ... ... 995 2016 -02-03T13:36:49 996 .0 XXXXX XXXXX XXXXX ... China XXXXX 185421 .82 \" 996 2016-02-03T04:39:01 997.0 XXXXX XXXXX XXXXX ... Malaysia XXXXX 279671.68 997 2016-02-03T00:33:54 998.0 XXXXX XXXXX XXXXX ... Poland XXXXX 112275.78 998 2016-02-03T00:15:08 999.0 XXXXX XXXXX XXXXX ... Kazakhstan XXXXX 53564.76 Speech Pathologist 999 2016-02-03T00:53:53 1000.0 XXXXX XXXXX XXXXX ... Nigeria XXXXX 239858.70 [1000 rows x 13 columns] Alternatively, one can access the userdata dataset from a Jupyter notebook, as described in the notebook sample . To determine the virtual endpoint from which to access the data set, run: CATALOGED_ASSET_MODIFIED = $( echo $CATALOGED_ASSET | sed 's/\\./\\\\\\./g' ) ENDPOINT_SCHEME = $( kubectl get fybrikapplication my-app -o jsonpath ={ .status.assetStates. ${ CATALOGED_ASSET_MODIFIED } .endpoint.fybrik-arrow-flight.scheme } ) ENDPOINT_HOSTNAME = $( kubectl get fybrikapplication my-app -o jsonpath ={ .status.assetStates. ${ CATALOGED_ASSET_MODIFIED } .endpoint.fybrik-arrow-flight.hostname } ) ENDPOINT_PORT = $( kubectl get fybrikapplication my-app -o jsonpath ={ .status.assetStates. ${ CATALOGED_ASSET_MODIFIED } .endpoint.fybrik-arrow-flight.port } ) printf \" ${ ENDPOINT_SCHEME } :// ${ ENDPOINT_HOSTNAME } : ${ ENDPOINT_PORT } \" Note that the virtual endpoint determined from the FybrikApplication status points to the arrow-flight transform module, although this is transparent to the user. Insert a new notebook cell to install pandas and pyarrow packages: %pip install pandas pyarrow Finally, given the endpoint value determined above, insert the following to a new notebook cell: import json import pyarrow.flight as fl # Create a Flight client client = fl.connect ( '<ENDPOINT>' ) # Prepare the request request = { \"asset\" : \"openmetadata-https.default.openmetadata.userdata\" , } # Send request and fetch result as a pandas DataFrame info = client.get_flight_info ( fl.FlightDescriptor.for_command ( json.dumps ( request ))) reader: fl.FlightStreamReader = client.do_get ( info.endpoints [ 0 ] .ticket ) print ( reader.read_pandas ())","title":"FybrikModule chaining sample"},{"location":"samples/cleanup/","text":"Cleanup Fybrik cleanup When you're finished experimenting with a sample, you may clean up as follows: Stop kubectl port-forward processes (e.g., using pkill kubectl ) Delete the namespace created for this sample: kubectl delete namespace fybrik-notebook-sample Delete the policy created in the fybrik-system namespace: NS = \"fybrik-system\" ; kubectl -n $NS get configmap | awk '/sample/{print $1}' | xargs kubectl delete -n $NS configmap Cleaning up the OpenMetadata data catalog Assuming you installed OpenMetadata in the open-metadata namespace, run the following commands: Kubernetes IBM OpenShift helm delete openmetadata openmetadata-dependencies -n open-metadata kubectl delete ns open-metadata kubectl delete pv dag logs helm delete openmetadata openmetadata-dependencies -n open-metadata kubectl delete ns open-metadata If OpenMetadata was installed in a different namespace, replace open-metadata with the name of your chosen namespace.","title":"Cleanup"},{"location":"samples/cleanup/#cleanup","text":"","title":"Cleanup"},{"location":"samples/cleanup/#fybrik-cleanup","text":"When you're finished experimenting with a sample, you may clean up as follows: Stop kubectl port-forward processes (e.g., using pkill kubectl ) Delete the namespace created for this sample: kubectl delete namespace fybrik-notebook-sample Delete the policy created in the fybrik-system namespace: NS = \"fybrik-system\" ; kubectl -n $NS get configmap | awk '/sample/{print $1}' | xargs kubectl delete -n $NS configmap","title":"Fybrik cleanup"},{"location":"samples/cleanup/#cleaning-up-the-openmetadata-data-catalog","text":"Assuming you installed OpenMetadata in the open-metadata namespace, run the following commands: Kubernetes IBM OpenShift helm delete openmetadata openmetadata-dependencies -n open-metadata kubectl delete ns open-metadata kubectl delete pv dag logs helm delete openmetadata openmetadata-dependencies -n open-metadata kubectl delete ns open-metadata If OpenMetadata was installed in a different namespace, replace open-metadata with the name of your chosen namespace.","title":"Cleaning up the OpenMetadata data catalog"},{"location":"samples/dashboard-sample/","text":"Dashboard sample This sample shows how Fybrik enables a dashboard application to display data from a backend REST server and displaying only compliant information with the person operating the dashboard. In the sample we show how a Fybrik module that support REST protocol can control what information is displayed in the dashboard through policies. Specifically in this sample we demonstrate: - Just in time policy decisions - A Fybrik module that supports REST API Note: At this time some features in this demo use mocks as some underlying support in Fybrik is still pending. About the dashboard application The dashboard application here is based on the smart manufacturing use-case of the European Horizon 2020 research project Fogprotect . Details for the use case are available here . The dashboard included in this sample is a mock dashboard of a remote manufacturing factory which can be controlled/supervised by expert operators from remote, without the need for the expert operator to be physically present in the factory. The scenario here is simplified for the purpose of this sample and demonstrates the following scenario: 1. A manufacturing robot that operates the manufacturing facility which can be activated or stopped remotely. 2. A safety dashboard observing the manufacturing floor which is composed of two areas. A production and a non-production area, that are observed by a video camera with image processing which monitors and counts the number of employees with and without helmets in each area. This is called the safety data of the factory. We demonstrate Fybrik abilities to control what data is seen by each role. In this scenario we show the following roles: 1. A Foreman who is allowed to access all of the assets. Furthermore, only the Foreman is allowed to control the robot in the manufacturing area. 2. A Worker who is not allowed to control the robot, and doesn't have privileges to see the number of employees wearing/not wearing helmets in each of the available areas. However, a Worker can see the total number of employees in each area. 3. HR personnel who are also not allowed to control the robot, but have access to view the number of employees wearing/not wearing helmets in each area. Dashboard sample architecture The project contains 3 main components: - A backend data service, which provides the mock data for the dashboard. - A fybrik module , responsible for intercepting HTTP requests sent from a user trying to read or write data. - A dashboard application, which performs HTTP requests and displays the responses for the user. For a more detailed description of the implementation visit fogProtect-dashboard-sample . Before you begin Install Fybrik using the Quick Start guide. A web browser. Create a namespace for the sample Create a new Kubernetes namespace and set it as the active namespace: kubectl create namespace fogprotect kubectl config set-context --current --namespace = fogprotect This enables easy cleanup once you're done experimenting with the sample. Backend data server The backend data server is responsible for reading or writing data to a database or any other resource. In this example the backend data server is a simple server that returns the mock data for the dashboard. Run the backend data server: helm chart pull ghcr.io/fybrik/backend-server-chart:v0.0.1 helm chart export --destination = ./tmp ghcr.io/fybrik/backend-server-chart:v0.0.1 helm install rel1-backend-server ./tmp/backend_server Register the assets In this example we use 3 of the endpoints that the backend data server exposes. For each endpoint, we define an asset describing the data that will be returned as response from the backend data server. The description is used later to apply policies on the data. Register the assets: kubectl apply -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/assets/asset_get_safety_data.yaml kubectl apply -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/assets/asset_start_robot.yaml kubectl apply -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/assets/asset_stop_robot.yaml The identifier of the assets is <namespace>/<name> , and it should be used in the FybrikApplication that we will describe later on. For example the identifier of the asset defined in the file asset_start_robot.yaml is fogprotect/api.control.start-robot . Create JWT authentication key As the HTTP requests should contain the role of the user in the header, the dashboard application uses JWT to pass the JWT of the relevant role in the header. The JWT is authenticated using a secret key that we store as a secret in the cluster, in both fogprotect and fybrik-blueprints namespaces. Create the JWT secret: kubectl apply -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/secrets/jwt_key_secret.yaml kubectl apply -n fybrik-blueprints -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/secrets/jwt_key_secret.yaml Fybrik manager RBAC In order for the fybrik manager to be able to access the assets and to deploy the fybrik module that we created, give the manager the relevant RBAC authorization : kubectl apply -n fybrik-system -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/fybrik-system-manager-rbac.yaml Deploy the module In our case, the module as we described earlier is responsible for intercepting the HTTP requests received from the user. Once a request is received a decision must be made regarding the request, it should be either allowed, columns redacted or blocked depending on the role of the user. The decision is made using OpenPolicyAgent , and applying the policy described in About the dashboard application and specified here . Deploy the fybrik module and application: kubectl apply -n fybrik-system -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/rest-read-module.yaml kubectl apply -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/rest-read-application.yaml kubectl wait --for = condition = ready --all pod --timeout = 120s Deploy the dashboard We now deploy the dashboard that will display a table that contains the safety data of the factory, along with two buttons to start and stop the manufacturing robot. One can change the role of the user using a pull down menu. First create a port-forwarding to the service that will be intercepting the HTTP requests: kubectl -n fybrik-blueprints port-forward svc/rest-read 5559 :5559 & Afterwards, deploy the dashboard application: helm chart pull ghcr.io/fybrik/factory-gui-chart:v0.0.1 helm chart export --destination = ./tmp ghcr.io/fybrik/factory-gui-chart:v0.0.1 helm install rel1-factory-gui ./tmp/factory_gui kubectl wait --for = condition = ready --all pod --timeout = 120s Lastly, create a port-forwarding to the dashboard service in order to be able to open the dashboard in your browser: kubectl port-forward svc/factory-gui 3001 :3000 & Open your browser and go to http://127.0.0.1:3001. Cleanup Stop the port-forwarding: pgrep -f \"kubectl port-forward svc/factory-gui 3001:3000\" | xargs kill pgrep -f \"kubectl -n fybrik-blueprints port-forward svc/rest-read 5559:5559\" | xargs kill Remove the tmp directory that was created temporarily: rm -r tmp Delete the fybrik application and module: kubectl delete fybrikapplication rest-read kubectl -n fybrik-system delete fybrikmodule rest-read-module Delete the fogprotect namespace: kubectl delete namespace fogprotect Delete the RBAC authorization of the manager: kubectl -n fybrik-system delete -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/fybrik-system-manager-rbac.yaml Delete the JWT secret: kubectl delete -n fybrik-blueprints -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/secrets/jwt_key_secret.yaml","title":"Dashboard sample"},{"location":"samples/dashboard-sample/#dashboard-sample","text":"This sample shows how Fybrik enables a dashboard application to display data from a backend REST server and displaying only compliant information with the person operating the dashboard. In the sample we show how a Fybrik module that support REST protocol can control what information is displayed in the dashboard through policies. Specifically in this sample we demonstrate: - Just in time policy decisions - A Fybrik module that supports REST API Note: At this time some features in this demo use mocks as some underlying support in Fybrik is still pending.","title":"Dashboard sample"},{"location":"samples/dashboard-sample/#about-the-dashboard-application","text":"The dashboard application here is based on the smart manufacturing use-case of the European Horizon 2020 research project Fogprotect . Details for the use case are available here . The dashboard included in this sample is a mock dashboard of a remote manufacturing factory which can be controlled/supervised by expert operators from remote, without the need for the expert operator to be physically present in the factory. The scenario here is simplified for the purpose of this sample and demonstrates the following scenario: 1. A manufacturing robot that operates the manufacturing facility which can be activated or stopped remotely. 2. A safety dashboard observing the manufacturing floor which is composed of two areas. A production and a non-production area, that are observed by a video camera with image processing which monitors and counts the number of employees with and without helmets in each area. This is called the safety data of the factory. We demonstrate Fybrik abilities to control what data is seen by each role. In this scenario we show the following roles: 1. A Foreman who is allowed to access all of the assets. Furthermore, only the Foreman is allowed to control the robot in the manufacturing area. 2. A Worker who is not allowed to control the robot, and doesn't have privileges to see the number of employees wearing/not wearing helmets in each of the available areas. However, a Worker can see the total number of employees in each area. 3. HR personnel who are also not allowed to control the robot, but have access to view the number of employees wearing/not wearing helmets in each area.","title":"About the dashboard application"},{"location":"samples/dashboard-sample/#dashboard-sample-architecture","text":"The project contains 3 main components: - A backend data service, which provides the mock data for the dashboard. - A fybrik module , responsible for intercepting HTTP requests sent from a user trying to read or write data. - A dashboard application, which performs HTTP requests and displays the responses for the user. For a more detailed description of the implementation visit fogProtect-dashboard-sample .","title":"Dashboard sample architecture"},{"location":"samples/dashboard-sample/#before-you-begin","text":"Install Fybrik using the Quick Start guide. A web browser.","title":"Before you begin"},{"location":"samples/dashboard-sample/#create-a-namespace-for-the-sample","text":"Create a new Kubernetes namespace and set it as the active namespace: kubectl create namespace fogprotect kubectl config set-context --current --namespace = fogprotect This enables easy cleanup once you're done experimenting with the sample.","title":"Create a namespace for the sample"},{"location":"samples/dashboard-sample/#backend-data-server","text":"The backend data server is responsible for reading or writing data to a database or any other resource. In this example the backend data server is a simple server that returns the mock data for the dashboard. Run the backend data server: helm chart pull ghcr.io/fybrik/backend-server-chart:v0.0.1 helm chart export --destination = ./tmp ghcr.io/fybrik/backend-server-chart:v0.0.1 helm install rel1-backend-server ./tmp/backend_server","title":"Backend data server"},{"location":"samples/dashboard-sample/#register-the-assets","text":"In this example we use 3 of the endpoints that the backend data server exposes. For each endpoint, we define an asset describing the data that will be returned as response from the backend data server. The description is used later to apply policies on the data. Register the assets: kubectl apply -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/assets/asset_get_safety_data.yaml kubectl apply -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/assets/asset_start_robot.yaml kubectl apply -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/assets/asset_stop_robot.yaml The identifier of the assets is <namespace>/<name> , and it should be used in the FybrikApplication that we will describe later on. For example the identifier of the asset defined in the file asset_start_robot.yaml is fogprotect/api.control.start-robot .","title":"Register the assets"},{"location":"samples/dashboard-sample/#create-jwt-authentication-key","text":"As the HTTP requests should contain the role of the user in the header, the dashboard application uses JWT to pass the JWT of the relevant role in the header. The JWT is authenticated using a secret key that we store as a secret in the cluster, in both fogprotect and fybrik-blueprints namespaces. Create the JWT secret: kubectl apply -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/secrets/jwt_key_secret.yaml kubectl apply -n fybrik-blueprints -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/secrets/jwt_key_secret.yaml","title":"Create JWT authentication key"},{"location":"samples/dashboard-sample/#fybrik-manager-rbac","text":"In order for the fybrik manager to be able to access the assets and to deploy the fybrik module that we created, give the manager the relevant RBAC authorization : kubectl apply -n fybrik-system -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/fybrik-system-manager-rbac.yaml","title":"Fybrik manager RBAC"},{"location":"samples/dashboard-sample/#deploy-the-module","text":"In our case, the module as we described earlier is responsible for intercepting the HTTP requests received from the user. Once a request is received a decision must be made regarding the request, it should be either allowed, columns redacted or blocked depending on the role of the user. The decision is made using OpenPolicyAgent , and applying the policy described in About the dashboard application and specified here . Deploy the fybrik module and application: kubectl apply -n fybrik-system -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/rest-read-module.yaml kubectl apply -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/rest-read-application.yaml kubectl wait --for = condition = ready --all pod --timeout = 120s","title":"Deploy the module"},{"location":"samples/dashboard-sample/#deploy-the-dashboard","text":"We now deploy the dashboard that will display a table that contains the safety data of the factory, along with two buttons to start and stop the manufacturing robot. One can change the role of the user using a pull down menu. First create a port-forwarding to the service that will be intercepting the HTTP requests: kubectl -n fybrik-blueprints port-forward svc/rest-read 5559 :5559 & Afterwards, deploy the dashboard application: helm chart pull ghcr.io/fybrik/factory-gui-chart:v0.0.1 helm chart export --destination = ./tmp ghcr.io/fybrik/factory-gui-chart:v0.0.1 helm install rel1-factory-gui ./tmp/factory_gui kubectl wait --for = condition = ready --all pod --timeout = 120s Lastly, create a port-forwarding to the dashboard service in order to be able to open the dashboard in your browser: kubectl port-forward svc/factory-gui 3001 :3000 & Open your browser and go to http://127.0.0.1:3001.","title":"Deploy the dashboard"},{"location":"samples/dashboard-sample/#cleanup","text":"Stop the port-forwarding: pgrep -f \"kubectl port-forward svc/factory-gui 3001:3000\" | xargs kill pgrep -f \"kubectl -n fybrik-blueprints port-forward svc/rest-read 5559:5559\" | xargs kill Remove the tmp directory that was created temporarily: rm -r tmp Delete the fybrik application and module: kubectl delete fybrikapplication rest-read kubectl -n fybrik-system delete fybrikmodule rest-read-module Delete the fogprotect namespace: kubectl delete namespace fogprotect Delete the RBAC authorization of the manager: kubectl -n fybrik-system delete -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/fybrik-system-manager-rbac.yaml Delete the JWT secret: kubectl delete -n fybrik-blueprints -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/secrets/jwt_key_secret.yaml","title":"Cleanup"},{"location":"samples/delete-sample/","text":"Sample for the delete flow This sample demonstrate how to delete an S3 object from a bucket. Install module To apply the latest development version of the delete-module: kubectl apply -f https://raw.githubusercontent.com/fybrik/delete-module/main/module.yaml -n fybrik-system Prepare dataset This sample uses the Synthetic Financial Datasets For Fraud Detection dataset 1 as the data that the notebook needs to read. Download and extract the file to your machine. You should now see a file named PS_20174392719_1491204439457_log.csv . Alternatively, use a sample of 100 lines of the same dataset by downloading PS_20174392719_1491204439457_log.csv from GitHub. Upload the CSV file to an object storage of your choice such as AWS S3, IBM Cloud Object Storage or Ceph. Make a note of the service endpoint, bucket name, and access credentials. You will need them later. Setup and upload to localstack For experimentation you can install localstack to your cluster instead of using a cloud service. Define variables for access key and secret key export ACCESS_KEY = \"myaccesskey\" export SECRET_KEY = \"mysecretkey\" Install localstack to the currently active namespace and wait for it to be ready: Kubernetes OpenShift helm repo add localstack-charts https://localstack.github.io/helm-charts helm install localstack localstack-charts/localstack \\ --version 0.4.3 \\ --set image.tag=\"1.2.0\" \\ --set startServices=\"s3\" \\ --set service.type=ClusterIP \\ --set livenessProbe.initialDelaySeconds=25 kubectl wait --for=condition=ready --all pod -n fybrik-notebook-sample --timeout=120s helm repo add localstack-charts https://localstack.github.io/helm-charts helm install localstack localstack-charts/localstack \\ --version 0.4.3 \\ --set image.tag=\"1.2.0\" \\ --set startServices=\"s3\" \\ --set service.type=ClusterIP \\ --set livenessProbe.initialDelaySeconds=25 \\ --set persistence.enabled=true \\ --set persistence.storageClass=ibmc-file-gold-gid \\ --set persistence.accessModes[0]=ReadWriteMany kubectl wait --for=condition=ready --all pod -n fybrik-notebook-sample --timeout=120s Create a port-forward to communicate with localstack server: kubectl port-forward svc/localstack 4566 :4566 & Use AWS CLI to upload the dataset to a new created bucket in the localstack server: export ENDPOINT = \"http://127.0.0.1:4566\" export BUCKET = \"demo\" export OBJECT_KEY = \"PS_20174392719_1491204439457_log.csv\" export FILEPATH = \"/path/to/PS_20174392719_1491204439457_log.csv\" export REGION = theshire aws configure set aws_access_key_id ${ ACCESS_KEY } && aws configure set aws_secret_access_key ${ SECRET_KEY } aws configure set region ${ REGION } aws --endpoint-url = ${ ENDPOINT } s3api create-bucket --bucket ${ BUCKET } --region ${ REGION } --create-bucket-configuration LocationConstraint = ${ REGION } aws --endpoint-url = ${ ENDPOINT } s3api put-object --bucket ${ BUCKET } --key ${ OBJECT_KEY } --body ${ FILEPATH } Before we delete the object, we make sure it's been created. You can check with the object storage serive that you used or with AWS CLI : aws --endpoint-url=${ENDPOINT} s3api list-objects --bucket=${BUCKET} You should see the new created object: { \"Contents\": [ { \"Key\": \"PS_20174392719_1491204439457_log.csv\", \"LastModified\": \"2022-06-06T07:12:16.000Z\", \"ETag\": \"\\\"9a34903326938d8c33c29f4a1170a7b1\\\"\", \"Size\": 6551, \"StorageClass\": \"STANDARD\", \"Owner\": { \"DisplayName\": \"webfile\", \"ID\": \"75aa57f09aa0c8caeab4f8c24e99d10f8e7faeebf76c078efc7c6caea54ba06a\" } } ] } Register the dataset in a data catalog In this step you are performing the role of the data owner, registering his data in the data catalog and registering the credentials for accessing the data in the credential manager. With OpenMetadata With Katalog Datasets can be registered either directly, through the OpenMetadata UI, or indirectly, through the data-catalog connector: Register an asset through the OpenMetadata UI Registering Dataset via Connector To register an asset directly through the OpenMetadata UI, follow the instructions here . These instructions also explain how to determine the asset ID. Store the asset ID in a CATALOGED_ASSET variable. For instance: CATALOGED_ASSET = \"openmetadata-s3.default.demo.\\\"PS_20174392719_1491204439457_log.csv\\\"\" We now explain how to register a dataset using the OpenMetadata connector. Begin by registering the credentials required for accessing the dataset as a kubernetes secret. Replace the values for access_key and secret_key with the values from the object storage service that you used and run: cat << EOF | kubectl apply -f - apiVersion : v1 kind : Secret metadata : name : paysim-csv type : Opaque stringData : access_key : \"${ACCESS_KEY}\" secret_key : \"${SECRET_KEY}\" EOF Next, register the data asset itself in the data catalog. We use port-forwarding to send asset creation requests to the OpenMetadata connector. kubectl port-forward svc/openmetadata-connector -n fybrik-system 8081 :8080 & cat << EOF | curl -X POST localhost:8081/createAsset -d @- { \"destinationCatalogID\": \"openmetadata\", \"destinationAssetID\": \"paysim-csv\", \"credentials\": \"/v1/kubernetes-secrets/paysim-csv?namespace=fybrik-notebook-sample\", \"details\": { \"dataFormat\": \"csv\", \"connection\": { \"name\": \"s3\", \"s3\": { \"endpoint\": \"http://localstack.fybrik-notebook-sample.svc.cluster.local:4566\", \"bucket\": \"demo\", \"object_key\": \"PS_20174392719_1491204439457_log.csv\" } } }, \"resourceMetadata\": { \"name\": \"Synthetic Financial Datasets For Fraud Detection\", \"geography\": \"theshire \", \"tags\": { \"Purpose.finance\": \"true\" }, \"columns\": [ { \"name\": \"nameOrig\", \"tags\": { \"PII.Sensitive\": \"true\" } }, { \"name\": \"oldbalanceOrg\", \"tags\": { \"PII.Sensitive\": \"true\" } }, { \"name\": \"newbalanceOrig\", \"tags\": { \"PII.Sensitive\": \"true\" } } ] } } EOF The response from the OpenMetadata connector should look like this: { \"assetID\" : \"openmetadata-s3.default.demo.\\\"PS_20174392719_1491204439457_log.csv\\\"\" } Store the asset ID in a CATALOGED_ASSET variable: CATALOGED_ASSET = \"openmetadata-s3.default.demo.\\\"PS_20174392719_1491204439457_log.csv\\\"\" We now explain how to register a dataset in the Katalog data catalog. Begin by registering the credentials required for accessing the dataset as a kubernetes secret. Replace the values for access_key and secret_key with the values from the object storage service that you used and run: cat << EOF | kubectl apply -f - apiVersion : v1 kind : Secret metadata : name : paysim-csv type : Opaque stringData : access_key : \"${ACCESS_KEY}\" secret_key : \"${SECRET_KEY}\" EOF Next, register the data asset itself in the data catalog. We use port-forwarding to send asset creation requests to the Katalog connector. cat << EOF | kubectl apply -f - apiVersion: katalog.fybrik.io/v1alpha1 kind: Asset metadata: name: paysim-csv spec: secretRef: name: paysim-csv details: dataFormat: csv connection: name: s3 s3: endpoint: \"http://localstack.fybrik-notebook-sample.svc.cluster.local:4566\" bucket: \"demo\" object_key: \"PS_20174392719_1491204439457_log.csv\" metadata: name: Synthetic Financial Datasets For Fraud Detection geography: theshire tags: Purpose.finance: true columns: - name: nameOrig tags: PII.Sensitive: true - name: oldbalanceOrg tags: PII.Sensitive: true - name: newbalanceOrig tags: PII.Sensitive: true EOF Store the asset name in a CATALOGED_ASSET variable: CATALOGED_ASSET = \"fybrik-notebook-sample/paysim-csv\" The asset is now registered in the catalog. Notice the resourceMetadata field above. It specifies the dataset geography and tags. These attributes can later be used in policies. For example, in the json above, the geography is set to theshire . You need make sure that it is same as the region of your fybrik control plane. You can get this information using the following command: kubectl get configmap cluster-metadata -n fybrik-system -o 'jsonpath={.data.Region}' Quick Start installs a fybrik control plane with the region theshire by default. If you change it or the geography in the json above, a copy module will be required by the policies, but we do not install any copy module in the Quick Start . Define data access policy Acting as the data steward, define an OpenPolicyAgent policy. In this sample we only specify the action taken. Below is the policy (written in Rego language): package dataapi.authz rule[{}] { description := \"allow the delete operation\" input.action.actionType == \"delete\" } In this sample only the policy above is applied. Copy the policy to a file named sample-policy.rego and then run: NS = \"fybrik-system\" ; kubectl -n $NS get configmap | awk '/sample/{print $1}' | xargs kubectl delete -n $NS configmap kubectl -n fybrik-system create configmap sample-policy --from-file = sample-policy.rego kubectl -n fybrik-system label configmap sample-policy openpolicyagent.org/policy = rego while [[ $( kubectl get cm sample-policy -n fybrik-system -o 'jsonpath={.metadata.annotations.openpolicyagent\\.org/policy-status}' ) ! = '{\"status\":\"ok\"}' ]] ; do echo \"waiting for policy to be applied\" && sleep 5 ; done You can similarly apply a directory holding multiple rego files. Create a FybrikApplication resource Create a FybrikApplication resource to register the notebook workload to the control plane of Fybrik: cat <<EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1beta1 kind : FybrikApplication metadata : name : delete-app namespace : fybrik-notebook-sample spec : selector : workloadSelector : matchLabels : {} appInfo : intent : Fraud Detection role : Security data : - dataSetID : ${CATALOGED_ASSET} flow : delete requirements : {} EOF Notice that the data field includes a dataSetID that matches the asset identifier in the catalog. Run the following command to wait until the FybrikApplication is ready: while [[ $( kubectl get fybrikapplication delete-app -o 'jsonpath={.status.ready}' ) ! = \"true\" ]] ; do echo \"waiting for FybrikApplication\" && sleep 5 ; done CATALOGED_ASSET_MODIFIED = $( echo $CATALOGED_ASSET | sed 's/\\./\\\\\\./g' ) while [[ $( kubectl get fybrikapplication delete-app -o \"jsonpath={.status.assetStates. ${ CATALOGED_ASSET_MODIFIED } .conditions[?(@.type == 'Ready')].status}\" ) ! = \"True\" ]] ; do echo \"waiting for ${ CATALOGED_ASSET } asset\" && sleep 5 ; done Ensure the object is deleted Now the object should be deleted. We can check again with AWS CLI : aws --endpoint-url=${ENDPOINT} s3api list-objects --bucket=${BUCKET} Now you should see that the object is no longer in the list (or that the bucket is empty). Created by NTNU and shared under the CC BY-SA 4.0 license. \u21a9","title":"Sample for the delete flow"},{"location":"samples/delete-sample/#sample-for-the-delete-flow","text":"This sample demonstrate how to delete an S3 object from a bucket.","title":"Sample for the delete flow"},{"location":"samples/delete-sample/#install-module","text":"To apply the latest development version of the delete-module: kubectl apply -f https://raw.githubusercontent.com/fybrik/delete-module/main/module.yaml -n fybrik-system","title":"Install module"},{"location":"samples/delete-sample/#prepare-dataset","text":"This sample uses the Synthetic Financial Datasets For Fraud Detection dataset 1 as the data that the notebook needs to read. Download and extract the file to your machine. You should now see a file named PS_20174392719_1491204439457_log.csv . Alternatively, use a sample of 100 lines of the same dataset by downloading PS_20174392719_1491204439457_log.csv from GitHub. Upload the CSV file to an object storage of your choice such as AWS S3, IBM Cloud Object Storage or Ceph. Make a note of the service endpoint, bucket name, and access credentials. You will need them later. Setup and upload to localstack For experimentation you can install localstack to your cluster instead of using a cloud service. Define variables for access key and secret key export ACCESS_KEY = \"myaccesskey\" export SECRET_KEY = \"mysecretkey\" Install localstack to the currently active namespace and wait for it to be ready: Kubernetes OpenShift helm repo add localstack-charts https://localstack.github.io/helm-charts helm install localstack localstack-charts/localstack \\ --version 0.4.3 \\ --set image.tag=\"1.2.0\" \\ --set startServices=\"s3\" \\ --set service.type=ClusterIP \\ --set livenessProbe.initialDelaySeconds=25 kubectl wait --for=condition=ready --all pod -n fybrik-notebook-sample --timeout=120s helm repo add localstack-charts https://localstack.github.io/helm-charts helm install localstack localstack-charts/localstack \\ --version 0.4.3 \\ --set image.tag=\"1.2.0\" \\ --set startServices=\"s3\" \\ --set service.type=ClusterIP \\ --set livenessProbe.initialDelaySeconds=25 \\ --set persistence.enabled=true \\ --set persistence.storageClass=ibmc-file-gold-gid \\ --set persistence.accessModes[0]=ReadWriteMany kubectl wait --for=condition=ready --all pod -n fybrik-notebook-sample --timeout=120s Create a port-forward to communicate with localstack server: kubectl port-forward svc/localstack 4566 :4566 & Use AWS CLI to upload the dataset to a new created bucket in the localstack server: export ENDPOINT = \"http://127.0.0.1:4566\" export BUCKET = \"demo\" export OBJECT_KEY = \"PS_20174392719_1491204439457_log.csv\" export FILEPATH = \"/path/to/PS_20174392719_1491204439457_log.csv\" export REGION = theshire aws configure set aws_access_key_id ${ ACCESS_KEY } && aws configure set aws_secret_access_key ${ SECRET_KEY } aws configure set region ${ REGION } aws --endpoint-url = ${ ENDPOINT } s3api create-bucket --bucket ${ BUCKET } --region ${ REGION } --create-bucket-configuration LocationConstraint = ${ REGION } aws --endpoint-url = ${ ENDPOINT } s3api put-object --bucket ${ BUCKET } --key ${ OBJECT_KEY } --body ${ FILEPATH } Before we delete the object, we make sure it's been created. You can check with the object storage serive that you used or with AWS CLI : aws --endpoint-url=${ENDPOINT} s3api list-objects --bucket=${BUCKET} You should see the new created object: { \"Contents\": [ { \"Key\": \"PS_20174392719_1491204439457_log.csv\", \"LastModified\": \"2022-06-06T07:12:16.000Z\", \"ETag\": \"\\\"9a34903326938d8c33c29f4a1170a7b1\\\"\", \"Size\": 6551, \"StorageClass\": \"STANDARD\", \"Owner\": { \"DisplayName\": \"webfile\", \"ID\": \"75aa57f09aa0c8caeab4f8c24e99d10f8e7faeebf76c078efc7c6caea54ba06a\" } } ] }","title":"Prepare dataset"},{"location":"samples/delete-sample/#register-the-dataset-in-a-data-catalog","text":"In this step you are performing the role of the data owner, registering his data in the data catalog and registering the credentials for accessing the data in the credential manager. With OpenMetadata With Katalog Datasets can be registered either directly, through the OpenMetadata UI, or indirectly, through the data-catalog connector: Register an asset through the OpenMetadata UI Registering Dataset via Connector To register an asset directly through the OpenMetadata UI, follow the instructions here . These instructions also explain how to determine the asset ID. Store the asset ID in a CATALOGED_ASSET variable. For instance: CATALOGED_ASSET = \"openmetadata-s3.default.demo.\\\"PS_20174392719_1491204439457_log.csv\\\"\" We now explain how to register a dataset using the OpenMetadata connector. Begin by registering the credentials required for accessing the dataset as a kubernetes secret. Replace the values for access_key and secret_key with the values from the object storage service that you used and run: cat << EOF | kubectl apply -f - apiVersion : v1 kind : Secret metadata : name : paysim-csv type : Opaque stringData : access_key : \"${ACCESS_KEY}\" secret_key : \"${SECRET_KEY}\" EOF Next, register the data asset itself in the data catalog. We use port-forwarding to send asset creation requests to the OpenMetadata connector. kubectl port-forward svc/openmetadata-connector -n fybrik-system 8081 :8080 & cat << EOF | curl -X POST localhost:8081/createAsset -d @- { \"destinationCatalogID\": \"openmetadata\", \"destinationAssetID\": \"paysim-csv\", \"credentials\": \"/v1/kubernetes-secrets/paysim-csv?namespace=fybrik-notebook-sample\", \"details\": { \"dataFormat\": \"csv\", \"connection\": { \"name\": \"s3\", \"s3\": { \"endpoint\": \"http://localstack.fybrik-notebook-sample.svc.cluster.local:4566\", \"bucket\": \"demo\", \"object_key\": \"PS_20174392719_1491204439457_log.csv\" } } }, \"resourceMetadata\": { \"name\": \"Synthetic Financial Datasets For Fraud Detection\", \"geography\": \"theshire \", \"tags\": { \"Purpose.finance\": \"true\" }, \"columns\": [ { \"name\": \"nameOrig\", \"tags\": { \"PII.Sensitive\": \"true\" } }, { \"name\": \"oldbalanceOrg\", \"tags\": { \"PII.Sensitive\": \"true\" } }, { \"name\": \"newbalanceOrig\", \"tags\": { \"PII.Sensitive\": \"true\" } } ] } } EOF The response from the OpenMetadata connector should look like this: { \"assetID\" : \"openmetadata-s3.default.demo.\\\"PS_20174392719_1491204439457_log.csv\\\"\" } Store the asset ID in a CATALOGED_ASSET variable: CATALOGED_ASSET = \"openmetadata-s3.default.demo.\\\"PS_20174392719_1491204439457_log.csv\\\"\" We now explain how to register a dataset in the Katalog data catalog. Begin by registering the credentials required for accessing the dataset as a kubernetes secret. Replace the values for access_key and secret_key with the values from the object storage service that you used and run: cat << EOF | kubectl apply -f - apiVersion : v1 kind : Secret metadata : name : paysim-csv type : Opaque stringData : access_key : \"${ACCESS_KEY}\" secret_key : \"${SECRET_KEY}\" EOF Next, register the data asset itself in the data catalog. We use port-forwarding to send asset creation requests to the Katalog connector. cat << EOF | kubectl apply -f - apiVersion: katalog.fybrik.io/v1alpha1 kind: Asset metadata: name: paysim-csv spec: secretRef: name: paysim-csv details: dataFormat: csv connection: name: s3 s3: endpoint: \"http://localstack.fybrik-notebook-sample.svc.cluster.local:4566\" bucket: \"demo\" object_key: \"PS_20174392719_1491204439457_log.csv\" metadata: name: Synthetic Financial Datasets For Fraud Detection geography: theshire tags: Purpose.finance: true columns: - name: nameOrig tags: PII.Sensitive: true - name: oldbalanceOrg tags: PII.Sensitive: true - name: newbalanceOrig tags: PII.Sensitive: true EOF Store the asset name in a CATALOGED_ASSET variable: CATALOGED_ASSET = \"fybrik-notebook-sample/paysim-csv\" The asset is now registered in the catalog. Notice the resourceMetadata field above. It specifies the dataset geography and tags. These attributes can later be used in policies. For example, in the json above, the geography is set to theshire . You need make sure that it is same as the region of your fybrik control plane. You can get this information using the following command: kubectl get configmap cluster-metadata -n fybrik-system -o 'jsonpath={.data.Region}' Quick Start installs a fybrik control plane with the region theshire by default. If you change it or the geography in the json above, a copy module will be required by the policies, but we do not install any copy module in the Quick Start .","title":"Register the dataset in a data catalog"},{"location":"samples/delete-sample/#define-data-access-policy","text":"Acting as the data steward, define an OpenPolicyAgent policy. In this sample we only specify the action taken. Below is the policy (written in Rego language): package dataapi.authz rule[{}] { description := \"allow the delete operation\" input.action.actionType == \"delete\" } In this sample only the policy above is applied. Copy the policy to a file named sample-policy.rego and then run: NS = \"fybrik-system\" ; kubectl -n $NS get configmap | awk '/sample/{print $1}' | xargs kubectl delete -n $NS configmap kubectl -n fybrik-system create configmap sample-policy --from-file = sample-policy.rego kubectl -n fybrik-system label configmap sample-policy openpolicyagent.org/policy = rego while [[ $( kubectl get cm sample-policy -n fybrik-system -o 'jsonpath={.metadata.annotations.openpolicyagent\\.org/policy-status}' ) ! = '{\"status\":\"ok\"}' ]] ; do echo \"waiting for policy to be applied\" && sleep 5 ; done You can similarly apply a directory holding multiple rego files.","title":"Define data access policy"},{"location":"samples/delete-sample/#create-a-fybrikapplication-resource","text":"Create a FybrikApplication resource to register the notebook workload to the control plane of Fybrik: cat <<EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1beta1 kind : FybrikApplication metadata : name : delete-app namespace : fybrik-notebook-sample spec : selector : workloadSelector : matchLabels : {} appInfo : intent : Fraud Detection role : Security data : - dataSetID : ${CATALOGED_ASSET} flow : delete requirements : {} EOF Notice that the data field includes a dataSetID that matches the asset identifier in the catalog. Run the following command to wait until the FybrikApplication is ready: while [[ $( kubectl get fybrikapplication delete-app -o 'jsonpath={.status.ready}' ) ! = \"true\" ]] ; do echo \"waiting for FybrikApplication\" && sleep 5 ; done CATALOGED_ASSET_MODIFIED = $( echo $CATALOGED_ASSET | sed 's/\\./\\\\\\./g' ) while [[ $( kubectl get fybrikapplication delete-app -o \"jsonpath={.status.assetStates. ${ CATALOGED_ASSET_MODIFIED } .conditions[?(@.type == 'Ready')].status}\" ) ! = \"True\" ]] ; do echo \"waiting for ${ CATALOGED_ASSET } asset\" && sleep 5 ; done","title":"Create a FybrikApplication resource"},{"location":"samples/delete-sample/#ensure-the-object-is-deleted","text":"Now the object should be deleted. We can check again with AWS CLI : aws --endpoint-url=${ENDPOINT} s3api list-objects --bucket=${BUCKET} Now you should see that the object is no longer in the list (or that the bucket is empty). Created by NTNU and shared under the CC BY-SA 4.0 license. \u21a9","title":"Ensure the object is deleted"},{"location":"samples/notebook-read/","text":"Notebook sample for the read flow This sample demonstrates the following: how Fybrik enables a Jupyter notebook workload to access a cataloged dataset. how arrow-flight module is used for reading and transforming data. how policies regarding the use of personal information are seamlessly applied when accessing a dataset containing financial data. In this sample you play multiple roles: As a data owner you upload a dataset and register it in a data catalog As a data steward you setup data governance policies As a data user you specify your data usage requirements and use a notebook to consume the data Prepare a dataset to be accessed by the notebook This sample uses the Synthetic Financial Datasets For Fraud Detection dataset 1 as the data that the notebook needs to read. Download and extract the file to your machine. You should now see a file named PS_20174392719_1491204439457_log.csv . Alternatively, use a sample of 100 lines of the same dataset by downloading PS_20174392719_1491204439457_log.csv from GitHub. Upload the CSV file to an object storage of your choice such as AWS S3, IBM Cloud Object Storage or Ceph. Make a note of the service endpoint, bucket name, and access credentials. You will need them later. Setup and upload to localstack For experimentation you can install localstack to your cluster instead of using a cloud service. Define variables for access key and secret key export ACCESS_KEY = \"myaccesskey\" export SECRET_KEY = \"mysecretkey\" Install localstack to the currently active namespace and wait for it to be ready: Kubernetes OpenShift helm repo add localstack-charts https://localstack.github.io/helm-charts helm install localstack localstack-charts/localstack \\ --version 0.4.3 \\ --set image.tag=\"1.2.0\" \\ --set startServices=\"s3\" \\ --set service.type=ClusterIP \\ --set livenessProbe.initialDelaySeconds=25 kubectl wait --for=condition=ready --all pod -n fybrik-notebook-sample --timeout=120s helm repo add localstack-charts https://localstack.github.io/helm-charts helm install localstack localstack-charts/localstack \\ --version 0.4.3 \\ --set image.tag=\"1.2.0\" \\ --set startServices=\"s3\" \\ --set service.type=ClusterIP \\ --set livenessProbe.initialDelaySeconds=25 \\ --set persistence.enabled=true \\ --set persistence.storageClass=ibmc-file-gold-gid \\ --set persistence.accessModes[0]=ReadWriteMany kubectl wait --for=condition=ready --all pod -n fybrik-notebook-sample --timeout=120s Create a port-forward to communicate with localstack server: kubectl port-forward svc/localstack 4566 :4566 & Use AWS CLI to upload the dataset to a new created bucket in the localstack server: export ENDPOINT = \"http://127.0.0.1:4566\" export BUCKET = \"demo\" export OBJECT_KEY = \"PS_20174392719_1491204439457_log.csv\" export FILEPATH = \"/path/to/PS_20174392719_1491204439457_log.csv\" export REGION = theshire aws configure set aws_access_key_id ${ ACCESS_KEY } && aws configure set aws_secret_access_key ${ SECRET_KEY } aws configure set region ${ REGION } aws --endpoint-url = ${ ENDPOINT } s3api create-bucket --bucket ${ BUCKET } --region ${ REGION } --create-bucket-configuration LocationConstraint = ${ REGION } aws --endpoint-url = ${ ENDPOINT } s3api put-object --bucket ${ BUCKET } --key ${ OBJECT_KEY } --body ${ FILEPATH } Register the dataset in a data catalog In this step you are performing the role of the data owner, registering his data in the data catalog and registering the credentials for accessing the data in the credential manager. With OpenMetadata With Katalog Datasets can be registered either directly, through the OpenMetadata UI, or indirectly, through the data-catalog connector: Register an asset through the OpenMetadata UI Registering Dataset via Connector To register an asset directly through the OpenMetadata UI, follow the instructions here . These instructions also explain how to determine the asset ID. Store the asset ID in a CATALOGED_ASSET variable. For instance: CATALOGED_ASSET = \"openmetadata-s3.default.demo.\\\"PS_20174392719_1491204439457_log.csv\\\"\" We now explain how to register a dataset using the OpenMetadata connector. Begin by registering the credentials required for accessing the dataset as a kubernetes secret. Replace the values for access_key and secret_key with the values from the object storage service that you used and run: cat << EOF | kubectl apply -f - apiVersion : v1 kind : Secret metadata : name : paysim-csv type : Opaque stringData : access_key : \"${ACCESS_KEY}\" secret_key : \"${SECRET_KEY}\" EOF Next, register the data asset itself in the data catalog. We use port-forwarding to send asset creation requests to the OpenMetadata connector. kubectl port-forward svc/openmetadata-connector -n fybrik-system 8081 :8080 & cat << EOF | curl -X POST localhost:8081/createAsset -d @- { \"destinationCatalogID\": \"openmetadata\", \"destinationAssetID\": \"paysim-csv\", \"credentials\": \"/v1/kubernetes-secrets/paysim-csv?namespace=fybrik-notebook-sample\", \"details\": { \"dataFormat\": \"csv\", \"connection\": { \"name\": \"s3\", \"s3\": { \"endpoint\": \"http://localstack.fybrik-notebook-sample.svc.cluster.local:4566\", \"bucket\": \"demo\", \"object_key\": \"PS_20174392719_1491204439457_log.csv\" } } }, \"resourceMetadata\": { \"name\": \"Synthetic Financial Datasets For Fraud Detection\", \"geography\": \"theshire \", \"tags\": { \"Purpose.finance\": \"true\" }, \"columns\": [ { \"name\": \"nameOrig\", \"tags\": { \"PII.Sensitive\": \"true\" } }, { \"name\": \"oldbalanceOrg\", \"tags\": { \"PII.Sensitive\": \"true\" } }, { \"name\": \"newbalanceOrig\", \"tags\": { \"PII.Sensitive\": \"true\" } } ] } } EOF The response from the OpenMetadata connector should look like this: { \"assetID\" : \"openmetadata-s3.default.demo.\\\"PS_20174392719_1491204439457_log.csv\\\"\" } Store the asset ID in a CATALOGED_ASSET variable: CATALOGED_ASSET = \"openmetadata-s3.default.demo.\\\"PS_20174392719_1491204439457_log.csv\\\"\" We now explain how to register a dataset in the Katalog data catalog. Begin by registering the credentials required for accessing the dataset as a kubernetes secret. Replace the values for access_key and secret_key with the values from the object storage service that you used and run: cat << EOF | kubectl apply -f - apiVersion : v1 kind : Secret metadata : name : paysim-csv type : Opaque stringData : access_key : \"${ACCESS_KEY}\" secret_key : \"${SECRET_KEY}\" EOF Next, register the data asset itself in the data catalog. We use port-forwarding to send asset creation requests to the Katalog connector. cat << EOF | kubectl apply -f - apiVersion: katalog.fybrik.io/v1alpha1 kind: Asset metadata: name: paysim-csv spec: secretRef: name: paysim-csv details: dataFormat: csv connection: name: s3 s3: endpoint: \"http://localstack.fybrik-notebook-sample.svc.cluster.local:4566\" bucket: \"demo\" object_key: \"PS_20174392719_1491204439457_log.csv\" metadata: name: Synthetic Financial Datasets For Fraud Detection geography: theshire tags: Purpose.finance: true columns: - name: nameOrig tags: PII.Sensitive: true - name: oldbalanceOrg tags: PII.Sensitive: true - name: newbalanceOrig tags: PII.Sensitive: true EOF Store the asset name in a CATALOGED_ASSET variable: CATALOGED_ASSET = \"fybrik-notebook-sample/paysim-csv\" If you look at the asset creation request above, you will notice that in the resourceMetadata field, we request that the asset should be tagged with the Purpose.finance tag, and that three of its columns should be tagged with the PII.Sensitive tag. Those tags will be referenced below in the access policy rules. Tags are important because they are used to determine whether an application would be allowed to access a dataset, and if so, which transformations should be applied to it. The asset is now registered in the catalog. Notice the resourceMetadata field above. It specifies the dataset geography and tags. These attributes can later be used in policies. For example, in the json above, the geography is set to theshire . You need make sure that it is same as the region of your fybrik control plane. You can get this information using the following command: kubectl get configmap cluster-metadata -n fybrik-system -o 'jsonpath={.data.Region}' Quick Start installs a fybrik control plane with the region theshire by default. If you change it or the geography in the json above, a copy module will be required by the policies, but we do not install any copy module in the Quick Start . Define data access policies Acting as the data steward, define an OpenPolicyAgent policy to redact the columns tagged as PII.Sensitive for datasets tagged with Purpose.finance . Below is the policy (written in Rego language): package dataapi.authz # If the conditions between the curly braces are true then Fybrik will get an object containing information about # \"RedactAction\". rule[{\"action\": {\"name\":\"RedactAction\", \"columns\": column_names}, \"policy\": description}] { description := \"Redact columns tagged as PII.Sensitive in datasets tagged with Purpose.finance = true\" # this condition is true if it is a read operation input.action.actionType == \"read\" # this condition is true if the asset has \"Purpose.finance\" tag input.resource.metadata.tags[\"Purpose.finance\"] # this statement assigns to column_names variable all the columns that contain \"PII.Sensitive\" tag column_names := [input.resource.metadata.columns[i].name | input.resource.metadata.columns[i].tags[\"PII.Sensitive\"]] # this condition is true if column_names is not empty # we need this check to apply the RedactAction action only in cases where sensitive data exists. count(column_names) > 0 } In this sample only the policy above is applied. Copy the policy to a file named sample-policy.rego and then run: kubectl -n fybrik-system create configmap sample-policy --from-file = sample-policy.rego kubectl -n fybrik-system label configmap sample-policy openpolicyagent.org/policy = rego while [[ $( kubectl get cm sample-policy -n fybrik-system -o 'jsonpath={.metadata.annotations.openpolicyagent\\.org/policy-status}' ) ! = '{\"status\":\"ok\"}' ]] ; do echo \"waiting for policy to be applied\" && sleep 5 ; done You can similarly apply a directory holding multiple rego files. Deploy a Jupyter notebook In this sample a Jupyter notebook is used as the user workload and its business logic requires reading the asset that we registered (e.g., for creating a fraud detection model). Deploy a notebook to your cluster: Deploy JupyterLab: kubectl create deployment my-notebook --image = jupyter/base-notebook --port = 8888 -- start.sh jupyter lab --LabApp.token = '' kubectl set env deployment my-notebook JUPYTER_ENABLE_LAB = yes kubectl label deployment my-notebook app.kubernetes.io/name = my-notebook kubectl wait --for = condition = available --timeout = 120s deployment/my-notebook kubectl expose deployment my-notebook --port = 80 --target-port = 8888 Create a port-forward to communicate with JupyterLab: kubectl port-forward svc/my-notebook 8080 :80 & Open your browser and go to http://localhost:8080/ . Create a new notebook in the server Create a FybrikApplication resource for the notebook Create a FybrikApplication resource to register the notebook workload to the control plane of Fybrik. The value you place in the dataSetID field is your asset ID, as explained above. If you registered your dataset through the data catalog connector, enter the assetID which was returned to you by the connector, e.g. \"openmetadata-s3.default.demo.\\\"PS_20174392719_1491204439457_log.csv\\\"\" . cat <<EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1beta1 kind : FybrikApplication metadata : name : my-notebook labels : app : my-notebook spec : selector : workloadSelector : matchLabels : app : my-notebook appInfo : intent : Fraud Detection data : - dataSetID : ${CATALOGED_ASSET} requirements : interface : protocol : fybrik-arrow-flight EOF Notice that: The selector field matches the labels of our Jupyter notebook workload. The data field includes a dataSetID that matches the asset identifier in the catalog. The protocol indicates that the developer wants to consume the data using Apache Arrow Flight. For some protocols a dataformat can be specified as well (e.g., s3 protocol and parquet format). Run the following command to wait until the FybrikApplication is ready and set the CATALOGED_ASSET_MODIFIED environment variable: while [[ $( kubectl get fybrikapplication my-notebook -o 'jsonpath={.status.ready}' ) ! = \"true\" ]] ; do echo \"waiting for FybrikApplication\" && sleep 5 ; done CATALOGED_ASSET_MODIFIED = $( echo $CATALOGED_ASSET | sed 's/\\./\\\\\\./g' ) while [[ $( kubectl get fybrikapplication my-notebook -o \"jsonpath={.status.assetStates. ${ CATALOGED_ASSET_MODIFIED } .conditions[?(@.type == 'Ready')].status}\" ) ! = \"True\" ]] ; do echo \"waiting for ${ CATALOGED_ASSET } asset\" && sleep 5 ; done Read the dataset from the notebook In your terminal , run the following command to print the endpoint to use for reading the data. It fetches the code from the FybrikApplication resource: ENDPOINT_SCHEME = $( kubectl get fybrikapplication my-notebook -o \"jsonpath={.status.assetStates. ${ CATALOGED_ASSET_MODIFIED } .endpoint.fybrik-arrow-flight.scheme}\" ) ENDPOINT_HOSTNAME = $( kubectl get fybrikapplication my-notebook -o \"jsonpath={.status.assetStates. ${ CATALOGED_ASSET_MODIFIED } .endpoint.fybrik-arrow-flight.hostname}\" ) ENDPOINT_PORT = $( kubectl get fybrikapplication my-notebook -o \"jsonpath={.status.assetStates. ${ CATALOGED_ASSET_MODIFIED } .endpoint.fybrik-arrow-flight.port}\" ) printf \"\\n ${ ENDPOINT_SCHEME } :// ${ ENDPOINT_HOSTNAME } : ${ ENDPOINT_PORT } \\n\\n\" The next steps use the endpoint to read the data in a python notebook Insert a new notebook cell to install pandas and pyarrow packages: % pip install pandas pyarrow == 7.0 .* Insert a new notebook cell to read the data. You need to replace both the ENDPOINT and the CATALOGED_ASSET values, which were obtained in previous steps: import json import pyarrow.flight as fl import pandas as pd # Create a Flight client client = fl.connect ( '<ENDPOINT>' ) # Prepare the request request = { \"asset\" : '<CATALOGED_ASSET>' , # To request specific columns add to the request a \"columns\" key with a list of column names # \"columns\": [...] } # Send request and fetch result as a pandas DataFrame info = client.get_flight_info ( fl.FlightDescriptor.for_command ( json.dumps ( request ))) reader: fl.FlightStreamReader = client.do_get ( info.endpoints [ 0 ] .ticket ) df: pd.DataFrame = reader.read_pandas () Insert a new notebook cell with the following command to visualize the result: df Execute all notebook cells and notice that some of the columns appear redacted. Created by NTNU and shared under the CC BY-SA 4.0 license. \u21a9","title":"Notebook sample for the read flow"},{"location":"samples/notebook-read/#notebook-sample-for-the-read-flow","text":"This sample demonstrates the following: how Fybrik enables a Jupyter notebook workload to access a cataloged dataset. how arrow-flight module is used for reading and transforming data. how policies regarding the use of personal information are seamlessly applied when accessing a dataset containing financial data. In this sample you play multiple roles: As a data owner you upload a dataset and register it in a data catalog As a data steward you setup data governance policies As a data user you specify your data usage requirements and use a notebook to consume the data","title":"Notebook sample for the read flow"},{"location":"samples/notebook-read/#prepare-a-dataset-to-be-accessed-by-the-notebook","text":"This sample uses the Synthetic Financial Datasets For Fraud Detection dataset 1 as the data that the notebook needs to read. Download and extract the file to your machine. You should now see a file named PS_20174392719_1491204439457_log.csv . Alternatively, use a sample of 100 lines of the same dataset by downloading PS_20174392719_1491204439457_log.csv from GitHub. Upload the CSV file to an object storage of your choice such as AWS S3, IBM Cloud Object Storage or Ceph. Make a note of the service endpoint, bucket name, and access credentials. You will need them later. Setup and upload to localstack For experimentation you can install localstack to your cluster instead of using a cloud service. Define variables for access key and secret key export ACCESS_KEY = \"myaccesskey\" export SECRET_KEY = \"mysecretkey\" Install localstack to the currently active namespace and wait for it to be ready: Kubernetes OpenShift helm repo add localstack-charts https://localstack.github.io/helm-charts helm install localstack localstack-charts/localstack \\ --version 0.4.3 \\ --set image.tag=\"1.2.0\" \\ --set startServices=\"s3\" \\ --set service.type=ClusterIP \\ --set livenessProbe.initialDelaySeconds=25 kubectl wait --for=condition=ready --all pod -n fybrik-notebook-sample --timeout=120s helm repo add localstack-charts https://localstack.github.io/helm-charts helm install localstack localstack-charts/localstack \\ --version 0.4.3 \\ --set image.tag=\"1.2.0\" \\ --set startServices=\"s3\" \\ --set service.type=ClusterIP \\ --set livenessProbe.initialDelaySeconds=25 \\ --set persistence.enabled=true \\ --set persistence.storageClass=ibmc-file-gold-gid \\ --set persistence.accessModes[0]=ReadWriteMany kubectl wait --for=condition=ready --all pod -n fybrik-notebook-sample --timeout=120s Create a port-forward to communicate with localstack server: kubectl port-forward svc/localstack 4566 :4566 & Use AWS CLI to upload the dataset to a new created bucket in the localstack server: export ENDPOINT = \"http://127.0.0.1:4566\" export BUCKET = \"demo\" export OBJECT_KEY = \"PS_20174392719_1491204439457_log.csv\" export FILEPATH = \"/path/to/PS_20174392719_1491204439457_log.csv\" export REGION = theshire aws configure set aws_access_key_id ${ ACCESS_KEY } && aws configure set aws_secret_access_key ${ SECRET_KEY } aws configure set region ${ REGION } aws --endpoint-url = ${ ENDPOINT } s3api create-bucket --bucket ${ BUCKET } --region ${ REGION } --create-bucket-configuration LocationConstraint = ${ REGION } aws --endpoint-url = ${ ENDPOINT } s3api put-object --bucket ${ BUCKET } --key ${ OBJECT_KEY } --body ${ FILEPATH }","title":"Prepare a dataset to be accessed by the notebook"},{"location":"samples/notebook-read/#register-the-dataset-in-a-data-catalog","text":"In this step you are performing the role of the data owner, registering his data in the data catalog and registering the credentials for accessing the data in the credential manager. With OpenMetadata With Katalog Datasets can be registered either directly, through the OpenMetadata UI, or indirectly, through the data-catalog connector: Register an asset through the OpenMetadata UI Registering Dataset via Connector To register an asset directly through the OpenMetadata UI, follow the instructions here . These instructions also explain how to determine the asset ID. Store the asset ID in a CATALOGED_ASSET variable. For instance: CATALOGED_ASSET = \"openmetadata-s3.default.demo.\\\"PS_20174392719_1491204439457_log.csv\\\"\" We now explain how to register a dataset using the OpenMetadata connector. Begin by registering the credentials required for accessing the dataset as a kubernetes secret. Replace the values for access_key and secret_key with the values from the object storage service that you used and run: cat << EOF | kubectl apply -f - apiVersion : v1 kind : Secret metadata : name : paysim-csv type : Opaque stringData : access_key : \"${ACCESS_KEY}\" secret_key : \"${SECRET_KEY}\" EOF Next, register the data asset itself in the data catalog. We use port-forwarding to send asset creation requests to the OpenMetadata connector. kubectl port-forward svc/openmetadata-connector -n fybrik-system 8081 :8080 & cat << EOF | curl -X POST localhost:8081/createAsset -d @- { \"destinationCatalogID\": \"openmetadata\", \"destinationAssetID\": \"paysim-csv\", \"credentials\": \"/v1/kubernetes-secrets/paysim-csv?namespace=fybrik-notebook-sample\", \"details\": { \"dataFormat\": \"csv\", \"connection\": { \"name\": \"s3\", \"s3\": { \"endpoint\": \"http://localstack.fybrik-notebook-sample.svc.cluster.local:4566\", \"bucket\": \"demo\", \"object_key\": \"PS_20174392719_1491204439457_log.csv\" } } }, \"resourceMetadata\": { \"name\": \"Synthetic Financial Datasets For Fraud Detection\", \"geography\": \"theshire \", \"tags\": { \"Purpose.finance\": \"true\" }, \"columns\": [ { \"name\": \"nameOrig\", \"tags\": { \"PII.Sensitive\": \"true\" } }, { \"name\": \"oldbalanceOrg\", \"tags\": { \"PII.Sensitive\": \"true\" } }, { \"name\": \"newbalanceOrig\", \"tags\": { \"PII.Sensitive\": \"true\" } } ] } } EOF The response from the OpenMetadata connector should look like this: { \"assetID\" : \"openmetadata-s3.default.demo.\\\"PS_20174392719_1491204439457_log.csv\\\"\" } Store the asset ID in a CATALOGED_ASSET variable: CATALOGED_ASSET = \"openmetadata-s3.default.demo.\\\"PS_20174392719_1491204439457_log.csv\\\"\" We now explain how to register a dataset in the Katalog data catalog. Begin by registering the credentials required for accessing the dataset as a kubernetes secret. Replace the values for access_key and secret_key with the values from the object storage service that you used and run: cat << EOF | kubectl apply -f - apiVersion : v1 kind : Secret metadata : name : paysim-csv type : Opaque stringData : access_key : \"${ACCESS_KEY}\" secret_key : \"${SECRET_KEY}\" EOF Next, register the data asset itself in the data catalog. We use port-forwarding to send asset creation requests to the Katalog connector. cat << EOF | kubectl apply -f - apiVersion: katalog.fybrik.io/v1alpha1 kind: Asset metadata: name: paysim-csv spec: secretRef: name: paysim-csv details: dataFormat: csv connection: name: s3 s3: endpoint: \"http://localstack.fybrik-notebook-sample.svc.cluster.local:4566\" bucket: \"demo\" object_key: \"PS_20174392719_1491204439457_log.csv\" metadata: name: Synthetic Financial Datasets For Fraud Detection geography: theshire tags: Purpose.finance: true columns: - name: nameOrig tags: PII.Sensitive: true - name: oldbalanceOrg tags: PII.Sensitive: true - name: newbalanceOrig tags: PII.Sensitive: true EOF Store the asset name in a CATALOGED_ASSET variable: CATALOGED_ASSET = \"fybrik-notebook-sample/paysim-csv\" If you look at the asset creation request above, you will notice that in the resourceMetadata field, we request that the asset should be tagged with the Purpose.finance tag, and that three of its columns should be tagged with the PII.Sensitive tag. Those tags will be referenced below in the access policy rules. Tags are important because they are used to determine whether an application would be allowed to access a dataset, and if so, which transformations should be applied to it. The asset is now registered in the catalog. Notice the resourceMetadata field above. It specifies the dataset geography and tags. These attributes can later be used in policies. For example, in the json above, the geography is set to theshire . You need make sure that it is same as the region of your fybrik control plane. You can get this information using the following command: kubectl get configmap cluster-metadata -n fybrik-system -o 'jsonpath={.data.Region}' Quick Start installs a fybrik control plane with the region theshire by default. If you change it or the geography in the json above, a copy module will be required by the policies, but we do not install any copy module in the Quick Start .","title":"Register the dataset in a data catalog"},{"location":"samples/notebook-read/#define-data-access-policies","text":"Acting as the data steward, define an OpenPolicyAgent policy to redact the columns tagged as PII.Sensitive for datasets tagged with Purpose.finance . Below is the policy (written in Rego language): package dataapi.authz # If the conditions between the curly braces are true then Fybrik will get an object containing information about # \"RedactAction\". rule[{\"action\": {\"name\":\"RedactAction\", \"columns\": column_names}, \"policy\": description}] { description := \"Redact columns tagged as PII.Sensitive in datasets tagged with Purpose.finance = true\" # this condition is true if it is a read operation input.action.actionType == \"read\" # this condition is true if the asset has \"Purpose.finance\" tag input.resource.metadata.tags[\"Purpose.finance\"] # this statement assigns to column_names variable all the columns that contain \"PII.Sensitive\" tag column_names := [input.resource.metadata.columns[i].name | input.resource.metadata.columns[i].tags[\"PII.Sensitive\"]] # this condition is true if column_names is not empty # we need this check to apply the RedactAction action only in cases where sensitive data exists. count(column_names) > 0 } In this sample only the policy above is applied. Copy the policy to a file named sample-policy.rego and then run: kubectl -n fybrik-system create configmap sample-policy --from-file = sample-policy.rego kubectl -n fybrik-system label configmap sample-policy openpolicyagent.org/policy = rego while [[ $( kubectl get cm sample-policy -n fybrik-system -o 'jsonpath={.metadata.annotations.openpolicyagent\\.org/policy-status}' ) ! = '{\"status\":\"ok\"}' ]] ; do echo \"waiting for policy to be applied\" && sleep 5 ; done You can similarly apply a directory holding multiple rego files.","title":"Define data access policies"},{"location":"samples/notebook-read/#deploy-a-jupyter-notebook","text":"In this sample a Jupyter notebook is used as the user workload and its business logic requires reading the asset that we registered (e.g., for creating a fraud detection model). Deploy a notebook to your cluster: Deploy JupyterLab: kubectl create deployment my-notebook --image = jupyter/base-notebook --port = 8888 -- start.sh jupyter lab --LabApp.token = '' kubectl set env deployment my-notebook JUPYTER_ENABLE_LAB = yes kubectl label deployment my-notebook app.kubernetes.io/name = my-notebook kubectl wait --for = condition = available --timeout = 120s deployment/my-notebook kubectl expose deployment my-notebook --port = 80 --target-port = 8888 Create a port-forward to communicate with JupyterLab: kubectl port-forward svc/my-notebook 8080 :80 & Open your browser and go to http://localhost:8080/ . Create a new notebook in the server","title":"Deploy a Jupyter notebook"},{"location":"samples/notebook-read/#create-a-fybrikapplication-resource-for-the-notebook","text":"Create a FybrikApplication resource to register the notebook workload to the control plane of Fybrik. The value you place in the dataSetID field is your asset ID, as explained above. If you registered your dataset through the data catalog connector, enter the assetID which was returned to you by the connector, e.g. \"openmetadata-s3.default.demo.\\\"PS_20174392719_1491204439457_log.csv\\\"\" . cat <<EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1beta1 kind : FybrikApplication metadata : name : my-notebook labels : app : my-notebook spec : selector : workloadSelector : matchLabels : app : my-notebook appInfo : intent : Fraud Detection data : - dataSetID : ${CATALOGED_ASSET} requirements : interface : protocol : fybrik-arrow-flight EOF Notice that: The selector field matches the labels of our Jupyter notebook workload. The data field includes a dataSetID that matches the asset identifier in the catalog. The protocol indicates that the developer wants to consume the data using Apache Arrow Flight. For some protocols a dataformat can be specified as well (e.g., s3 protocol and parquet format). Run the following command to wait until the FybrikApplication is ready and set the CATALOGED_ASSET_MODIFIED environment variable: while [[ $( kubectl get fybrikapplication my-notebook -o 'jsonpath={.status.ready}' ) ! = \"true\" ]] ; do echo \"waiting for FybrikApplication\" && sleep 5 ; done CATALOGED_ASSET_MODIFIED = $( echo $CATALOGED_ASSET | sed 's/\\./\\\\\\./g' ) while [[ $( kubectl get fybrikapplication my-notebook -o \"jsonpath={.status.assetStates. ${ CATALOGED_ASSET_MODIFIED } .conditions[?(@.type == 'Ready')].status}\" ) ! = \"True\" ]] ; do echo \"waiting for ${ CATALOGED_ASSET } asset\" && sleep 5 ; done","title":"Create a FybrikApplication resource for the notebook"},{"location":"samples/notebook-read/#read-the-dataset-from-the-notebook","text":"In your terminal , run the following command to print the endpoint to use for reading the data. It fetches the code from the FybrikApplication resource: ENDPOINT_SCHEME = $( kubectl get fybrikapplication my-notebook -o \"jsonpath={.status.assetStates. ${ CATALOGED_ASSET_MODIFIED } .endpoint.fybrik-arrow-flight.scheme}\" ) ENDPOINT_HOSTNAME = $( kubectl get fybrikapplication my-notebook -o \"jsonpath={.status.assetStates. ${ CATALOGED_ASSET_MODIFIED } .endpoint.fybrik-arrow-flight.hostname}\" ) ENDPOINT_PORT = $( kubectl get fybrikapplication my-notebook -o \"jsonpath={.status.assetStates. ${ CATALOGED_ASSET_MODIFIED } .endpoint.fybrik-arrow-flight.port}\" ) printf \"\\n ${ ENDPOINT_SCHEME } :// ${ ENDPOINT_HOSTNAME } : ${ ENDPOINT_PORT } \\n\\n\" The next steps use the endpoint to read the data in a python notebook Insert a new notebook cell to install pandas and pyarrow packages: % pip install pandas pyarrow == 7.0 .* Insert a new notebook cell to read the data. You need to replace both the ENDPOINT and the CATALOGED_ASSET values, which were obtained in previous steps: import json import pyarrow.flight as fl import pandas as pd # Create a Flight client client = fl.connect ( '<ENDPOINT>' ) # Prepare the request request = { \"asset\" : '<CATALOGED_ASSET>' , # To request specific columns add to the request a \"columns\" key with a list of column names # \"columns\": [...] } # Send request and fetch result as a pandas DataFrame info = client.get_flight_info ( fl.FlightDescriptor.for_command ( json.dumps ( request ))) reader: fl.FlightStreamReader = client.do_get ( info.endpoints [ 0 ] .ticket ) df: pd.DataFrame = reader.read_pandas () Insert a new notebook cell with the following command to visualize the result: df Execute all notebook cells and notice that some of the columns appear redacted. Created by NTNU and shared under the CC BY-SA 4.0 license. \u21a9","title":"Read the dataset from the notebook"},{"location":"samples/notebook-write/","text":"Notebook sample for the write flow This sample shows three scenarios: how fybrik prevents writing a new asset due to governance restrictions. how to write data generated by the workload to an object store. how to read data from a dataset stored in an object store. In this sample you play multiple roles: As a data governance officer you setup data governance policies. As a data user you specify your data usage requirements and use a notebook to write and read the data. Create an account in object storage Create an account in object storage of your choice such as AWS S3, IBM Cloud Object Storage or Ceph. Make a note of the service endpoint and access credentials. You will need them later. Setup localstack For experimentation you can install localstack to your cluster instead of using a cloud service. Define variables for access key and secret key export ACCESS_KEY = \"myaccesskey\" export SECRET_KEY = \"mysecretkey\" Install localstack to the currently active namespace and wait for it to be ready: Kubernetes OpenShift helm repo add localstack-charts https://localstack.github.io/helm-charts helm install localstack localstack-charts/localstack \\ --version 0.4.3 \\ --set image.tag=\"1.2.0\" \\ --set startServices=\"s3\" \\ --set service.type=ClusterIP \\ --set livenessProbe.initialDelaySeconds=25 kubectl wait --for=condition=ready --all pod -n fybrik-notebook-sample --timeout=120s helm repo add localstack-charts https://localstack.github.io/helm-charts helm install localstack localstack-charts/localstack \\ --version 0.4.3 \\ --set image.tag=\"1.2.0\" \\ --set startServices=\"s3\" \\ --set service.type=ClusterIP \\ --set livenessProbe.initialDelaySeconds=25 \\ --set persistence.enabled=true \\ --set persistence.storageClass=ibmc-file-gold-gid \\ --set persistence.accessModes[0]=ReadWriteMany kubectl wait --for=condition=ready --all pod -n fybrik-notebook-sample --timeout=120s create a port-forward to communicate with localstack server: kubectl port-forward svc/localstack 4566 :4566 & 3. Use AWS CLI to configure localstack server: export REGION = theshire aws configure set aws_access_key_id ${ ACCESS_KEY } aws configure set aws_secret_access_key ${ SECRET_KEY } aws configure set region ${ REGION } Deploy resources for write scenarios Register the credentials required for accessing the object storage. Replace the values for access_key and secret_key with the values from the object storage service that you used and run: cat << EOF | kubectl apply -f - apiVersion : v1 kind : Secret metadata : name : bucket-creds namespace : fybrik-system type : Opaque stringData : access_key : \"${ACCESS_KEY}\" secret_key : \"${SECRET_KEY}\" EOF Then, register two storage accounts: one in theshire and one in neverland . Replace the value for endpoint with value from the object storage service that you used and run: cat << EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1beta2 kind : FybrikStorageAccount metadata : name : theshire-storage-account namespace : fybrik-system spec : id : theshire-object-store type : s3 geography : theshire s3 : endpoint : \"http://localstack.fybrik-notebook-sample.svc.cluster.local:4566\" secretRef : bucket-creds EOF cat << EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1beta2 kind : FybrikStorageAccount metadata : name : neverland-storage-account namespace : fybrik-system spec : id : neverland-object-store geography : neverland type : s3 s3 : endpoint : \"http://localstack.fybrik-notebook-sample.svc.cluster.local:4566\" secretRef : bucket-creds EOF Note that for evaluation purposes the same object store is used for different regions in the storage accounts. Scenario one: write is forbidden due to governance restrictions Define data governance policies for write Define an OpenPolicyAgent policy to forbid the writing of personal data to regions neverland and theshire in datasets tagged with Purpose.finance . This policy prevents the writing as the deployed fybrik storage account resources applied are in neverland and theshire . Below is the policy (written in Rego language): package dataapi.authz # If the conditions between the curly braces are true then Fybrik will get an object for the \"Deny\" action. rule[{\"action\": {\"name\":\"Deny\"}, \"policy\": description}] { description := \"Forbid writing sensitive data to theshire object-stores in datasets tagged with `finance`\" # this condition is true if it is a write operation input.action.actionType == \"write\" # this condition is true if the asset has \"Purpose.finance\" tag input.resource.metadata.tags[\"Purpose.finance\"] # this condition is true if write destination is theshire object-stores input.action.destination == \"theshire\" # this statement is true if one of the columns has \"PersonalData.Personal\" tag input.resource.metadata.columns[i].tags[\"PersonalData.Personal\"] } # If the conditions between the curly braces are true then Fybrik will get an object for the \"Deny\" action. rule[{\"action\": {\"name\":\"Deny\"}, \"policy\": description}] { description := \"Forbid writing sensitive data to neverland object-stores in datasets tagged with `finance`\" # this condition is true if it is a write operation input.action.actionType == \"write\" # this condition is true if the asset has \"Purpose.finance\" tag input.resource.metadata.tags[\"Purpose.finance\"] # this condition is true if write destination is neverland object-stores input.action.destination == \"neverland\" # this statement is true if one of the columns has \"PersonalData.Personal\" tag input.resource.metadata.columns[i].tags[\"PersonalData.Personal\"] } For more details on OPA policies please refer to Using OPA for Data Governance task. Copy the policy to a file named sample-policy-write.rego and then run: kubectl -n fybrik-system create configmap sample-policy-write --from-file = sample-policy-write.rego kubectl -n fybrik-system label configmap sample-policy-write openpolicyagent.org/policy = rego while [[ $( kubectl get cm sample-policy-write -n fybrik-system -o 'jsonpath={.metadata.annotations.openpolicyagent\\.org/policy-status}' ) ! = '{\"status\":\"ok\"}' ]] ; do echo \"waiting for policy to be applied\" && sleep 5 ; done Create a FybrikApplication resource to write the new asset Create a FybrikApplication resource to register the notebook workload to the control plane of Fybrik: cat <<EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1beta1 kind : FybrikApplication metadata : name : my-notebook-write namespace : fybrik-notebook-sample labels : app : my-notebook-write spec : selector : clusterName : thegreendragon workloadSelector : matchLabels : app : my-notebook-write appInfo : intent : Fraud Detection data : - dataSetID : 'new-data' flow : write requirements : flowParams : isNewDataSet : true catalog : fybrik-notebook-sample metadata : tags : Purpose.finance : true columns : - name : nameOrig tags : PII.Sensitive : true - name : oldbalanceOrg tags : PersonalData.Personal : true interface : protocol : fybrik-arrow-flight EOF Notice that: The selector field matches the labels of our Jupyter notebook workload. The data field includes a dataSetID that matches the asset identifier in the catalog. The protocol indicates that the developer wants to consume the data using Apache Arrow Flight. For some protocols a dataformat can be specified as well (e.g., s3 protocol and parquet format). The isNewDataSet field indicates is a new asset. The catalog field holds the catalog id. It will be used by fybrik to register the new asset in the catalog. metadata field specifies the dataset tags. These attributes can later be used in policies. Run the following command to wait until the FybrikApplication status is updated: while [[ $( kubectl get fybrikapplication my-notebook-write -o 'jsonpath={.status.ready}' ) ! = \"true\" ]] ; do echo \"waiting for FybrikApplication\" && sleep 5 ; done while [[ $( kubectl get fybrikapplication my-notebook-write -n fybrik-notebook-sample -o 'jsonpath={.status.assetStates.new-data.conditions[?(@.type == \"Deny\")].status}' ) ! = \"True\" ]] ; do echo \"waiting for my-notebook-write asset\" && sleep 5 ; done We expect the asset's status in FybrikApplication.status to be denied due to the policy defined above. Next, a new policy will be applied which will allow the writing to theshire object store. Cleanup scenario one Before proceeding to scenario two the OPA policy and fybrikapplications should be deleted: kubectl delete cm sample-policy-write -n fybrik-system kubectl delete fybrikapplications.app.fybrik.io my-notebook-write -n fybrik-notebook-sample Notice that FybrikStorageAccount resources are still applied after the cleanup. Scenario two: write new data To write the new data a new policy should be defined. Define data access policies for writing the data Define an OpenPolicyAgent policy to return the list of column names tagged as PersonalData.Personal , whose destination is not neverland, when the actionType is write. The columns are passed to the FybrikModule together with RedactAction upon deployment of the module by Fybrik. Below is the policy (written in Rego language): package dataapi.authz rule[{\"action\": {\"name\":\"RedactAction\",\"columns\": column_names}, \"policy\": description}] { description := \"Redact written columns tagged as PersonalData.Personal in datasets tagged with Purpose.finance = true. The data should not be stored in `neverland` storage account\" input.action.actionType == \"write\" input.resource.metadata.tags[\"Purpose.finance\"] input.action.destination != \"neverland\" column_names := [input.resource.metadata.columns[i].name | input.resource.metadata.columns[i].tags[\"PersonalData.Personal\"]] } Copy the policy to a file named sample-policy-write.rego and then run: kubectl -n fybrik-system create configmap sample-policy-write --from-file = sample-policy-write.rego kubectl -n fybrik-system label configmap sample-policy-write openpolicyagent.org/policy = rego while [[ $( kubectl get cm sample-policy-write -n fybrik-system -o 'jsonpath={.metadata.annotations.openpolicyagent\\.org/policy-status}' ) ! = '{\"status\":\"ok\"}' ]] ; do echo \"waiting for policy to be applied\" && sleep 5 ; done Deploy a Jupyter notebook In this sample a Jupyter notebook is used as the user workload and its business logic requires writing the new asset. Deploy a notebook to your cluster: execute the instructions from Deploy a Jupyter notebook section in the notebook sample for the read flow to deploy a Jupyter notebook. Create a FybrikApplication resource associated with the notebook Re-apply FybrikApplication as defined in scenario 1. Run the following command to wait until the FybrikApplication status is ready: while [[ $( kubectl get fybrikapplication my-notebook-write -o 'jsonpath={.status.ready}' ) ! = \"true\" ]] ; do echo \"waiting for FybrikApplication\" && sleep 5 ; done while [[ $( kubectl get fybrikapplication my-notebook-write -o 'jsonpath={.status.assetStates.new-data.conditions[?(@.type == \"Ready\")].status}' ) ! = \"True\" ]] ; do echo \"waiting for new-data asset\" && sleep 5 ; done Although the dataset has not yet been written to the object storage, a data asset has already been created in the data catalog. We will need the name of the cataloged asset in Scenario 3 , where we will read the contents of the dataset. Obtaining the name of the asset depends on the data catalog with which Fybrik is configured to work. With OpenMetadata With Katalog Run the following command to extract the new cataloged asset id from fybrikapplication status. This asset id will be used in the third scenario when we try to read the new asset. CATALOGED_ASSET = $( kubectl get fybrikapplication my-notebook-write -o 'jsonpath={.status.assetStates.new-data.catalogedAsset}' ) CATALOGED_ASSET_MODIFIED = $( echo $CATALOGED_ASSET | sed 's/\\./\\\\\\./g' ) BUCKET = $( kubectl get fybrikapplication my-notebook-write -o 'jsonpath={.status.provisionedStorage.new-data.details.connection.s3.bucket}' ) Run the following command to extract the new cataloged asset id from fybrikapplication status. This asset id will be used in the third scenario when we try to read the new asset. CATALOGED_ASSET = $( kubectl get fybrikapplication my-notebook-write -o 'jsonpath={.status.assetStates.new-data.catalogedAsset}' ) CATALOGED_ASSET = fybrik-notebook-sample/ ${ CATALOGED_ASSET } CATALOGED_ASSET_MODIFIED = ${ CATALOGED_ASSET } BUCKET = $( kubectl get fybrikapplication my-notebook-write -o 'jsonpath={.status.provisionedStorage.new-data.details.connection.s3.bucket}' ) Write the data from the notebook This sample uses the Synthetic Financial Datasets For Fraud Detection dataset 1 as the data that the notebook needs to write. Download and extract the file to your machine. You should now see a file named PS_20174392719_1491204439457_log.csv . Alternatively, use a sample of 100 lines of the same dataset by downloading PS_20174392719_1491204439457_log.csv from GitHub. To reference PS_20174392719_1491204439457_log.csv from Jupyter notebook cells as shown later in this section do the following: Jupyter notebook has an Upload Files button that can be used to upload PS_20174392719_1491204439457_log.csv to the notebook from the local machine. When referencing PS_20174392719_1491204439457_log.csv in the notebook cell the following should be used: file_path = \"PS_20174392719_1491204439457_log.csv\" Alternatively, in your terminal , run the following commands to copy PS_20174392719_1491204439457_log.csv file from your local machine into /tmp directory in the Jupyter notebook pod: export FILEPATH = \"/path/to/PS_20174392719_1491204439457_log.csv\" export NOTEBOOK_POD_NAME = $( kubectl get pods | grep notebook | awk '{print $1}' ) kubectl cp $FILEPATH $NOTEBOOK_POD_NAME :/tmp In that case, when referencing PS_20174392719_1491204439457_log.csv in the notebook cell, /tmp/ directory should be specified, for example: file_path = \"/tmp/PS_20174392719_1491204439457_log.csv\" In your terminal , run the following command to print the endpoint to use for reading the data. It fetches the code from the FybrikApplication resource: ENDPOINT_SCHEME = $( kubectl get fybrikapplication my-notebook-write -o jsonpath ={ .status.assetStates.new-data.endpoint.fybrik-arrow-flight.scheme } ) ENDPOINT_HOSTNAME = $( kubectl get fybrikapplication my-notebook-write -o jsonpath ={ .status.assetStates.new-data.endpoint.fybrik-arrow-flight.hostname } ) ENDPOINT_PORT = $( kubectl get fybrikapplication my-notebook-write -o jsonpath ={ .status.assetStates.new-data.endpoint.fybrik-arrow-flight.port } ) printf \"\\n ${ ENDPOINT_SCHEME } :// ${ ENDPOINT_HOSTNAME } : ${ ENDPOINT_PORT } \\n\\n\" The next steps use the endpoint to write the data in a python notebook Insert a new notebook cell to install needed packages: % pip install pyarrow == 7.0 .* Insert a new notebook cell to write data using the endpoint value extracted from the FybrikApplication in the previous step: import pyarrow.flight as fl import json from pyarrow import csv # Create a Flight client client = fl.connect('<ENDPOINT>') # Prepare the request request = { \"asset\": \"new-data\", # To request specific columns add to the request a \"columns\" key with a list of column names # \"columns\": [...] } # write the new dataset file_path = \"/path/to/PS_20174392719_1491204439457_log.csv\" my_table = csv.read_csv(file_path) writer, _ = client.do_put(fl.FlightDescriptor.for_command(json.dumps(request)), my_table.schema) # Note that we do not indicate the data store nor allocate a bucket in which # to write the dataset. This is all done by Fybrik. writer.write_table(my_table) writer.close() View new asset through OpenMetadata UI If Fybrik is configured to work with the OpenMetadata data catalog, then the newly-created asset is registered in OpenMetadata and can be viewed through the OpenMetadata UI. A tutorial on working with the OpenMetadata UI can be found here . It begins with an explanation how to connect to the UI and login. Once you are logged in, choose Tables on the menu on the left and you will see all the registered assets. Cleanup scenario two kubectl delete cm sample-policy-write -n fybrik-system kubectl delete fybrikapplications.app.fybrik.io my-notebook-write -n fybrik-notebook-sample Scenario 3: Read the newly written data Create a FybrikApplication resource to read the data for the notebook Create a FybrikApplication resource to register the notebook workload to the control plane of Fybrik: cat <<EOF | kubectl apply -f - apiVersion: app.fybrik.io/v1beta1 kind: FybrikApplication metadata: name: my-notebook-read namespace: fybrik-notebook-sample labels: app: my-notebook-read spec: selector: clusterName: thegreendragon workloadSelector: matchLabels: app: my-notebook-read appInfo: intent: Fraud Detection data: - dataSetID: ${CATALOGED_ASSET} flow: read requirements: interface: protocol: fybrik-arrow-flight EOF Run the following command to wait until the FybrikApplication is ready: while [[ $( kubectl get fybrikapplication my-notebook-read -o 'jsonpath={.status.ready}' ) ! = \"true\" ]] ; do echo \"waiting for FybrikApplication\" && sleep 5 ; done while [[ $( kubectl get fybrikapplication my-notebook-read -o \"jsonpath={.status.assetStates. ${ CATALOGED_ASSET_MODIFIED } .conditions[?(@.type == 'Ready')].status}\" ) ! = \"True\" ]] ; do echo \"waiting for ${ CATALOGED_ASSET } asset\" && sleep 5 ; done Read the dataset from the notebook In your terminal , run the following command to print the endpoint to use for reading the data. It fetches the code from the FybrikApplication resource: ENDPOINT_SCHEME = $( kubectl get fybrikapplication my-notebook-read -o jsonpath ={ .status.assetStates. ${ CATALOGED_ASSET_MODIFIED } .endpoint.fybrik-arrow-flight.scheme } ) ENDPOINT_HOSTNAME = $( kubectl get fybrikapplication my-notebook-read -o jsonpath ={ .status.assetStates. ${ CATALOGED_ASSET_MODIFIED } .endpoint.fybrik-arrow-flight.hostname } ) ENDPOINT_PORT = $( kubectl get fybrikapplication my-notebook-read -o jsonpath ={ .status.assetStates. ${ CATALOGED_ASSET_MODIFIED } .endpoint.fybrik-arrow-flight.port } ) printf \"\\n ${ ENDPOINT_SCHEME } :// ${ ENDPOINT_HOSTNAME } : ${ ENDPOINT_PORT } \\n\\n\" The next steps use the endpoint to read the data in a python notebook. Insert a new notebook cell to install pandas and pyarrow packages: % pip install pandas pyarrow == 7.0 .* Insert a new notebook cell to read the data. Notice : ENDPOINT and CATALOGED_ASSET should be replaced with the values extracted from the FybrikApplication as described in previous steps. import json import pyarrow.flight as fl import pandas as pd # Create a Flight client client = fl.connect ( '<ENDPOINT>' ) # Prepare the request request = { \"asset\" : \"<CATALOGED_ASSET>\" , # To request specific columns add to the request a \"columns\" key with a list of column names # \"columns\": [...] } # show all columns pd.set_option ( 'display.max_columns' , None ) # Send request and fetch result as a pandas DataFrame info = client.get_flight_info ( fl.FlightDescriptor.for_command ( json.dumps ( request ))) reader: fl.FlightStreamReader = client.do_get ( info.endpoints [ 0 ] .ticket ) df_read: pd.DataFrame = reader.read_pandas () Insert a new notebook cell with the following command to visualize the result: df_read Execute all notebook cells and notice that data in the oldbalanceOrg column was redacted. Cleanup You can use the AWS CLI to remove the bucket and objects created in this sample. To list all the created objects, run: aws --endpoint-url = http://localhost:4566 s3api --bucket = ${ BUCKET } list-objects The output should look something like: { \"Contents\" : [ { \"Key\" : \"new-data22fb16f0c0/\" , \"LastModified\" : \"2023-03-02T10:02:26+00:00\" , \"ETag\" : \"\\\"d41d8cd98f00b204e9800998ecf8427e\\\"\" , \"Size\" : 0 , \"StorageClass\" : \"STANDARD\" , \"Owner\" : { \"DisplayName\" : \"webfile\" , \"ID\" : \"75aa57f09aa0c8caeab4f8c24e99d10f8e7faeebf76c078efc7c6caea54ba06a\" } } , { \"Key\" : \"new-data22fb16f0c0/part-2023-03-02-10-02-19-979068-0.parquet\" , \"LastModified\" : \"2023-03-02T10:02:26+00:00\" , \"ETag\" : \"\\\"a91aefdb4bf09a1a94254a9c8b6ba473-1\\\"\" , \"Size\" : 8396 , \"StorageClass\" : \"STANDARD\" , \"Owner\" : { \"DisplayName\" : \"webfile\" , \"ID\" : \"75aa57f09aa0c8caeab4f8c24e99d10f8e7faeebf76c078efc7c6caea54ba06a\" } } ] } Given the object keys returned by the previous command, run: aws --endpoint-url = http://localhost:4566 s3api --bucket = ${ BUCKET } delete-objects --delete = '{\"Objects\": [{\"Key\": \"new-data22fb16f0c0/\"}, {\"Key\": \"new-data22fb16f0c0/part-2023-03-02-10-02-19-979068-0.parquet\"}]}' Be sure to replace the keys in the previous command with those returned by the AWS list-objects command above. Finally, remove the bucket by running: aws --endpoint-url = http://localhost:4566 s3api --bucket = ${ BUCKET } delete-bucket Removing the dataset object does not remove the corresponding entry from OpenMetadata. To do so, go to the OpenMetadata UI through your browser. Choose your data asset table. At the top right, press the three vertical dots and choose Delete . Type DELETE into the form to confirm deletion. Created by NTNU and shared under the CC BY-SA 4.0 license. \u21a9","title":"Notebook sample for the write flow"},{"location":"samples/notebook-write/#notebook-sample-for-the-write-flow","text":"This sample shows three scenarios: how fybrik prevents writing a new asset due to governance restrictions. how to write data generated by the workload to an object store. how to read data from a dataset stored in an object store. In this sample you play multiple roles: As a data governance officer you setup data governance policies. As a data user you specify your data usage requirements and use a notebook to write and read the data.","title":"Notebook sample for the write flow"},{"location":"samples/notebook-write/#create-an-account-in-object-storage","text":"Create an account in object storage of your choice such as AWS S3, IBM Cloud Object Storage or Ceph. Make a note of the service endpoint and access credentials. You will need them later. Setup localstack For experimentation you can install localstack to your cluster instead of using a cloud service. Define variables for access key and secret key export ACCESS_KEY = \"myaccesskey\" export SECRET_KEY = \"mysecretkey\" Install localstack to the currently active namespace and wait for it to be ready: Kubernetes OpenShift helm repo add localstack-charts https://localstack.github.io/helm-charts helm install localstack localstack-charts/localstack \\ --version 0.4.3 \\ --set image.tag=\"1.2.0\" \\ --set startServices=\"s3\" \\ --set service.type=ClusterIP \\ --set livenessProbe.initialDelaySeconds=25 kubectl wait --for=condition=ready --all pod -n fybrik-notebook-sample --timeout=120s helm repo add localstack-charts https://localstack.github.io/helm-charts helm install localstack localstack-charts/localstack \\ --version 0.4.3 \\ --set image.tag=\"1.2.0\" \\ --set startServices=\"s3\" \\ --set service.type=ClusterIP \\ --set livenessProbe.initialDelaySeconds=25 \\ --set persistence.enabled=true \\ --set persistence.storageClass=ibmc-file-gold-gid \\ --set persistence.accessModes[0]=ReadWriteMany kubectl wait --for=condition=ready --all pod -n fybrik-notebook-sample --timeout=120s create a port-forward to communicate with localstack server: kubectl port-forward svc/localstack 4566 :4566 & 3. Use AWS CLI to configure localstack server: export REGION = theshire aws configure set aws_access_key_id ${ ACCESS_KEY } aws configure set aws_secret_access_key ${ SECRET_KEY } aws configure set region ${ REGION }","title":"Create an account in object storage"},{"location":"samples/notebook-write/#deploy-resources-for-write-scenarios","text":"Register the credentials required for accessing the object storage. Replace the values for access_key and secret_key with the values from the object storage service that you used and run: cat << EOF | kubectl apply -f - apiVersion : v1 kind : Secret metadata : name : bucket-creds namespace : fybrik-system type : Opaque stringData : access_key : \"${ACCESS_KEY}\" secret_key : \"${SECRET_KEY}\" EOF Then, register two storage accounts: one in theshire and one in neverland . Replace the value for endpoint with value from the object storage service that you used and run: cat << EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1beta2 kind : FybrikStorageAccount metadata : name : theshire-storage-account namespace : fybrik-system spec : id : theshire-object-store type : s3 geography : theshire s3 : endpoint : \"http://localstack.fybrik-notebook-sample.svc.cluster.local:4566\" secretRef : bucket-creds EOF cat << EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1beta2 kind : FybrikStorageAccount metadata : name : neverland-storage-account namespace : fybrik-system spec : id : neverland-object-store geography : neverland type : s3 s3 : endpoint : \"http://localstack.fybrik-notebook-sample.svc.cluster.local:4566\" secretRef : bucket-creds EOF Note that for evaluation purposes the same object store is used for different regions in the storage accounts.","title":"Deploy resources for write scenarios"},{"location":"samples/notebook-write/#scenario-one-write-is-forbidden-due-to-governance-restrictions","text":"","title":"Scenario one: write is forbidden due to governance restrictions"},{"location":"samples/notebook-write/#define-data-governance-policies-for-write","text":"Define an OpenPolicyAgent policy to forbid the writing of personal data to regions neverland and theshire in datasets tagged with Purpose.finance . This policy prevents the writing as the deployed fybrik storage account resources applied are in neverland and theshire . Below is the policy (written in Rego language): package dataapi.authz # If the conditions between the curly braces are true then Fybrik will get an object for the \"Deny\" action. rule[{\"action\": {\"name\":\"Deny\"}, \"policy\": description}] { description := \"Forbid writing sensitive data to theshire object-stores in datasets tagged with `finance`\" # this condition is true if it is a write operation input.action.actionType == \"write\" # this condition is true if the asset has \"Purpose.finance\" tag input.resource.metadata.tags[\"Purpose.finance\"] # this condition is true if write destination is theshire object-stores input.action.destination == \"theshire\" # this statement is true if one of the columns has \"PersonalData.Personal\" tag input.resource.metadata.columns[i].tags[\"PersonalData.Personal\"] } # If the conditions between the curly braces are true then Fybrik will get an object for the \"Deny\" action. rule[{\"action\": {\"name\":\"Deny\"}, \"policy\": description}] { description := \"Forbid writing sensitive data to neverland object-stores in datasets tagged with `finance`\" # this condition is true if it is a write operation input.action.actionType == \"write\" # this condition is true if the asset has \"Purpose.finance\" tag input.resource.metadata.tags[\"Purpose.finance\"] # this condition is true if write destination is neverland object-stores input.action.destination == \"neverland\" # this statement is true if one of the columns has \"PersonalData.Personal\" tag input.resource.metadata.columns[i].tags[\"PersonalData.Personal\"] } For more details on OPA policies please refer to Using OPA for Data Governance task. Copy the policy to a file named sample-policy-write.rego and then run: kubectl -n fybrik-system create configmap sample-policy-write --from-file = sample-policy-write.rego kubectl -n fybrik-system label configmap sample-policy-write openpolicyagent.org/policy = rego while [[ $( kubectl get cm sample-policy-write -n fybrik-system -o 'jsonpath={.metadata.annotations.openpolicyagent\\.org/policy-status}' ) ! = '{\"status\":\"ok\"}' ]] ; do echo \"waiting for policy to be applied\" && sleep 5 ; done","title":"Define data governance policies for write"},{"location":"samples/notebook-write/#create-a-fybrikapplication-resource-to-write-the-new-asset","text":"Create a FybrikApplication resource to register the notebook workload to the control plane of Fybrik: cat <<EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1beta1 kind : FybrikApplication metadata : name : my-notebook-write namespace : fybrik-notebook-sample labels : app : my-notebook-write spec : selector : clusterName : thegreendragon workloadSelector : matchLabels : app : my-notebook-write appInfo : intent : Fraud Detection data : - dataSetID : 'new-data' flow : write requirements : flowParams : isNewDataSet : true catalog : fybrik-notebook-sample metadata : tags : Purpose.finance : true columns : - name : nameOrig tags : PII.Sensitive : true - name : oldbalanceOrg tags : PersonalData.Personal : true interface : protocol : fybrik-arrow-flight EOF Notice that: The selector field matches the labels of our Jupyter notebook workload. The data field includes a dataSetID that matches the asset identifier in the catalog. The protocol indicates that the developer wants to consume the data using Apache Arrow Flight. For some protocols a dataformat can be specified as well (e.g., s3 protocol and parquet format). The isNewDataSet field indicates is a new asset. The catalog field holds the catalog id. It will be used by fybrik to register the new asset in the catalog. metadata field specifies the dataset tags. These attributes can later be used in policies. Run the following command to wait until the FybrikApplication status is updated: while [[ $( kubectl get fybrikapplication my-notebook-write -o 'jsonpath={.status.ready}' ) ! = \"true\" ]] ; do echo \"waiting for FybrikApplication\" && sleep 5 ; done while [[ $( kubectl get fybrikapplication my-notebook-write -n fybrik-notebook-sample -o 'jsonpath={.status.assetStates.new-data.conditions[?(@.type == \"Deny\")].status}' ) ! = \"True\" ]] ; do echo \"waiting for my-notebook-write asset\" && sleep 5 ; done We expect the asset's status in FybrikApplication.status to be denied due to the policy defined above. Next, a new policy will be applied which will allow the writing to theshire object store.","title":"Create a FybrikApplication resource to write the new asset"},{"location":"samples/notebook-write/#cleanup-scenario-one","text":"Before proceeding to scenario two the OPA policy and fybrikapplications should be deleted: kubectl delete cm sample-policy-write -n fybrik-system kubectl delete fybrikapplications.app.fybrik.io my-notebook-write -n fybrik-notebook-sample Notice that FybrikStorageAccount resources are still applied after the cleanup.","title":"Cleanup scenario one"},{"location":"samples/notebook-write/#scenario-two-write-new-data","text":"To write the new data a new policy should be defined.","title":"Scenario two: write new data"},{"location":"samples/notebook-write/#define-data-access-policies-for-writing-the-data","text":"Define an OpenPolicyAgent policy to return the list of column names tagged as PersonalData.Personal , whose destination is not neverland, when the actionType is write. The columns are passed to the FybrikModule together with RedactAction upon deployment of the module by Fybrik. Below is the policy (written in Rego language): package dataapi.authz rule[{\"action\": {\"name\":\"RedactAction\",\"columns\": column_names}, \"policy\": description}] { description := \"Redact written columns tagged as PersonalData.Personal in datasets tagged with Purpose.finance = true. The data should not be stored in `neverland` storage account\" input.action.actionType == \"write\" input.resource.metadata.tags[\"Purpose.finance\"] input.action.destination != \"neverland\" column_names := [input.resource.metadata.columns[i].name | input.resource.metadata.columns[i].tags[\"PersonalData.Personal\"]] } Copy the policy to a file named sample-policy-write.rego and then run: kubectl -n fybrik-system create configmap sample-policy-write --from-file = sample-policy-write.rego kubectl -n fybrik-system label configmap sample-policy-write openpolicyagent.org/policy = rego while [[ $( kubectl get cm sample-policy-write -n fybrik-system -o 'jsonpath={.metadata.annotations.openpolicyagent\\.org/policy-status}' ) ! = '{\"status\":\"ok\"}' ]] ; do echo \"waiting for policy to be applied\" && sleep 5 ; done","title":"Define data access policies for writing the data"},{"location":"samples/notebook-write/#deploy-a-jupyter-notebook","text":"In this sample a Jupyter notebook is used as the user workload and its business logic requires writing the new asset. Deploy a notebook to your cluster: execute the instructions from Deploy a Jupyter notebook section in the notebook sample for the read flow to deploy a Jupyter notebook.","title":"Deploy a Jupyter notebook"},{"location":"samples/notebook-write/#create-a-fybrikapplication-resource-associated-with-the-notebook","text":"Re-apply FybrikApplication as defined in scenario 1. Run the following command to wait until the FybrikApplication status is ready: while [[ $( kubectl get fybrikapplication my-notebook-write -o 'jsonpath={.status.ready}' ) ! = \"true\" ]] ; do echo \"waiting for FybrikApplication\" && sleep 5 ; done while [[ $( kubectl get fybrikapplication my-notebook-write -o 'jsonpath={.status.assetStates.new-data.conditions[?(@.type == \"Ready\")].status}' ) ! = \"True\" ]] ; do echo \"waiting for new-data asset\" && sleep 5 ; done Although the dataset has not yet been written to the object storage, a data asset has already been created in the data catalog. We will need the name of the cataloged asset in Scenario 3 , where we will read the contents of the dataset. Obtaining the name of the asset depends on the data catalog with which Fybrik is configured to work. With OpenMetadata With Katalog Run the following command to extract the new cataloged asset id from fybrikapplication status. This asset id will be used in the third scenario when we try to read the new asset. CATALOGED_ASSET = $( kubectl get fybrikapplication my-notebook-write -o 'jsonpath={.status.assetStates.new-data.catalogedAsset}' ) CATALOGED_ASSET_MODIFIED = $( echo $CATALOGED_ASSET | sed 's/\\./\\\\\\./g' ) BUCKET = $( kubectl get fybrikapplication my-notebook-write -o 'jsonpath={.status.provisionedStorage.new-data.details.connection.s3.bucket}' ) Run the following command to extract the new cataloged asset id from fybrikapplication status. This asset id will be used in the third scenario when we try to read the new asset. CATALOGED_ASSET = $( kubectl get fybrikapplication my-notebook-write -o 'jsonpath={.status.assetStates.new-data.catalogedAsset}' ) CATALOGED_ASSET = fybrik-notebook-sample/ ${ CATALOGED_ASSET } CATALOGED_ASSET_MODIFIED = ${ CATALOGED_ASSET } BUCKET = $( kubectl get fybrikapplication my-notebook-write -o 'jsonpath={.status.provisionedStorage.new-data.details.connection.s3.bucket}' )","title":"Create a FybrikApplication resource associated with the notebook"},{"location":"samples/notebook-write/#write-the-data-from-the-notebook","text":"This sample uses the Synthetic Financial Datasets For Fraud Detection dataset 1 as the data that the notebook needs to write. Download and extract the file to your machine. You should now see a file named PS_20174392719_1491204439457_log.csv . Alternatively, use a sample of 100 lines of the same dataset by downloading PS_20174392719_1491204439457_log.csv from GitHub. To reference PS_20174392719_1491204439457_log.csv from Jupyter notebook cells as shown later in this section do the following: Jupyter notebook has an Upload Files button that can be used to upload PS_20174392719_1491204439457_log.csv to the notebook from the local machine. When referencing PS_20174392719_1491204439457_log.csv in the notebook cell the following should be used: file_path = \"PS_20174392719_1491204439457_log.csv\" Alternatively, in your terminal , run the following commands to copy PS_20174392719_1491204439457_log.csv file from your local machine into /tmp directory in the Jupyter notebook pod: export FILEPATH = \"/path/to/PS_20174392719_1491204439457_log.csv\" export NOTEBOOK_POD_NAME = $( kubectl get pods | grep notebook | awk '{print $1}' ) kubectl cp $FILEPATH $NOTEBOOK_POD_NAME :/tmp In that case, when referencing PS_20174392719_1491204439457_log.csv in the notebook cell, /tmp/ directory should be specified, for example: file_path = \"/tmp/PS_20174392719_1491204439457_log.csv\" In your terminal , run the following command to print the endpoint to use for reading the data. It fetches the code from the FybrikApplication resource: ENDPOINT_SCHEME = $( kubectl get fybrikapplication my-notebook-write -o jsonpath ={ .status.assetStates.new-data.endpoint.fybrik-arrow-flight.scheme } ) ENDPOINT_HOSTNAME = $( kubectl get fybrikapplication my-notebook-write -o jsonpath ={ .status.assetStates.new-data.endpoint.fybrik-arrow-flight.hostname } ) ENDPOINT_PORT = $( kubectl get fybrikapplication my-notebook-write -o jsonpath ={ .status.assetStates.new-data.endpoint.fybrik-arrow-flight.port } ) printf \"\\n ${ ENDPOINT_SCHEME } :// ${ ENDPOINT_HOSTNAME } : ${ ENDPOINT_PORT } \\n\\n\" The next steps use the endpoint to write the data in a python notebook Insert a new notebook cell to install needed packages: % pip install pyarrow == 7.0 .* Insert a new notebook cell to write data using the endpoint value extracted from the FybrikApplication in the previous step: import pyarrow.flight as fl import json from pyarrow import csv # Create a Flight client client = fl.connect('<ENDPOINT>') # Prepare the request request = { \"asset\": \"new-data\", # To request specific columns add to the request a \"columns\" key with a list of column names # \"columns\": [...] } # write the new dataset file_path = \"/path/to/PS_20174392719_1491204439457_log.csv\" my_table = csv.read_csv(file_path) writer, _ = client.do_put(fl.FlightDescriptor.for_command(json.dumps(request)), my_table.schema) # Note that we do not indicate the data store nor allocate a bucket in which # to write the dataset. This is all done by Fybrik. writer.write_table(my_table) writer.close()","title":"Write the data from the notebook"},{"location":"samples/notebook-write/#view-new-asset-through-openmetadata-ui","text":"If Fybrik is configured to work with the OpenMetadata data catalog, then the newly-created asset is registered in OpenMetadata and can be viewed through the OpenMetadata UI. A tutorial on working with the OpenMetadata UI can be found here . It begins with an explanation how to connect to the UI and login. Once you are logged in, choose Tables on the menu on the left and you will see all the registered assets.","title":"View new asset through OpenMetadata UI"},{"location":"samples/notebook-write/#cleanup-scenario-two","text":"kubectl delete cm sample-policy-write -n fybrik-system kubectl delete fybrikapplications.app.fybrik.io my-notebook-write -n fybrik-notebook-sample","title":"Cleanup scenario two"},{"location":"samples/notebook-write/#scenario-3-read-the-newly-written-data","text":"","title":"Scenario 3: Read the newly written data"},{"location":"samples/notebook-write/#create-a-fybrikapplication-resource-to-read-the-data-for-the-notebook","text":"Create a FybrikApplication resource to register the notebook workload to the control plane of Fybrik: cat <<EOF | kubectl apply -f - apiVersion: app.fybrik.io/v1beta1 kind: FybrikApplication metadata: name: my-notebook-read namespace: fybrik-notebook-sample labels: app: my-notebook-read spec: selector: clusterName: thegreendragon workloadSelector: matchLabels: app: my-notebook-read appInfo: intent: Fraud Detection data: - dataSetID: ${CATALOGED_ASSET} flow: read requirements: interface: protocol: fybrik-arrow-flight EOF Run the following command to wait until the FybrikApplication is ready: while [[ $( kubectl get fybrikapplication my-notebook-read -o 'jsonpath={.status.ready}' ) ! = \"true\" ]] ; do echo \"waiting for FybrikApplication\" && sleep 5 ; done while [[ $( kubectl get fybrikapplication my-notebook-read -o \"jsonpath={.status.assetStates. ${ CATALOGED_ASSET_MODIFIED } .conditions[?(@.type == 'Ready')].status}\" ) ! = \"True\" ]] ; do echo \"waiting for ${ CATALOGED_ASSET } asset\" && sleep 5 ; done","title":"Create a FybrikApplication resource to read the data for the notebook"},{"location":"samples/notebook-write/#read-the-dataset-from-the-notebook","text":"In your terminal , run the following command to print the endpoint to use for reading the data. It fetches the code from the FybrikApplication resource: ENDPOINT_SCHEME = $( kubectl get fybrikapplication my-notebook-read -o jsonpath ={ .status.assetStates. ${ CATALOGED_ASSET_MODIFIED } .endpoint.fybrik-arrow-flight.scheme } ) ENDPOINT_HOSTNAME = $( kubectl get fybrikapplication my-notebook-read -o jsonpath ={ .status.assetStates. ${ CATALOGED_ASSET_MODIFIED } .endpoint.fybrik-arrow-flight.hostname } ) ENDPOINT_PORT = $( kubectl get fybrikapplication my-notebook-read -o jsonpath ={ .status.assetStates. ${ CATALOGED_ASSET_MODIFIED } .endpoint.fybrik-arrow-flight.port } ) printf \"\\n ${ ENDPOINT_SCHEME } :// ${ ENDPOINT_HOSTNAME } : ${ ENDPOINT_PORT } \\n\\n\" The next steps use the endpoint to read the data in a python notebook. Insert a new notebook cell to install pandas and pyarrow packages: % pip install pandas pyarrow == 7.0 .* Insert a new notebook cell to read the data. Notice : ENDPOINT and CATALOGED_ASSET should be replaced with the values extracted from the FybrikApplication as described in previous steps. import json import pyarrow.flight as fl import pandas as pd # Create a Flight client client = fl.connect ( '<ENDPOINT>' ) # Prepare the request request = { \"asset\" : \"<CATALOGED_ASSET>\" , # To request specific columns add to the request a \"columns\" key with a list of column names # \"columns\": [...] } # show all columns pd.set_option ( 'display.max_columns' , None ) # Send request and fetch result as a pandas DataFrame info = client.get_flight_info ( fl.FlightDescriptor.for_command ( json.dumps ( request ))) reader: fl.FlightStreamReader = client.do_get ( info.endpoints [ 0 ] .ticket ) df_read: pd.DataFrame = reader.read_pandas () Insert a new notebook cell with the following command to visualize the result: df_read Execute all notebook cells and notice that data in the oldbalanceOrg column was redacted.","title":"Read the dataset from the notebook"},{"location":"samples/notebook-write/#cleanup","text":"You can use the AWS CLI to remove the bucket and objects created in this sample. To list all the created objects, run: aws --endpoint-url = http://localhost:4566 s3api --bucket = ${ BUCKET } list-objects The output should look something like: { \"Contents\" : [ { \"Key\" : \"new-data22fb16f0c0/\" , \"LastModified\" : \"2023-03-02T10:02:26+00:00\" , \"ETag\" : \"\\\"d41d8cd98f00b204e9800998ecf8427e\\\"\" , \"Size\" : 0 , \"StorageClass\" : \"STANDARD\" , \"Owner\" : { \"DisplayName\" : \"webfile\" , \"ID\" : \"75aa57f09aa0c8caeab4f8c24e99d10f8e7faeebf76c078efc7c6caea54ba06a\" } } , { \"Key\" : \"new-data22fb16f0c0/part-2023-03-02-10-02-19-979068-0.parquet\" , \"LastModified\" : \"2023-03-02T10:02:26+00:00\" , \"ETag\" : \"\\\"a91aefdb4bf09a1a94254a9c8b6ba473-1\\\"\" , \"Size\" : 8396 , \"StorageClass\" : \"STANDARD\" , \"Owner\" : { \"DisplayName\" : \"webfile\" , \"ID\" : \"75aa57f09aa0c8caeab4f8c24e99d10f8e7faeebf76c078efc7c6caea54ba06a\" } } ] } Given the object keys returned by the previous command, run: aws --endpoint-url = http://localhost:4566 s3api --bucket = ${ BUCKET } delete-objects --delete = '{\"Objects\": [{\"Key\": \"new-data22fb16f0c0/\"}, {\"Key\": \"new-data22fb16f0c0/part-2023-03-02-10-02-19-979068-0.parquet\"}]}' Be sure to replace the keys in the previous command with those returned by the AWS list-objects command above. Finally, remove the bucket by running: aws --endpoint-url = http://localhost:4566 s3api --bucket = ${ BUCKET } delete-bucket Removing the dataset object does not remove the corresponding entry from OpenMetadata. To do so, go to the OpenMetadata UI through your browser. Choose your data asset table. At the top right, press the three vertical dots and choose Delete . Type DELETE into the form to confirm deletion. Created by NTNU and shared under the CC BY-SA 4.0 license. \u21a9","title":"Cleanup"},{"location":"samples/pre-steps/","text":"Tools used by the actors The data owner would typically register the dataset in a proprietary or open source catalog. We use OpenMetadata . The data owner needs to provide credentials for accessing a dataset. This is usually done via the data catalog, but credentials could be stored in kubernetes secrets as an alternative. Proprietary and open source data governance systems are available either as part of a data catalog or as stand-alone systems. This sample uses the open source OpenPolicyAgent . The data governance officer writes the policies in OPA's rego language. Any editor can be used to write the FybrikApplication.yaml via which the data user expresses the data usage requirements. A jupyter notebook is the workload from which the data is consumed by the data user. A Web Browser Prepare Fybrik environment Typically, this would be done by an IT administrator. Install Fybrik using the Quick Start guide. This sample assumes the use of OpenMetadata , OpenPolicyAgent and the flight module . Create a namespace for the sample Create a new Kubernetes namespace and set it as the active namespace: kubectl create namespace fybrik-notebook-sample kubectl config set-context --current --namespace = fybrik-notebook-sample This enables easy cleanup once you're done experimenting with the sample.","title":"Before we begin"},{"location":"samples/pre-steps/#tools-used-by-the-actors","text":"The data owner would typically register the dataset in a proprietary or open source catalog. We use OpenMetadata . The data owner needs to provide credentials for accessing a dataset. This is usually done via the data catalog, but credentials could be stored in kubernetes secrets as an alternative. Proprietary and open source data governance systems are available either as part of a data catalog or as stand-alone systems. This sample uses the open source OpenPolicyAgent . The data governance officer writes the policies in OPA's rego language. Any editor can be used to write the FybrikApplication.yaml via which the data user expresses the data usage requirements. A jupyter notebook is the workload from which the data is consumed by the data user. A Web Browser","title":"Tools used by the actors"},{"location":"samples/pre-steps/#prepare-fybrik-environment","text":"Typically, this would be done by an IT administrator. Install Fybrik using the Quick Start guide. This sample assumes the use of OpenMetadata , OpenPolicyAgent and the flight module .","title":"Prepare Fybrik environment"},{"location":"samples/pre-steps/#create-a-namespace-for-the-sample","text":"Create a new Kubernetes namespace and set it as the active namespace: kubectl create namespace fybrik-notebook-sample kubectl config set-context --current --namespace = fybrik-notebook-sample This enables easy cleanup once you're done experimenting with the sample.","title":"Create a namespace for the sample"},{"location":"tasks/add-vault-plugin/","text":"Adding a new HashiCorp Vault Plugin The following steps show how to add a new Vault secret plugin for Fybrik. More information on the process can be found in this blog . Before you begin Ensure that you have the Vault v1.9.x to execute Vault CLI commands. Steps to add the plugin Login into Vault Register and enable the plugin during Vault server initialization in a specific path. An example of that can be found in helm chart values.yaml file in the project where Vault-plugin-secrets-kubernetes-reader plugin is enabled in kubernetes-secrets path: SHA256 = $( sha256sum /usr/local/libexec/vault/vault-plugin-secrets-kubernetes-reader | cut -d ' ' -f1 ) && vault plugin register -sha256 = $SHA256 secret vault-plugin-secrets-kubernetes-reader vault secrets enable -path = kubernetes-secrets vault-plugin-secrets-kubernetes-reader Add Vault policy to allow the modules to access secrets using the plugin. Following is an example of a policy which gives permission to read secrets in Vault path kubernetes-secrets : vault policy write \"allow-all-dataset-creds\" - <<EOF path \"kubernetes-secrets/*\" { capabilities = [\"read\"] } EOF Have the data catalog getAsset response contain the Vault secret path which should be used to retrieve the credentials for a given asset. When the Vault plugin is used to retrieve the credentials, the parameters to the plugin should follow the plugin usage instructions. This path will later be passed on to the modules . For example, when the credentials are stored in kubernetes secret as is done in the Katalog built-in data catalog, the Vault-plugin-secrets-kubernetes-reader plugin can be used to retrieve the credentials. In this case two parameters should be passed: paysim-csv which is the kubernetes secret name that holds the credentials and fybrik-notebook-sample is the secret namespace, both are known to the katalog when constructing the path. The credentails field in getAsset response should contain \"/v1/kubernetes-secrets/paysim-csv?namespace=fybrik-notebook-sample\" in this case. Update the modules to use the Vault related values to retrieve dataset credentias during their runtime execution. The values contain secretPath field with the plugin path as described in the previous step. The following snippet contains an example of such values. vault: # Address is Vault address address: http://vault.fybrik-system:8200 # AuthPath is the path to auth method used to login to Vault authPath: /v1/auth/kubernetes/login # Role is the Vault role used for retrieving the credentials role: module # SecretPath is the path of the secret holding the Credentials in Vault secretPath: /v1/kubernetes-secrets/paysim-csv?namespace = fybrik-notebook-sample","title":"Adding a new HashiCorp Vault Plugin"},{"location":"tasks/add-vault-plugin/#adding-a-new-hashicorp-vault-plugin","text":"The following steps show how to add a new Vault secret plugin for Fybrik. More information on the process can be found in this blog .","title":"Adding a new HashiCorp Vault Plugin"},{"location":"tasks/add-vault-plugin/#before-you-begin","text":"Ensure that you have the Vault v1.9.x to execute Vault CLI commands.","title":"Before you begin"},{"location":"tasks/add-vault-plugin/#steps-to-add-the-plugin","text":"Login into Vault Register and enable the plugin during Vault server initialization in a specific path. An example of that can be found in helm chart values.yaml file in the project where Vault-plugin-secrets-kubernetes-reader plugin is enabled in kubernetes-secrets path: SHA256 = $( sha256sum /usr/local/libexec/vault/vault-plugin-secrets-kubernetes-reader | cut -d ' ' -f1 ) && vault plugin register -sha256 = $SHA256 secret vault-plugin-secrets-kubernetes-reader vault secrets enable -path = kubernetes-secrets vault-plugin-secrets-kubernetes-reader Add Vault policy to allow the modules to access secrets using the plugin. Following is an example of a policy which gives permission to read secrets in Vault path kubernetes-secrets : vault policy write \"allow-all-dataset-creds\" - <<EOF path \"kubernetes-secrets/*\" { capabilities = [\"read\"] } EOF Have the data catalog getAsset response contain the Vault secret path which should be used to retrieve the credentials for a given asset. When the Vault plugin is used to retrieve the credentials, the parameters to the plugin should follow the plugin usage instructions. This path will later be passed on to the modules . For example, when the credentials are stored in kubernetes secret as is done in the Katalog built-in data catalog, the Vault-plugin-secrets-kubernetes-reader plugin can be used to retrieve the credentials. In this case two parameters should be passed: paysim-csv which is the kubernetes secret name that holds the credentials and fybrik-notebook-sample is the secret namespace, both are known to the katalog when constructing the path. The credentails field in getAsset response should contain \"/v1/kubernetes-secrets/paysim-csv?namespace=fybrik-notebook-sample\" in this case. Update the modules to use the Vault related values to retrieve dataset credentias during their runtime execution. The values contain secretPath field with the plugin path as described in the previous step. The following snippet contains an example of such values. vault: # Address is Vault address address: http://vault.fybrik-system:8200 # AuthPath is the path to auth method used to login to Vault authPath: /v1/auth/kubernetes/login # Role is the Vault role used for retrieving the credentials role: module # SecretPath is the path of the secret holding the Credentials in Vault secretPath: /v1/kubernetes-secrets/paysim-csv?namespace = fybrik-notebook-sample","title":"Steps to add the plugin"},{"location":"tasks/control-plane-security/","text":"Enable Control Plane Security Kubernetes NetworkPolicies , TLS/mTLS and optionally Istio can be used to protect components of the control plane. Specifically, traffic to connectors that run as part of the control plane must be secured. Follow this page to enable control plane security. Ingress traffic policy The installation of Fybrik applies a Kubernetes NetworkPolicy resource to the fybrik-system namespace. This resource ensures that ingress traffic to connectors is only allowed from workloads that run in the fybrik-system namespace and thus disallow access to connectors from other namespaces or external parties. The NetworkPolicy is always created. However, your Kubernetes cluster must have a Network Plugin with NetworkPolicy support. Otherwise, NetworkPolicy resources will have no affect. While most Kubernetes distributions include a network plugin that enfoces network policies, some like Kind do not and require you to install a separate network plugin instead. Transport Layer Security (TLS) Configure Fybrik to use TLS Fybrik can be configured to protect traffic between the manager and connectors by using TLS. In addition, mutual TLS authentication is possible too. In the TLS mode, the connectors (aka the servers) should have their certificates available to provide them to the manager (aka client) in the TLS protocol handshake process. In mutual TLS mode, both the manager and connector should have their certificates available. If private Certificate Authorities (CA) is used then its credentials should be installed too. Generating TLS certificates and keys For development and testing the TLS certificates and certificate keys can be generated using openSSL library. For more information on the process please refer to online documentation such as this useful tutorial . Cert-manager can also be used to automatically generate and renew the TLS certificate using its Certificate resource. The following is an example of a Certificate resource for the opa-connector where a tls type secret named tls-opa-connector-certs containing the certificate and certificate key is automatically created by the cert-manager. The issuerRef field points to a cert-manager resource name Issuer that holds the information about the CA that signs the certificate. apiVersion : cert-manager.io/v1 kind : Certificate metadata : name : opa-connector-cert namespace : fybrik-system spec : dnsNames : - opa-connector issuerRef : kind : Issuer name : ca-issuer secretName : tls-opa-connector-certs Adding TLS Secrets The manager/connectors certificates are kept in Kubernetes secret of type tls which are mounted to the manager/connectors pods in fybrik deployment. For each component copy its certificate into a file named tls.crt. Copy the certificate key into a file named tls.key. (the files names used in this example can be changed) Use kubectl with the tls secret type to create the secrets. kubectl -n fybrik-system create secret tls tls-opa-connector-certs \\ --cert = tls.crt \\ --key = tls.key If cert-manager is used to manage the certificates then the secret is automatically created as shown above. Using a Private CA Signed Certificate If you are using a private CA, Fybrik requires a copy of the CA certificate which is used by connector/manager to validate the connection to the manager/connectors. If no private CA certificates are provided then the system CA certificates are used, otherwise the private CA certificates replace the system CA certificates. For each component copy the CA certificate into a file and use kubectl to create or patch the tls-ca secret in the fybrik-system namespace. In the following example ca.crt file is used to create the secret: kubectl -n fybrik-system create secret generic tls-ca \\ --from-file = ca.crt = ./ca.crt Note that Fybrik expects that the keys of the CA certificates in the secret will end with .crt suffix. Here is an example of a self-signed issuer managed by cert-manager. The secret tls-ca that holds the CA certificate is created and automatically renewed by cert-manager. apiVersion : cert-manager.io/v1 kind : ClusterIssuer metadata : name : ca-issuer-self-signed spec : selfSigned : {} --- apiVersion : cert-manager.io/v1 kind : Issuer metadata : name : ca-issuer namespace : fybrik-system spec : ca : secretName : tls-ca --- apiVersion : cert-manager.io/v1 kind : Certificate metadata : name : ca-certificate namespace : fybrik-system spec : isCA : true commonName : fybrik secretName : tls-ca issuerRef : name : ca-issuer-self-signed kind : ClusterIssuer group : cert-manager.io Update Values.yaml file To use TLS the infomation about the secrets above should be inserted to the fields in values.yaml file upon Fybrik deployment using helm. Here is an example of the tls related fields in the opa-connector section that are filled based on the secrets created above: opaConnector : tls : # MinVersion contains the minimum TLS version that is acceptable. # If not provided, minimal supported TLS protocol is taken as the minimum. minVersion : v13 # Specifies whether the opa connector communication should use tls. use_tls : true # Specifies whether the opa connector communication should use mutual tls. use_mtls : true # Relavent if the connection between the manager and the connectors # uses tls. certs : # Name of kubernetes tls secret that holds opa-connector certificates. # The secret should be of `kubernetes.io/tls` type. # Relavent if tls is used. certSecretName : \"tls-opa-connector-certs\" # Name of kubernetes secret that holds the certificate authority (CA) certificates # which are used by opa-connector to validate the connection to the manager if # mtls is enabled. # The CA certificates key in the secret should have `.crt` suffix. # The provided certificates replaces the certificates in the system CA certificate store. # If the secret is not provided then the CA certificates are taken from the system # CA certificate store, for example `/etc/ssl/certs/`. cacertSecretName : \"tls-ca\" Using Istio Alternatively, if Istio is installed in the cluster then you can use automatic mutual TLS to encrypt the traffic to the connectors. Follow these steps to enable mutual TLS: Ensure that Istio 1.6 or above is installed. Enable Istio sidecar injection in the fybrik-system namespace: kubectl label namespace fybrik-system istio-injection = enabled - Create Istio PeerAuthentication resource to enable mutual TLS between containers with Istio sidecars: cat << EOF | kubectl apply -f - apiVersion: \"security.istio.io/v1beta1\" kind: \"PeerAuthentication\" metadata: name: \"premissive-mtls-in-control-plane\" namespace: fybrik-system spec: mtls: mode: PERMISSIVE EOF - Create Istio Sidecar resource to allow any egress traffic from the control plane containers: cat << EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: Sidecar metadata: name: sidecar-default namespace: fybrik-system spec: egress: - hosts: - \"*/*\" outboundTrafficPolicy: mode: ALLOW_ANY EOF - Restart the control plane pods: kubectl delete pod --all -n fybrik-system","title":"Enable Control Plane Security"},{"location":"tasks/control-plane-security/#enable-control-plane-security","text":"Kubernetes NetworkPolicies , TLS/mTLS and optionally Istio can be used to protect components of the control plane. Specifically, traffic to connectors that run as part of the control plane must be secured. Follow this page to enable control plane security.","title":"Enable Control Plane Security"},{"location":"tasks/control-plane-security/#ingress-traffic-policy","text":"The installation of Fybrik applies a Kubernetes NetworkPolicy resource to the fybrik-system namespace. This resource ensures that ingress traffic to connectors is only allowed from workloads that run in the fybrik-system namespace and thus disallow access to connectors from other namespaces or external parties. The NetworkPolicy is always created. However, your Kubernetes cluster must have a Network Plugin with NetworkPolicy support. Otherwise, NetworkPolicy resources will have no affect. While most Kubernetes distributions include a network plugin that enfoces network policies, some like Kind do not and require you to install a separate network plugin instead.","title":"Ingress traffic policy"},{"location":"tasks/control-plane-security/#transport-layer-security-tls","text":"","title":"Transport Layer Security (TLS)"},{"location":"tasks/control-plane-security/#configure-fybrik-to-use-tls","text":"Fybrik can be configured to protect traffic between the manager and connectors by using TLS. In addition, mutual TLS authentication is possible too. In the TLS mode, the connectors (aka the servers) should have their certificates available to provide them to the manager (aka client) in the TLS protocol handshake process. In mutual TLS mode, both the manager and connector should have their certificates available. If private Certificate Authorities (CA) is used then its credentials should be installed too.","title":"Configure Fybrik to use TLS"},{"location":"tasks/control-plane-security/#generating-tls-certificates-and-keys","text":"For development and testing the TLS certificates and certificate keys can be generated using openSSL library. For more information on the process please refer to online documentation such as this useful tutorial . Cert-manager can also be used to automatically generate and renew the TLS certificate using its Certificate resource. The following is an example of a Certificate resource for the opa-connector where a tls type secret named tls-opa-connector-certs containing the certificate and certificate key is automatically created by the cert-manager. The issuerRef field points to a cert-manager resource name Issuer that holds the information about the CA that signs the certificate. apiVersion : cert-manager.io/v1 kind : Certificate metadata : name : opa-connector-cert namespace : fybrik-system spec : dnsNames : - opa-connector issuerRef : kind : Issuer name : ca-issuer secretName : tls-opa-connector-certs","title":"Generating TLS certificates and keys"},{"location":"tasks/control-plane-security/#adding-tls-secrets","text":"The manager/connectors certificates are kept in Kubernetes secret of type tls which are mounted to the manager/connectors pods in fybrik deployment. For each component copy its certificate into a file named tls.crt. Copy the certificate key into a file named tls.key. (the files names used in this example can be changed) Use kubectl with the tls secret type to create the secrets. kubectl -n fybrik-system create secret tls tls-opa-connector-certs \\ --cert = tls.crt \\ --key = tls.key If cert-manager is used to manage the certificates then the secret is automatically created as shown above.","title":"Adding TLS Secrets"},{"location":"tasks/control-plane-security/#using-a-private-ca-signed-certificate","text":"If you are using a private CA, Fybrik requires a copy of the CA certificate which is used by connector/manager to validate the connection to the manager/connectors. If no private CA certificates are provided then the system CA certificates are used, otherwise the private CA certificates replace the system CA certificates. For each component copy the CA certificate into a file and use kubectl to create or patch the tls-ca secret in the fybrik-system namespace. In the following example ca.crt file is used to create the secret: kubectl -n fybrik-system create secret generic tls-ca \\ --from-file = ca.crt = ./ca.crt Note that Fybrik expects that the keys of the CA certificates in the secret will end with .crt suffix. Here is an example of a self-signed issuer managed by cert-manager. The secret tls-ca that holds the CA certificate is created and automatically renewed by cert-manager. apiVersion : cert-manager.io/v1 kind : ClusterIssuer metadata : name : ca-issuer-self-signed spec : selfSigned : {} --- apiVersion : cert-manager.io/v1 kind : Issuer metadata : name : ca-issuer namespace : fybrik-system spec : ca : secretName : tls-ca --- apiVersion : cert-manager.io/v1 kind : Certificate metadata : name : ca-certificate namespace : fybrik-system spec : isCA : true commonName : fybrik secretName : tls-ca issuerRef : name : ca-issuer-self-signed kind : ClusterIssuer group : cert-manager.io","title":"Using a Private CA Signed Certificate"},{"location":"tasks/control-plane-security/#update-valuesyaml-file","text":"To use TLS the infomation about the secrets above should be inserted to the fields in values.yaml file upon Fybrik deployment using helm. Here is an example of the tls related fields in the opa-connector section that are filled based on the secrets created above: opaConnector : tls : # MinVersion contains the minimum TLS version that is acceptable. # If not provided, minimal supported TLS protocol is taken as the minimum. minVersion : v13 # Specifies whether the opa connector communication should use tls. use_tls : true # Specifies whether the opa connector communication should use mutual tls. use_mtls : true # Relavent if the connection between the manager and the connectors # uses tls. certs : # Name of kubernetes tls secret that holds opa-connector certificates. # The secret should be of `kubernetes.io/tls` type. # Relavent if tls is used. certSecretName : \"tls-opa-connector-certs\" # Name of kubernetes secret that holds the certificate authority (CA) certificates # which are used by opa-connector to validate the connection to the manager if # mtls is enabled. # The CA certificates key in the secret should have `.crt` suffix. # The provided certificates replaces the certificates in the system CA certificate store. # If the secret is not provided then the CA certificates are taken from the system # CA certificate store, for example `/etc/ssl/certs/`. cacertSecretName : \"tls-ca\"","title":"Update Values.yaml file"},{"location":"tasks/control-plane-security/#using-istio","text":"Alternatively, if Istio is installed in the cluster then you can use automatic mutual TLS to encrypt the traffic to the connectors. Follow these steps to enable mutual TLS: Ensure that Istio 1.6 or above is installed. Enable Istio sidecar injection in the fybrik-system namespace: kubectl label namespace fybrik-system istio-injection = enabled - Create Istio PeerAuthentication resource to enable mutual TLS between containers with Istio sidecars: cat << EOF | kubectl apply -f - apiVersion: \"security.istio.io/v1beta1\" kind: \"PeerAuthentication\" metadata: name: \"premissive-mtls-in-control-plane\" namespace: fybrik-system spec: mtls: mode: PERMISSIVE EOF - Create Istio Sidecar resource to allow any egress traffic from the control plane containers: cat << EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: Sidecar metadata: name: sidecar-default namespace: fybrik-system spec: egress: - hosts: - \"*/*\" outboundTrafficPolicy: mode: ALLOW_ANY EOF - Restart the control plane pods: kubectl delete pod --all -n fybrik-system","title":"Using Istio"},{"location":"tasks/custom-taxonomy/","text":"Using a Custom Taxonomy for Resource Validation Background Fybrik acts as an orchestrator of independent components. For example, the author of the data governance policy manager, which provides the governance decisions, and the components that enforce those decisions are not necessarily the same. Thus, there is no common terminology between them. Similarly, the data formats and protocols defined in the data catalog may be defined differently than the components used for reading/writing data. In order to enable all these independent components to be used in a single architecture, Fybrik provides a taxonomy. It provides a mechanism for all the components to interact using a common dialect. The project defines a set of immutable structural JSON schemas, or \"taxonomies\" for resources deployed in Fybrik. However, since the taxonomy is meant to be configurable, a taxonomy.json file is referenced from these schemas for any definition that is customizable. The taxonomy.json file is generated from a base taxonomy and zero or more taxonomy layers: The base taxonomy is maintained by the project and includes all of the structural definitions that are subject to customization (e.g.: tags, actions). The taxonomy layers are maintained by users and external systems that add customizations over the base taxonomy (e.g., defining specific tags, actions). This task describes how to deploy Fybrik with a custom taxonomy.json file that is generated with the Taxonomy Compile CLI tool. Taxonomy Compile CLI tool A CLI tool for compiling a base taxonomy and zero or more taxonomy layers is provided in the taxonomy-cli repository , along with a Docker image to directly run the tool. The base taxonomy can be found in charts/fybrik/files/taxonomy/taxonomy.json and example layers can be found in samples/taxonomy/example . The following command can be used to run the Taxonomy Compile CLI tool from the provided Docker image. Usage: docker run --rm --volume ${ PWD } :/local --workdir /local/ ghcr.io/fybrik/taxonomy-cli:0.1.0 compile --out <outputFile> --base <baseFile> [ <layerFile> ... ] [ --codegen ] Flags: -b, --base string : File with base taxonomy definitions (required) --codegen : Best effort to make output suitable for code generation tools -o, --out string : Path for output file (default taxonomy.json ) This will generate a taxonomy.json file with the layers specified. Alternatively, the tool can be run from the root of the taxonomy-cli repository. Usage: go run main.go compile --out <outputFile> --base <baseFile> [ <layerFile> ... ] [ --codegen ] Deploy Fybrik with Custom Taxonomy To deploy Fybrik with the generated taxonomy.json file, follow the quickstart guide but use the command below instead of helm install fybrik fybrik-charts/fybrik -n fybrik-system --wait : helm install fybrik fybrik-charts/fybrik -n fybrik-system --wait --set-file taxonomyOverride = taxonomy.json The --set-file flag will pass in your custom taxonomy.json file to use for taxonomy validation in Fybrik. If this flag is not provided, Fybrik will use the default taxonomy.json file with no layers compiled into it. For an already deployed fybrik instance, it is possible to upgrade fybrik with an updated custom taxonomy file ( taxonomy.json ) with the following command: helm upgrade fybrik fybrik-charts/fybrik -n fybrik-system --wait --set-file taxonomyOverride = taxonomy.json Examples of changing taxonomy Example 1: Add new intent for FybrikApplication In this example we show how to update the application taxonomy. We show that when a FybrikApplication yaml containing a Marketing intent is submitted, it's validation fails because initially the application's taxonomy does not include Marketing . We then describe how to add Marketing to the taxonomy, enabling the validation to pass when we re-submit the FybrikApplication yaml. Follow the quickstart guide but stop before the command helm install fybrik fybrik-charts/fybrik -n fybrik-system --wait (or helm install fybrik charts/fybrik --set global.tag=master --set global.imagePullPolicy=Always -n fybrik-system --wait in development mode). The initial taxonomy to be used in this example is a base taxonomy that can be found in charts/fybrik/files/taxonomy/taxonomy.json with the following taxonomy layer: definitions : AppInfo : properties : intent : type : string enum : - Customer Support - Fraud Detection - Customer Behavior Analysis required : - intent Copy the taxonomy layer to a taxonomy-layer.yaml file. The working directory is the fybrik repository. In order to compile and merge the two taxonomies, the Taxonomy Compile CLI tool is used in the following way: docker run --rm --volume ${ PWD } :/local --workdir /local/ ghcr.io/fybrik/taxonomy-cli:0.1.0 compile \\ --out custom-taxonomy.json --base charts/fybrik/files/taxonomy/taxonomy.json taxonomy-layer.yaml This command creates a custom-taxonomy.json file, which is included in the helm installation of fybrik using the following command: helm install fybrik charts/fybrik --set global.tag = master --set global.imagePullPolicy = Always -n fybrik-system --wait --set-file taxonomyOverride = custom-taxonomy.json Trying to deploy a fybrikapplication.yaml that has an intent of Marketing should fail validation beacuse there is no Marketing intent in the taxonomy. The following command should fail with a description of a validation error : cat << EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1beta1 kind : FybrikApplication metadata : name : taxonomy-test spec : selector : workloadSelector : matchLabels : { app : notebook } appInfo : intent : Marketing role : Business Analyst data : - dataSetID : \"default/fake.csv\" requirements : interface : protocol : s3 dataformat : csv EOF The expected error is The FybrikApplication \"taxonomy-test\" is invalid: spec.appInfo.intent: Invalid value: \"Marketing\": spec.appInfo.intent must be one of the following: \"Customer Behavior Analysis\", \"Customer Support\", \"Fraud Detection\" . Thus, no FybrikApplication CRD was created. To fix this, a new intent with Marketing value should be added to the taxonomy. Add a new value of \"Marketing\" in custom-taxonomy.json file in intent property as follows: \"intent\": { \"type\": \"string\", \"enum\": [ \"Customer Behavior Analysis\", \"Customer Support\", \"Fraud Detection\", \"Marketing\" ] } Now we upgrade the fybrik helm chart using the following command: helm upgrade fybrik charts/fybrik --set global.tag = master --set global.imagePullPolicy = Always -n fybrik-system --wait --set-file taxonomyOverride = custom-taxonomy.json After updating fybrik to get fybrikapplications with Marketing intent, the deployment of a fybrikapplication.yaml that has an intent of Marketing will succeed: cat << EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1beta1 kind : FybrikApplication metadata : name : taxonomy-test spec : selector : workloadSelector : matchLabels : { app : notebook } appInfo : intent : Marketing role : Business Analyst data : - dataSetID : \"default/fake.csv\" requirements : interface : protocol : s3 dataformat : csv EOF The result is a FybrikApplication Custom Resource Definition instance called taxonomy-test. Example 2: Add new action for FybrikModule In this example we show how to update the module taxonomy. We show that when a FybrikModule yaml containing a FilterAction action is submitted, it's validation fails because initially the module's taxonomy does not include FilterAction . We then describe how to add a new action FilterAction to the taxonomy, enabling the validation to pass when we re-submit the FybrikModule yaml. Follow the quickstart guide but stop before the command helm install fybrik fybrik-charts/fybrik -n fybrik-system --wait (or helm install fybrik charts/fybrik --set global.tag=master --set global.imagePullPolicy=Always -n fybrik-system --wait in development mode). The initial taxonomy to be used in this example is a base taxonomy that can be found in charts/fybrik/files/taxonomy/taxonomy.json with the following taxonomy layer: definitions : Action : oneOf : - $ref : \"#/definitions/RedactAction\" - $ref : \"#/definitions/RemoveAction\" - $ref : \"#/definitions/Deny\" RedactAction : type : object properties : columns : items : type : string type : array required : - columns RemoveAction : type : object properties : columns : items : type : string type : array required : - columns Deny : type : object additionalProperties : false Copy the taxonomy layer to a taxonomy-layer.yaml file. The working directory is the fybrik repository. In order to compile and merge the two taxonomies, the Taxonomy Compile CLI tool is used in the following way: docker run --rm --volume ${ PWD } :/local --workdir /local/ ghcr.io/fybrik/taxonomy-cli:0.1.0 compile \\ --out custom-taxonomy.json --base charts/fybrik/files/taxonomy/taxonomy.json taxonomy-layer.yaml This command creates a custom-taxonomy.json file, which is included in the helm installation of fybrik using the following command: helm install fybrik charts/fybrik --set global.tag = master --set global.imagePullPolicy = Always -n fybrik-system --wait --set-file taxonomyOverride = custom-taxonomy.json Trying to deploy a fybrikmodule.yaml that has a FilterAction should fail validation beacuse there is no FilterAction in the taxonomy. The following command should fail with a description of a validation error : cat << EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1beta1 kind : FybrikModule metadata : name : taxonomy-module-test spec : type : service chart : name : ghcr.io/fybrik/fake values : image.tag : master capabilities : - capability : read scope : workload supportedInterfaces : - source : protocol : s3 dataformat : parquet - source : protocol : s3 dataformat : csv actions : - name : RedactAction - name : FilterAction EOF The expected error is The FybrikModule \"taxonomy-module-test\" is invalid: spec.capabilities.0.actions.0.name: Invalid value: \"FilterAction\": spec.capabilities.0.actions.0.name must be one of the following: \"Deny\", \"RedactAction\", \"RemoveAction\" . Thus, no FybrikModule CRD was created. To fix this, a new action FilterAction should be added to the taxonomy. Add a new file taxonomy-layer2.yaml with the new action FilterAction as follows: definitions : Action : oneOf : - $ref : \"#/definitions/FilterAction\" FilterAction : type : object properties : threshold : type : integer operation : type : string required : - threshold Now we create the custom-taxonomy.json file as before, by using the following command: docker run --rm --volume ${ PWD } :/local --workdir /local/ ghcr.io/fybrik/taxonomy-cli:0.1.0 compile \\ --out custom-taxonomy.json --base charts/fybrik/files/taxonomy/taxonomy.json taxonomy-layer.yaml taxonomy-layer2.yaml Now we upgrade the fybrik helm chart using the following command: helm upgrade fybrik charts/fybrik --set global.tag = master --set global.imagePullPolicy = Always -n fybrik-system --wait --set-file taxonomyOverride = custom-taxonomy.json After updating fybrik to get fybrikmodule with FilterAction , the deployment of a fybrikmodule.yaml that has a FilterAction will succeed: cat << EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1beta1 kind : FybrikModule metadata : name : taxonomy-module-test spec : type : service chart : name : ghcr.io/fybrik/fake values : image.tag : master capabilities : - capability : read scope : workload supportedInterfaces : - source : protocol : s3 dataformat : parquet - source : protocol : s3 dataformat : csv actions : - name : RedactAction - name : FilterAction EOF The result is a FybrikModule Custom Resource Definition instance called taxonomy-module-test.","title":"Using a Custom Taxonomy for Resource Validation"},{"location":"tasks/custom-taxonomy/#using-a-custom-taxonomy-for-resource-validation","text":"","title":"Using a Custom Taxonomy for Resource Validation"},{"location":"tasks/custom-taxonomy/#background","text":"Fybrik acts as an orchestrator of independent components. For example, the author of the data governance policy manager, which provides the governance decisions, and the components that enforce those decisions are not necessarily the same. Thus, there is no common terminology between them. Similarly, the data formats and protocols defined in the data catalog may be defined differently than the components used for reading/writing data. In order to enable all these independent components to be used in a single architecture, Fybrik provides a taxonomy. It provides a mechanism for all the components to interact using a common dialect. The project defines a set of immutable structural JSON schemas, or \"taxonomies\" for resources deployed in Fybrik. However, since the taxonomy is meant to be configurable, a taxonomy.json file is referenced from these schemas for any definition that is customizable. The taxonomy.json file is generated from a base taxonomy and zero or more taxonomy layers: The base taxonomy is maintained by the project and includes all of the structural definitions that are subject to customization (e.g.: tags, actions). The taxonomy layers are maintained by users and external systems that add customizations over the base taxonomy (e.g., defining specific tags, actions). This task describes how to deploy Fybrik with a custom taxonomy.json file that is generated with the Taxonomy Compile CLI tool.","title":"Background"},{"location":"tasks/custom-taxonomy/#taxonomy-compile-cli-tool","text":"A CLI tool for compiling a base taxonomy and zero or more taxonomy layers is provided in the taxonomy-cli repository , along with a Docker image to directly run the tool. The base taxonomy can be found in charts/fybrik/files/taxonomy/taxonomy.json and example layers can be found in samples/taxonomy/example . The following command can be used to run the Taxonomy Compile CLI tool from the provided Docker image. Usage: docker run --rm --volume ${ PWD } :/local --workdir /local/ ghcr.io/fybrik/taxonomy-cli:0.1.0 compile --out <outputFile> --base <baseFile> [ <layerFile> ... ] [ --codegen ] Flags: -b, --base string : File with base taxonomy definitions (required) --codegen : Best effort to make output suitable for code generation tools -o, --out string : Path for output file (default taxonomy.json ) This will generate a taxonomy.json file with the layers specified. Alternatively, the tool can be run from the root of the taxonomy-cli repository. Usage: go run main.go compile --out <outputFile> --base <baseFile> [ <layerFile> ... ] [ --codegen ]","title":"Taxonomy Compile CLI tool"},{"location":"tasks/custom-taxonomy/#deploy-fybrik-with-custom-taxonomy","text":"To deploy Fybrik with the generated taxonomy.json file, follow the quickstart guide but use the command below instead of helm install fybrik fybrik-charts/fybrik -n fybrik-system --wait : helm install fybrik fybrik-charts/fybrik -n fybrik-system --wait --set-file taxonomyOverride = taxonomy.json The --set-file flag will pass in your custom taxonomy.json file to use for taxonomy validation in Fybrik. If this flag is not provided, Fybrik will use the default taxonomy.json file with no layers compiled into it. For an already deployed fybrik instance, it is possible to upgrade fybrik with an updated custom taxonomy file ( taxonomy.json ) with the following command: helm upgrade fybrik fybrik-charts/fybrik -n fybrik-system --wait --set-file taxonomyOverride = taxonomy.json","title":"Deploy Fybrik with Custom Taxonomy"},{"location":"tasks/custom-taxonomy/#examples-of-changing-taxonomy","text":"","title":"Examples of changing taxonomy"},{"location":"tasks/custom-taxonomy/#example-1-add-new-intent-for-fybrikapplication","text":"In this example we show how to update the application taxonomy. We show that when a FybrikApplication yaml containing a Marketing intent is submitted, it's validation fails because initially the application's taxonomy does not include Marketing . We then describe how to add Marketing to the taxonomy, enabling the validation to pass when we re-submit the FybrikApplication yaml. Follow the quickstart guide but stop before the command helm install fybrik fybrik-charts/fybrik -n fybrik-system --wait (or helm install fybrik charts/fybrik --set global.tag=master --set global.imagePullPolicy=Always -n fybrik-system --wait in development mode). The initial taxonomy to be used in this example is a base taxonomy that can be found in charts/fybrik/files/taxonomy/taxonomy.json with the following taxonomy layer: definitions : AppInfo : properties : intent : type : string enum : - Customer Support - Fraud Detection - Customer Behavior Analysis required : - intent Copy the taxonomy layer to a taxonomy-layer.yaml file. The working directory is the fybrik repository. In order to compile and merge the two taxonomies, the Taxonomy Compile CLI tool is used in the following way: docker run --rm --volume ${ PWD } :/local --workdir /local/ ghcr.io/fybrik/taxonomy-cli:0.1.0 compile \\ --out custom-taxonomy.json --base charts/fybrik/files/taxonomy/taxonomy.json taxonomy-layer.yaml This command creates a custom-taxonomy.json file, which is included in the helm installation of fybrik using the following command: helm install fybrik charts/fybrik --set global.tag = master --set global.imagePullPolicy = Always -n fybrik-system --wait --set-file taxonomyOverride = custom-taxonomy.json Trying to deploy a fybrikapplication.yaml that has an intent of Marketing should fail validation beacuse there is no Marketing intent in the taxonomy. The following command should fail with a description of a validation error : cat << EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1beta1 kind : FybrikApplication metadata : name : taxonomy-test spec : selector : workloadSelector : matchLabels : { app : notebook } appInfo : intent : Marketing role : Business Analyst data : - dataSetID : \"default/fake.csv\" requirements : interface : protocol : s3 dataformat : csv EOF The expected error is The FybrikApplication \"taxonomy-test\" is invalid: spec.appInfo.intent: Invalid value: \"Marketing\": spec.appInfo.intent must be one of the following: \"Customer Behavior Analysis\", \"Customer Support\", \"Fraud Detection\" . Thus, no FybrikApplication CRD was created. To fix this, a new intent with Marketing value should be added to the taxonomy. Add a new value of \"Marketing\" in custom-taxonomy.json file in intent property as follows: \"intent\": { \"type\": \"string\", \"enum\": [ \"Customer Behavior Analysis\", \"Customer Support\", \"Fraud Detection\", \"Marketing\" ] } Now we upgrade the fybrik helm chart using the following command: helm upgrade fybrik charts/fybrik --set global.tag = master --set global.imagePullPolicy = Always -n fybrik-system --wait --set-file taxonomyOverride = custom-taxonomy.json After updating fybrik to get fybrikapplications with Marketing intent, the deployment of a fybrikapplication.yaml that has an intent of Marketing will succeed: cat << EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1beta1 kind : FybrikApplication metadata : name : taxonomy-test spec : selector : workloadSelector : matchLabels : { app : notebook } appInfo : intent : Marketing role : Business Analyst data : - dataSetID : \"default/fake.csv\" requirements : interface : protocol : s3 dataformat : csv EOF The result is a FybrikApplication Custom Resource Definition instance called taxonomy-test.","title":"Example 1: Add new intent for FybrikApplication"},{"location":"tasks/custom-taxonomy/#example-2-add-new-action-for-fybrikmodule","text":"In this example we show how to update the module taxonomy. We show that when a FybrikModule yaml containing a FilterAction action is submitted, it's validation fails because initially the module's taxonomy does not include FilterAction . We then describe how to add a new action FilterAction to the taxonomy, enabling the validation to pass when we re-submit the FybrikModule yaml. Follow the quickstart guide but stop before the command helm install fybrik fybrik-charts/fybrik -n fybrik-system --wait (or helm install fybrik charts/fybrik --set global.tag=master --set global.imagePullPolicy=Always -n fybrik-system --wait in development mode). The initial taxonomy to be used in this example is a base taxonomy that can be found in charts/fybrik/files/taxonomy/taxonomy.json with the following taxonomy layer: definitions : Action : oneOf : - $ref : \"#/definitions/RedactAction\" - $ref : \"#/definitions/RemoveAction\" - $ref : \"#/definitions/Deny\" RedactAction : type : object properties : columns : items : type : string type : array required : - columns RemoveAction : type : object properties : columns : items : type : string type : array required : - columns Deny : type : object additionalProperties : false Copy the taxonomy layer to a taxonomy-layer.yaml file. The working directory is the fybrik repository. In order to compile and merge the two taxonomies, the Taxonomy Compile CLI tool is used in the following way: docker run --rm --volume ${ PWD } :/local --workdir /local/ ghcr.io/fybrik/taxonomy-cli:0.1.0 compile \\ --out custom-taxonomy.json --base charts/fybrik/files/taxonomy/taxonomy.json taxonomy-layer.yaml This command creates a custom-taxonomy.json file, which is included in the helm installation of fybrik using the following command: helm install fybrik charts/fybrik --set global.tag = master --set global.imagePullPolicy = Always -n fybrik-system --wait --set-file taxonomyOverride = custom-taxonomy.json Trying to deploy a fybrikmodule.yaml that has a FilterAction should fail validation beacuse there is no FilterAction in the taxonomy. The following command should fail with a description of a validation error : cat << EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1beta1 kind : FybrikModule metadata : name : taxonomy-module-test spec : type : service chart : name : ghcr.io/fybrik/fake values : image.tag : master capabilities : - capability : read scope : workload supportedInterfaces : - source : protocol : s3 dataformat : parquet - source : protocol : s3 dataformat : csv actions : - name : RedactAction - name : FilterAction EOF The expected error is The FybrikModule \"taxonomy-module-test\" is invalid: spec.capabilities.0.actions.0.name: Invalid value: \"FilterAction\": spec.capabilities.0.actions.0.name must be one of the following: \"Deny\", \"RedactAction\", \"RemoveAction\" . Thus, no FybrikModule CRD was created. To fix this, a new action FilterAction should be added to the taxonomy. Add a new file taxonomy-layer2.yaml with the new action FilterAction as follows: definitions : Action : oneOf : - $ref : \"#/definitions/FilterAction\" FilterAction : type : object properties : threshold : type : integer operation : type : string required : - threshold Now we create the custom-taxonomy.json file as before, by using the following command: docker run --rm --volume ${ PWD } :/local --workdir /local/ ghcr.io/fybrik/taxonomy-cli:0.1.0 compile \\ --out custom-taxonomy.json --base charts/fybrik/files/taxonomy/taxonomy.json taxonomy-layer.yaml taxonomy-layer2.yaml Now we upgrade the fybrik helm chart using the following command: helm upgrade fybrik charts/fybrik --set global.tag = master --set global.imagePullPolicy = Always -n fybrik-system --wait --set-file taxonomyOverride = custom-taxonomy.json After updating fybrik to get fybrikmodule with FilterAction , the deployment of a fybrikmodule.yaml that has a FilterAction will succeed: cat << EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1beta1 kind : FybrikModule metadata : name : taxonomy-module-test spec : type : service chart : name : ghcr.io/fybrik/fake values : image.tag : master capabilities : - capability : read scope : workload supportedInterfaces : - source : protocol : s3 dataformat : parquet - source : protocol : s3 dataformat : csv actions : - name : RedactAction - name : FilterAction EOF The result is a FybrikModule Custom Resource Definition instance called taxonomy-module-test.","title":"Example 2: Add new action for FybrikModule"},{"location":"tasks/data-plane-optimization/","text":"Enabling data-plane optimization Fybrik takes into account data governance and hard IT config policies when building a data plane. However, it does not by default take into account IT config optimization policies (i.e., optimization goals). To enable data-plane optimization, the Optimizer component must be enabled. Enabling the optimizer Enabling the optimizer is done by setting the solver.enabled property to true in Fybrik's Helm chart. Assuming Fybrik is already deployed, the following command can be used to enable the optimizer: ```bash helm upgrade fybrik charts/fybrik --set global.tag=master --set global.imagePullPolicy=Always -n fybrik-system --wait --set solver.enabled=true Using a custom CSP solver The default CSP solver is the one provided by Google OR-Tools . A different solver from the list of FlatZinc-supporting solvers can be configured by following these steps: 1. Prepare a Docker image file containing the solver executable and the solver's dependencies (e.g., dynamically-linked libraries). The executable should be called solver and should be placed in the directory /data/tools/bin of the Docker image. 2. Upload the Docker image file to any public registry. 3. Run the following command to configure the solver (this assumes that Fybrik is already deployed): helm upgrade fybrik charts/fybrik --set global.tag = master --set global.imagePullPolicy = Always -n fybrik-system --wait --set solver.image = <image-of-your-solver>","title":"Enabling data-plane optimization"},{"location":"tasks/data-plane-optimization/#enabling-data-plane-optimization","text":"Fybrik takes into account data governance and hard IT config policies when building a data plane. However, it does not by default take into account IT config optimization policies (i.e., optimization goals). To enable data-plane optimization, the Optimizer component must be enabled.","title":"Enabling data-plane optimization"},{"location":"tasks/data-plane-optimization/#enabling-the-optimizer","text":"Enabling the optimizer is done by setting the solver.enabled property to true in Fybrik's Helm chart. Assuming Fybrik is already deployed, the following command can be used to enable the optimizer: ```bash helm upgrade fybrik charts/fybrik --set global.tag=master --set global.imagePullPolicy=Always -n fybrik-system --wait --set solver.enabled=true","title":"Enabling the optimizer"},{"location":"tasks/data-plane-optimization/#using-a-custom-csp-solver","text":"The default CSP solver is the one provided by Google OR-Tools . A different solver from the list of FlatZinc-supporting solvers can be configured by following these steps: 1. Prepare a Docker image file containing the solver executable and the solver's dependencies (e.g., dynamically-linked libraries). The executable should be called solver and should be placed in the directory /data/tools/bin of the Docker image. 2. Upload the Docker image file to any public registry. 3. Run the following command to configure the solver (this assumes that Fybrik is already deployed): helm upgrade fybrik charts/fybrik --set global.tag = master --set global.imagePullPolicy = Always -n fybrik-system --wait --set solver.image = <image-of-your-solver>","title":"Using a custom CSP solver"},{"location":"tasks/high-availability/","text":"High Availability When having more then one manager instance running Active/Passive high availability method is used: only one of them is being the leader (doing all the exclusive tasks) and other instances waiting in standby mode in case the leader dies to take over the leader role. The Active/Passive high availability is implemented with Kubernetes leader election mechanism and it is turned on by default. In this implementation a config-map resource called fybrik-operator-leader-election serves as a lock. The config-map also contains information about the chosen leader. To change Fybrik manager number of replicas the following setting should be added to Fybrik helm chart deployment command --set manager.replicaCount=<desired replica value> .","title":"High Availability"},{"location":"tasks/high-availability/#high-availability","text":"When having more then one manager instance running Active/Passive high availability method is used: only one of them is being the leader (doing all the exclusive tasks) and other instances waiting in standby mode in case the leader dies to take over the leader role. The Active/Passive high availability is implemented with Kubernetes leader election mechanism and it is turned on by default. In this implementation a config-map resource called fybrik-operator-leader-election serves as a lock. The config-map also contains information about the chosen leader. To change Fybrik manager number of replicas the following setting should be added to Fybrik helm chart deployment command --set manager.replicaCount=<desired replica value> .","title":"High Availability"},{"location":"tasks/infrastructure/","text":"Infrastructure attributes When writing configuration policies, infrastructure metrics and costs may also be taken into account in order to optimize the generated data plane. For example, selection of a storage account may be based on a storage cost, selection of a cluster may provide a restriction on cluster capacity, and so on. Collection of the metrics and their dynamic update is beyond the scope of Fybrik. One may develop or use 3rd party solutions for monitoring and updating these infrastructure metrics. Infrastructure attributes are stored in the /tmp/adminconfig/infrastructure.json directory of the manager pod. Metric metadata Prior to defining an infrastructure attribute, the corresponding metric should be defined, providing information about the attribute value, e.g. the measurement units and the scale of possible values. Several attributes may share the same metric, e.g. rate can be defined for both the error-rate and the load-rate . Example of a metric: \"name\": \"rate\", \"type\": \"numeric\", \"units\": \"%\", \"scale\": {\"min\": 0, \"max\": 100} How to define infrastructure attributes An infrastructure attribute is defined by a JSON object that includes the following fields: attribute - name of the infrastructure attribute, should be defined in the taxonomy description metricName - a reference to the metric value - the actual value of the attribute object - a resource the attribute relates to (storageaccount, module, cluster) instance - a reference to the resource instance, e.g. storage account name The infrastructure attributes are associated with resources managed by Fybrik: FybrikStorageAccount, FybrikModule and cluster (defined in the cluster-metadata config map). The valid values for the attribute object field are storageaccount , module and cluster , respectively. For example, the following attribute defines the storage cost of the \"account-theshire\" storage account. { \"attribute\": \"storage-cost\", \"description\": \"theshire object store\", \"value\": \"90\", \"metricName\": \"cost\", \"object\": \"storageaccount\", \"instance\": \"account-theshire\" } Add a new attribute definition to the taxonomy See metric taxonomy for an example how to define an attribute and the corresponding measurement units. Usage of infrastructure attributes in configuration policies An infrastructure attribute can be used as the property value in configuration policies . For example, the following policy restricts the storage account selection using the storage-cost infrastructure attribute: # restrict storage costs to a maximum of $95 when copying the data config[{\"capability\": \"copy\", \"decision\": decision}] { input.request.usage == \"copy\" input.request.dataset.geography != input.workload.cluster.metadata.region account_restrict := {\"property\": \"storage-cost\", \"range\": {\"max\": 95}} policy := {\"ID\": \"copy-restrict-storage\", \"description\":\"Use cheaper storage\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"restrictions\": {\"storageaccounts\": [account_restrict]}} } Usage of infrastructure attributes for optimization Attributes can also be used for optimizing a control plane. For example, the following rule reduces storage costs: # minimize storage cost optimize[decision] { policy := {\"ID\": \"save-cost\", \"description\":\"Save storage costs\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"strategy\": [{\"attribute\": \"storage-cost\", \"directive\": \"min\"}]} }","title":"Infrastructure attributes"},{"location":"tasks/infrastructure/#infrastructure-attributes","text":"When writing configuration policies, infrastructure metrics and costs may also be taken into account in order to optimize the generated data plane. For example, selection of a storage account may be based on a storage cost, selection of a cluster may provide a restriction on cluster capacity, and so on. Collection of the metrics and their dynamic update is beyond the scope of Fybrik. One may develop or use 3rd party solutions for monitoring and updating these infrastructure metrics. Infrastructure attributes are stored in the /tmp/adminconfig/infrastructure.json directory of the manager pod.","title":"Infrastructure attributes"},{"location":"tasks/infrastructure/#metric-metadata","text":"Prior to defining an infrastructure attribute, the corresponding metric should be defined, providing information about the attribute value, e.g. the measurement units and the scale of possible values. Several attributes may share the same metric, e.g. rate can be defined for both the error-rate and the load-rate . Example of a metric: \"name\": \"rate\", \"type\": \"numeric\", \"units\": \"%\", \"scale\": {\"min\": 0, \"max\": 100}","title":"Metric metadata"},{"location":"tasks/infrastructure/#how-to-define-infrastructure-attributes","text":"An infrastructure attribute is defined by a JSON object that includes the following fields: attribute - name of the infrastructure attribute, should be defined in the taxonomy description metricName - a reference to the metric value - the actual value of the attribute object - a resource the attribute relates to (storageaccount, module, cluster) instance - a reference to the resource instance, e.g. storage account name The infrastructure attributes are associated with resources managed by Fybrik: FybrikStorageAccount, FybrikModule and cluster (defined in the cluster-metadata config map). The valid values for the attribute object field are storageaccount , module and cluster , respectively. For example, the following attribute defines the storage cost of the \"account-theshire\" storage account. { \"attribute\": \"storage-cost\", \"description\": \"theshire object store\", \"value\": \"90\", \"metricName\": \"cost\", \"object\": \"storageaccount\", \"instance\": \"account-theshire\" }","title":"How to define infrastructure attributes"},{"location":"tasks/infrastructure/#add-a-new-attribute-definition-to-the-taxonomy","text":"See metric taxonomy for an example how to define an attribute and the corresponding measurement units.","title":"Add a new attribute definition to the taxonomy"},{"location":"tasks/infrastructure/#usage-of-infrastructure-attributes-in-configuration-policies","text":"An infrastructure attribute can be used as the property value in configuration policies . For example, the following policy restricts the storage account selection using the storage-cost infrastructure attribute: # restrict storage costs to a maximum of $95 when copying the data config[{\"capability\": \"copy\", \"decision\": decision}] { input.request.usage == \"copy\" input.request.dataset.geography != input.workload.cluster.metadata.region account_restrict := {\"property\": \"storage-cost\", \"range\": {\"max\": 95}} policy := {\"ID\": \"copy-restrict-storage\", \"description\":\"Use cheaper storage\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"restrictions\": {\"storageaccounts\": [account_restrict]}} }","title":"Usage of infrastructure attributes in configuration policies"},{"location":"tasks/infrastructure/#usage-of-infrastructure-attributes-for-optimization","text":"Attributes can also be used for optimizing a control plane. For example, the following rule reduces storage costs: # minimize storage cost optimize[decision] { policy := {\"ID\": \"save-cost\", \"description\":\"Save storage costs\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"strategy\": [{\"attribute\": \"storage-cost\", \"directive\": \"min\"}]} }","title":"Usage of infrastructure attributes for optimization"},{"location":"tasks/multicluster/","text":"Multicluster setup Fybrik is dynamic in its multi cluster capabilities in that it has abstractions to support multiple different cross-cluster orchestration mechanisms. Currently, only one multi cluster orchestration mechanism is implemented and is using Razee for the orchestration. Multicluster operation with Razee Razee is a multi-cluster continuous delivery tool for Kubernetes that can deploy software on remote clusters and track the deployment status of such deployments. There are multiple ways to run Razee. The two described here are a vanilla open source deployment on your own Kubernetes or as a managed service from a cloud provider. Due to the complex nature of installing Razee a managed service from a cloud provider is recommended. It's possible to define a multicluster group name that groups clusters that are used in a Fybrik instance. This will restrict the clusters that are usable in the Fybrik instance to the ones that are registered in the specified Razee group. This is especially helpful if Razee is also used for different purposes than Fybrik or multiple Fybrik instances should be used under the same Razee installation. In general there is a need for the following Razee components to be installed: Razee watch keeper (installed on all clusters) Razee cluster subscription manager (installed on all clusters) RazeeDash API (installed on coordinator cluster/as cloud service) Both methods below describe how the above components can be installed depending on what RazeeDash deployment method is used. Installing Razee on Kubernetes Coordinator cluster An installation of the open source components is described here . Please follow the instructions in the Razee documentation to install RazeeDash , Watch keeper and the cluster subscription agent . At the moment Razee supports GitHub, GitHub Enterprise and BitBucket for the OAUTH Authentication of this installation. Please be aware that the RazeeDash API needs to be reachable from all clusters. Thus, there may be the need for routes, ingresses or node ports in order to expose it to other networks and clusters. Once RazeeDash is installed the UI can be used to group registered clusters in a multicluster group that can be configured below. The API Key can also be retrieved from the UI following these two steps. From the RazeeDash console, click the arrow icon in the upper right corner. Then, select Profile. Copy the API key value. If no API key exists, click Generate to generate one. In order to configure Fybrik to use the installed Razee on Kubernetes the values of the helm charts have to be adapted to the following: coordinator: razee: # URL for Razee deployment url: \"https://your-razee-service:3333/graphql\" # Razee deployment with oauth API key authentication requires the apiKey parameter apiKey: \"<your Razee X_API_KEY>\" multiclusterGroup: \"<your group name>\" Remote cluster The remote clusters only need the watch keeper and cluster subscription agents installed. The remote clusters do not need the coordinator component of Fybrik. It's enough to follow this guide to install the agents and configure a group via the RazeeDash UI if needed. The coordinator configuration would look like the following: coordinator: enabled: false Installing using IBM Satellite Config When using IBM Satellite Config the RazeeDash API is running as a service in the cloud and all custom resource distribution is handled by the cloud. The process here describes how an already existing Kubernetes cluster can be registered and configured. Prerequisites: An IBM Cloud Account IBM Cloud Satellite service IAM API Keys with access to IBM Cloud Satellite service The step below has to be executed for each cluster that should be added to the Fybrik instance. This step is the same for coordinator and remote clusters. In the IBM Satellite Cloud service under the Clusters tab click on Register cluster . Enter a cluster name in the popup dialog and click Register cluster . (Please don't use spaces in the name) The next dialog will offer you a kubectl command that can be executed on the cluster that should be attached. After executing the kubectl command the Razee services will be installed in the razeedeploy namespace and the cluster will show up in your cluster list (like in the picture above). This installs the watch keeper and cluster subscription components. Create clusters groups by clicking on the Cluster groups tab: for each cluster create a group named fybrik-<cluster-name> and add the cluster to that group. In addition, create a single group for all the clusters: the name of this group is used when deploying the coordinator cluster as shown below. The next step is to configure Fybrik to use IBM Satellite config as multicluster orchestrator. This configuration is done via a Kubernetes secret that is created by the helm chart. Overwriting the coordinator.razee values in your deployment will make use of the multicluster tooling. A configuration using IBM Satellite Config would look like the following for the coordinator cluster: coordinator: # Configures the Razee instance to be used by the coordinator manager in a multicluster setup razee: # IBM Cloud IAM API Key of a user or service account that have access to IBM Cloud Satellite Config iamKey: \"<your IAM API KEY key>\" multiclusterGroup: \"<your group name>\" For the remote cluster the coordinator will be disabled: coordinator: enabled: false Configure Vault for multi-cluster deployment The Fybrik uses HashiCorp Vault to provide running Fybrik modules in the clusters with the dataset credentials when accessing data. This is done using Vault plugin system as described in vault plugin page . This section describes steps for the Fybrik modules to authenticate with Vault, in order to obtain database credentials. Some of the steps described below are not specific to the Fybrik project but rather are Vault specific and can be found in Vault-related online tutorials. Module authentication is done by configuring Vault to use Kubernetes auth method in each cluster. Using this method, the modules can authenticate to Vault by providing their service account token. Behind the scenes, Vault authenticates the token by submitting a TokenReview request to the API server of the Kubernetes cluster where the module is running. Prerequisites unless Fybrik modules are running on the same cluster as the Vault instance: The running Vault instance should have connectivity to the cluster API server for each cluster running Fybrik modules. The running Vault instance should have an Ingress resource to enable Fybrik modules to get the credentials. Before you begin Ensure that you have the Vault v1.9.x to execute Vault CLI commands. Enabling Kubernetes authentication for each cluster with running Fybrik modules: Create a token reviewer service account called vault-auth in the fybrik-system namespace and give it permissions to create tokenreviews.authentication.k8s.io at the cluster scope: apiVersion: v1 kind: ServiceAccount metadata: name: vault-auth namespace: fybrik-system --- apiVersion: v1 kind: Secret metadata: name: vault-auth namespace: fybrik-system annotations: kubernetes.io/service-account.name: vault-auth type: kubernetes.io/service-account-token --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: role-tokenreview-binding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:auth-delegator subjects: - kind: ServiceAccount name: vault-auth namespace: fybrik-system Login to Vault: vault login Enable the Kubernetes auth method in a new path: vault auth enable -path = <auth path> kubernetes Use the /config endpoint to configure Vault to talk to Kubernetes: TOKEN_REVIEW_JWT = $( kubectl get secret vault-auth -n fybrik-system -o jsonpath = \"{.data.token}\" | base64 --decode ) vault write auth/<auth path>/config \\ token_reviewer_jwt = \" $TOKEN_REVIEW_JWT \" \\ kubernetes_host = <Kubernetes api server address> \\ kubernetes_ca_cert = @ca.crt More details on the parameters in the command above can be found in Vault's documentation . Add the Vault policy and role to allow the modules to get the dataset credentials. More details on defining Vault policy for Fybrik can be found in HashiCorp Vault plugins page . The Vault role which binds the policy to the modules can be defined in the following example: vault write auth/<auth path>/role/module \\ bound_service_account_names = \"*\" \\ bound_service_account_namespaces = <modules namespace> \\ policies = \"allow-all-dataset-creds\" \\ ttl = 24h Deploy the Fybrik helm chart with --set cluster.vaultAuthPath=<auth path> parameter","title":"Multicluster setup"},{"location":"tasks/multicluster/#multicluster-setup","text":"Fybrik is dynamic in its multi cluster capabilities in that it has abstractions to support multiple different cross-cluster orchestration mechanisms. Currently, only one multi cluster orchestration mechanism is implemented and is using Razee for the orchestration.","title":"Multicluster setup"},{"location":"tasks/multicluster/#multicluster-operation-with-razee","text":"Razee is a multi-cluster continuous delivery tool for Kubernetes that can deploy software on remote clusters and track the deployment status of such deployments. There are multiple ways to run Razee. The two described here are a vanilla open source deployment on your own Kubernetes or as a managed service from a cloud provider. Due to the complex nature of installing Razee a managed service from a cloud provider is recommended. It's possible to define a multicluster group name that groups clusters that are used in a Fybrik instance. This will restrict the clusters that are usable in the Fybrik instance to the ones that are registered in the specified Razee group. This is especially helpful if Razee is also used for different purposes than Fybrik or multiple Fybrik instances should be used under the same Razee installation. In general there is a need for the following Razee components to be installed: Razee watch keeper (installed on all clusters) Razee cluster subscription manager (installed on all clusters) RazeeDash API (installed on coordinator cluster/as cloud service) Both methods below describe how the above components can be installed depending on what RazeeDash deployment method is used.","title":"Multicluster operation with Razee"},{"location":"tasks/multicluster/#installing-razee-on-kubernetes","text":"","title":"Installing Razee on Kubernetes"},{"location":"tasks/multicluster/#coordinator-cluster","text":"An installation of the open source components is described here . Please follow the instructions in the Razee documentation to install RazeeDash , Watch keeper and the cluster subscription agent . At the moment Razee supports GitHub, GitHub Enterprise and BitBucket for the OAUTH Authentication of this installation. Please be aware that the RazeeDash API needs to be reachable from all clusters. Thus, there may be the need for routes, ingresses or node ports in order to expose it to other networks and clusters. Once RazeeDash is installed the UI can be used to group registered clusters in a multicluster group that can be configured below. The API Key can also be retrieved from the UI following these two steps. From the RazeeDash console, click the arrow icon in the upper right corner. Then, select Profile. Copy the API key value. If no API key exists, click Generate to generate one. In order to configure Fybrik to use the installed Razee on Kubernetes the values of the helm charts have to be adapted to the following: coordinator: razee: # URL for Razee deployment url: \"https://your-razee-service:3333/graphql\" # Razee deployment with oauth API key authentication requires the apiKey parameter apiKey: \"<your Razee X_API_KEY>\" multiclusterGroup: \"<your group name>\"","title":"Coordinator cluster"},{"location":"tasks/multicluster/#remote-cluster","text":"The remote clusters only need the watch keeper and cluster subscription agents installed. The remote clusters do not need the coordinator component of Fybrik. It's enough to follow this guide to install the agents and configure a group via the RazeeDash UI if needed. The coordinator configuration would look like the following: coordinator: enabled: false","title":"Remote cluster"},{"location":"tasks/multicluster/#installing-using-ibm-satellite-config","text":"When using IBM Satellite Config the RazeeDash API is running as a service in the cloud and all custom resource distribution is handled by the cloud. The process here describes how an already existing Kubernetes cluster can be registered and configured. Prerequisites: An IBM Cloud Account IBM Cloud Satellite service IAM API Keys with access to IBM Cloud Satellite service The step below has to be executed for each cluster that should be added to the Fybrik instance. This step is the same for coordinator and remote clusters. In the IBM Satellite Cloud service under the Clusters tab click on Register cluster . Enter a cluster name in the popup dialog and click Register cluster . (Please don't use spaces in the name) The next dialog will offer you a kubectl command that can be executed on the cluster that should be attached. After executing the kubectl command the Razee services will be installed in the razeedeploy namespace and the cluster will show up in your cluster list (like in the picture above). This installs the watch keeper and cluster subscription components. Create clusters groups by clicking on the Cluster groups tab: for each cluster create a group named fybrik-<cluster-name> and add the cluster to that group. In addition, create a single group for all the clusters: the name of this group is used when deploying the coordinator cluster as shown below. The next step is to configure Fybrik to use IBM Satellite config as multicluster orchestrator. This configuration is done via a Kubernetes secret that is created by the helm chart. Overwriting the coordinator.razee values in your deployment will make use of the multicluster tooling. A configuration using IBM Satellite Config would look like the following for the coordinator cluster: coordinator: # Configures the Razee instance to be used by the coordinator manager in a multicluster setup razee: # IBM Cloud IAM API Key of a user or service account that have access to IBM Cloud Satellite Config iamKey: \"<your IAM API KEY key>\" multiclusterGroup: \"<your group name>\" For the remote cluster the coordinator will be disabled: coordinator: enabled: false","title":"Installing using IBM Satellite Config"},{"location":"tasks/multicluster/#configure-vault-for-multi-cluster-deployment","text":"The Fybrik uses HashiCorp Vault to provide running Fybrik modules in the clusters with the dataset credentials when accessing data. This is done using Vault plugin system as described in vault plugin page . This section describes steps for the Fybrik modules to authenticate with Vault, in order to obtain database credentials. Some of the steps described below are not specific to the Fybrik project but rather are Vault specific and can be found in Vault-related online tutorials. Module authentication is done by configuring Vault to use Kubernetes auth method in each cluster. Using this method, the modules can authenticate to Vault by providing their service account token. Behind the scenes, Vault authenticates the token by submitting a TokenReview request to the API server of the Kubernetes cluster where the module is running.","title":"Configure Vault for multi-cluster deployment"},{"location":"tasks/multicluster/#prerequisites-unless-fybrik-modules-are-running-on-the-same-cluster-as-the-vault-instance","text":"The running Vault instance should have connectivity to the cluster API server for each cluster running Fybrik modules. The running Vault instance should have an Ingress resource to enable Fybrik modules to get the credentials.","title":"Prerequisites unless Fybrik modules are running on the same cluster as the Vault instance:"},{"location":"tasks/multicluster/#before-you-begin","text":"Ensure that you have the Vault v1.9.x to execute Vault CLI commands.","title":"Before you begin"},{"location":"tasks/multicluster/#enabling-kubernetes-authentication-for-each-cluster-with-running-fybrik-modules","text":"Create a token reviewer service account called vault-auth in the fybrik-system namespace and give it permissions to create tokenreviews.authentication.k8s.io at the cluster scope: apiVersion: v1 kind: ServiceAccount metadata: name: vault-auth namespace: fybrik-system --- apiVersion: v1 kind: Secret metadata: name: vault-auth namespace: fybrik-system annotations: kubernetes.io/service-account.name: vault-auth type: kubernetes.io/service-account-token --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: role-tokenreview-binding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:auth-delegator subjects: - kind: ServiceAccount name: vault-auth namespace: fybrik-system Login to Vault: vault login Enable the Kubernetes auth method in a new path: vault auth enable -path = <auth path> kubernetes Use the /config endpoint to configure Vault to talk to Kubernetes: TOKEN_REVIEW_JWT = $( kubectl get secret vault-auth -n fybrik-system -o jsonpath = \"{.data.token}\" | base64 --decode ) vault write auth/<auth path>/config \\ token_reviewer_jwt = \" $TOKEN_REVIEW_JWT \" \\ kubernetes_host = <Kubernetes api server address> \\ kubernetes_ca_cert = @ca.crt More details on the parameters in the command above can be found in Vault's documentation . Add the Vault policy and role to allow the modules to get the dataset credentials. More details on defining Vault policy for Fybrik can be found in HashiCorp Vault plugins page . The Vault role which binds the policy to the modules can be defined in the following example: vault write auth/<auth path>/role/module \\ bound_service_account_names = \"*\" \\ bound_service_account_namespaces = <modules namespace> \\ policies = \"allow-all-dataset-creds\" \\ ttl = 24h Deploy the Fybrik helm chart with --set cluster.vaultAuthPath=<auth path> parameter","title":"Enabling Kubernetes authentication for each cluster with running Fybrik modules:"},{"location":"tasks/omd-discover-s3-asset/","text":"Discovering an S3 asset through the OpenMetadata UI This page explains how to discover an existing S3 asset through the OpenMetadata UI. This is useful when running workloads such as the one described in Notebook sample for the read flow . The screenshots refer to the localstack cloud storage, but the explanations also apply to other S3 services. Begin by opening your browser to the OpenMetadata UI. If you installed OpenMetadata in your kubernetes cluster in the open-metadata namespace, go to http://localhost:8585 after running: kubectl port-forward svc/openmetadata -n open-metadata 8585:8585 & To create a connection to S3 and discovering your CSV asset: Login to OpenMetadata. The default username and password are: admin / admin On the left menu, choose Services Press Add new Database Service Choose Datalake and press Next Enter a name for your service, such as openmetadata-s3 , and press Next Enter the connection information. That information includes the Access Key and Secret Key . The AWS Region is mandatory, but it is ignored if you enter an Endpoint URL . If your object storage is a local localstack deployment, enter its URL (e.g. http://localstack.fybrik-notebook-sample:4566 ). Optionally, you may enter a Bucket Name , thereby limiting the discovery process to a single bucket Scroll down and press Test Connection to make sure that the credentials you provided are correct. Once you see that the Connection test was successful , press Save Choose Add ingestion You need not change the ingestion configuration. Press Next Press Next Press Add & Deploy The Ingestion Pipeline is created. Press View Service Choose the Ingestions tab The status of the Ingestion Pipeline might be Queued ... ... or Running . Wait until the ingestion process has completed successfully, and press the Explore tab Given a list of all OpenMetadata tables, press the table in which you are interested You can learn the name that OpenMetadata gave your table by looking at the URL. If, for instance, the URL is localhost:8585/table/openmetadata-s3.default.demo.\"PS_20174392719_1491204439457_log.csv\" , then your assetID is openmetadata-s3.default.demo.\"PS_20174392719_1491204439457_log.csv\" . To add tags, press Add tag Choose a tag for the dataset, such as Purpose.finance Press the check mark Next, you can add tags to some of the columns For instance, you may choose PII.Sensitive for columns that need to be redacted Finally, press the Custom Properties tab Set the asset properties as needed. If the dataFormat field is left blank, it is assumed to be csv . If the discovered asset is of a different format (e.g. parquet ), set dataFormat accordingly. These asset properties will be returned to the Fybrik Manager and will be instrumental in the construction of a data path You are all set. OpenMetadata has discovered your asset, and you have added tags and metadata values. You can reference this asset using the asset ID","title":"Discovering an S3 asset through the OpenMetadata UI"},{"location":"tasks/omd-discover-s3-asset/#discovering-an-s3-asset-through-the-openmetadata-ui","text":"This page explains how to discover an existing S3 asset through the OpenMetadata UI. This is useful when running workloads such as the one described in Notebook sample for the read flow . The screenshots refer to the localstack cloud storage, but the explanations also apply to other S3 services. Begin by opening your browser to the OpenMetadata UI. If you installed OpenMetadata in your kubernetes cluster in the open-metadata namespace, go to http://localhost:8585 after running: kubectl port-forward svc/openmetadata -n open-metadata 8585:8585 & To create a connection to S3 and discovering your CSV asset: Login to OpenMetadata. The default username and password are: admin / admin On the left menu, choose Services Press Add new Database Service Choose Datalake and press Next Enter a name for your service, such as openmetadata-s3 , and press Next Enter the connection information. That information includes the Access Key and Secret Key . The AWS Region is mandatory, but it is ignored if you enter an Endpoint URL . If your object storage is a local localstack deployment, enter its URL (e.g. http://localstack.fybrik-notebook-sample:4566 ). Optionally, you may enter a Bucket Name , thereby limiting the discovery process to a single bucket Scroll down and press Test Connection to make sure that the credentials you provided are correct. Once you see that the Connection test was successful , press Save Choose Add ingestion You need not change the ingestion configuration. Press Next Press Next Press Add & Deploy The Ingestion Pipeline is created. Press View Service Choose the Ingestions tab The status of the Ingestion Pipeline might be Queued ... ... or Running . Wait until the ingestion process has completed successfully, and press the Explore tab Given a list of all OpenMetadata tables, press the table in which you are interested You can learn the name that OpenMetadata gave your table by looking at the URL. If, for instance, the URL is localhost:8585/table/openmetadata-s3.default.demo.\"PS_20174392719_1491204439457_log.csv\" , then your assetID is openmetadata-s3.default.demo.\"PS_20174392719_1491204439457_log.csv\" . To add tags, press Add tag Choose a tag for the dataset, such as Purpose.finance Press the check mark Next, you can add tags to some of the columns For instance, you may choose PII.Sensitive for columns that need to be redacted Finally, press the Custom Properties tab Set the asset properties as needed. If the dataFormat field is left blank, it is assumed to be csv . If the discovered asset is of a different format (e.g. parquet ), set dataFormat accordingly. These asset properties will be returned to the Fybrik Manager and will be instrumental in the construction of a data path You are all set. OpenMetadata has discovered your asset, and you have added tags and metadata values. You can reference this asset using the asset ID","title":"Discovering an S3 asset through the OpenMetadata UI"},{"location":"tasks/performance/","text":"Performance When using many fybric applications at the same time the custom resource operations may take some time. This is due to the default concurrency of controllers being one at a time and the Kubernetes client being rate limited by default. In order to increase the parallelism there are multiple parameters that can be controlled. Each controller parallelism (for each fybrik custom resource) can be controlled separately. When increasing this number it's highly recommended to also increase the managers Kubernetes client QPS and Boost settings so that the controller won't be limited by the amount of queries it can execute to the Kubernetes API. An adapted helm values configuration looks like the following: # Manager component manager: extraEnvs: - name: APPLICATION_CONCURRENT_RECONCILES value: \"5\" - name: BLUEPRINT_CONCURRENT_RECONCILES value: \"20\" - name: PLOTTER_CONCURRENT_RECONCILES value: \"2\" - name: CLIENT_QPS value: \"100.0\" - name: CLIENT_BURST value: \"200\" Please notice that QPS is a float while the other values are integer values.","title":"Performance"},{"location":"tasks/performance/#performance","text":"When using many fybric applications at the same time the custom resource operations may take some time. This is due to the default concurrency of controllers being one at a time and the Kubernetes client being rate limited by default. In order to increase the parallelism there are multiple parameters that can be controlled. Each controller parallelism (for each fybrik custom resource) can be controlled separately. When increasing this number it's highly recommended to also increase the managers Kubernetes client QPS and Boost settings so that the controller won't be limited by the amount of queries it can execute to the Kubernetes API. An adapted helm values configuration looks like the following: # Manager component manager: extraEnvs: - name: APPLICATION_CONCURRENT_RECONCILES value: \"5\" - name: BLUEPRINT_CONCURRENT_RECONCILES value: \"20\" - name: PLOTTER_CONCURRENT_RECONCILES value: \"2\" - name: CLIENT_QPS value: \"100.0\" - name: CLIENT_BURST value: \"200\" Please notice that QPS is a float while the other values are integer values.","title":"Performance"},{"location":"tasks/using-opa/","text":"Using OPA for Data Governance Open Policy Agent may be used as a data governance policy engine with Fybrik via the connector mechanism. When OPA is used for data governance, it is deployed as a stand-alone service. Policies are defined in rego and uploaded to OPA. For more details on OPA policies please refer to OPA documentation in particulate to the basics section which explains how a policy is evaluated. OPA Policies Syntax OPA policies for Fybrik are written in rego files and have the following syntax: rule[{\"action\": <action>, \"policy\": <policy>}] where policy is a string describing the action and action is JSON object with the following form: { \"name\": <name>, <property>: <value>, <property>: <value>, ... } name is the name of the action. For example: \"RedactAction\" property is the name of the action property as defined in the enforcement actions taxonomy . For example: \"columns\". Here is an example Fybrik rule: rule[{\"action\": {\"name\":\"RedactAction\",\"columns\": column_names}, \"policy\": description}] { description := \"Redact written columns tagged as sensitive in datasets tagged with finance = true. The data should not be stored in `neverland` storage account\" input.action.actionType == \"write\" input.resource.metadata.tags.finance input.action.destination != \"neverland\" column_names := [input.resource.metadata.columns[i].name | input.resource.metadata.columns[i].tags.sensitive] } An example of an object that may be returned by the rule above: { \"action\": {\"name\":\"RedactAction\",\"columns\": [\"Address\",\"Name\"]}, \"policy\": \"Redact written columns tagged as sensitive in datasets tagged with finance = true. The data should not be stored in `neverland` storage account\"} } Fybrik Default Policies Fybrik allows by default any request if no rule is triggered. This behavior can be changed to deny by default by altering the value of opaServer.allowByDefault to be false during Fybrik's installation: helm install fybrik fybrik-charts/fybrik --set opaServer.allowByDefault = false --set coordinator.catalog = <Catalog> -n fybrik-system --version master --wait Input to policies The input object includes the application properties and the requested action as well as dataset details (id, metadata). context : request context includes application/workload properties defined in FybrikApplication, e.g. context.properties.intent action : request action includes information about the request such as action.actionType as defined in policy manager taxonomy , e.g write , read , delete or copy resource : the request id and metadata as defined in catalog taxonomy, e.g resource.metadata.geography Managing OPA policies There are several ways to manage policies and data of the OPA service. One simple approach is to use OPA kube-mgmt and manage Rego policies in Kubernetes Configmap resources. By default, Fybrik installs OPA with kube-mgmt enabled. The following two sections show how to use OPA with kube-mgmt. Warning Due to size limits you must ensure that each configmap is smaller than 1MB when base64 encoded. Using a configmap YAML Create a configmap with a Rego policy and a openpolicyagent.org/policy=rego label in the fybrik-system namespace: apiVersion : v1 kind : ConfigMap metadata : name : <policy-name> namespace : fybrik-system labels : openpolicyagent.org/policy : rego data : main : | <you rego policy here> Apply the configmap: kubectl apply -f <policy-name>.yaml To remove the policy just remove the configmap: kubectl delete -f <policy-name>.yaml Using a Rego file You can use kubectl to create a configmap from a Rego file. To create a configmap named <policy-name> from a Rego file in path <policy-name.rego> : kubectl create configmap <policy-name> --from-file = main = <policy-name.rego> -n fybrik-system kubectl label configmap <policy-name> openpolicyagent.org/policy = rego -n fybrik-system Delete the policy with kubectl delete configmap <policy-name> -n fybrik-system . Using opaServer.bootstrapPolicies field Another method to upload policies to OPA is to write them as opaServer.bootstrapPolicies field in values.yaml file used for the Fybrik deployment. In this approach the policies are uploaded upon OPA startup. opaServer: # Bootstrap policies to load upon startup bootstrapPolicies: allowSamplePolicy: | - package dataapi.authz rule [{}] { true }","title":"Using OPA for Data Governance"},{"location":"tasks/using-opa/#using-opa-for-data-governance","text":"Open Policy Agent may be used as a data governance policy engine with Fybrik via the connector mechanism. When OPA is used for data governance, it is deployed as a stand-alone service. Policies are defined in rego and uploaded to OPA. For more details on OPA policies please refer to OPA documentation in particulate to the basics section which explains how a policy is evaluated.","title":"Using OPA for Data Governance"},{"location":"tasks/using-opa/#opa-policies-syntax","text":"OPA policies for Fybrik are written in rego files and have the following syntax: rule[{\"action\": <action>, \"policy\": <policy>}] where policy is a string describing the action and action is JSON object with the following form: { \"name\": <name>, <property>: <value>, <property>: <value>, ... } name is the name of the action. For example: \"RedactAction\" property is the name of the action property as defined in the enforcement actions taxonomy . For example: \"columns\". Here is an example Fybrik rule: rule[{\"action\": {\"name\":\"RedactAction\",\"columns\": column_names}, \"policy\": description}] { description := \"Redact written columns tagged as sensitive in datasets tagged with finance = true. The data should not be stored in `neverland` storage account\" input.action.actionType == \"write\" input.resource.metadata.tags.finance input.action.destination != \"neverland\" column_names := [input.resource.metadata.columns[i].name | input.resource.metadata.columns[i].tags.sensitive] } An example of an object that may be returned by the rule above: { \"action\": {\"name\":\"RedactAction\",\"columns\": [\"Address\",\"Name\"]}, \"policy\": \"Redact written columns tagged as sensitive in datasets tagged with finance = true. The data should not be stored in `neverland` storage account\"} }","title":"OPA Policies Syntax"},{"location":"tasks/using-opa/#fybrik-default-policies","text":"Fybrik allows by default any request if no rule is triggered. This behavior can be changed to deny by default by altering the value of opaServer.allowByDefault to be false during Fybrik's installation: helm install fybrik fybrik-charts/fybrik --set opaServer.allowByDefault = false --set coordinator.catalog = <Catalog> -n fybrik-system --version master --wait","title":"Fybrik Default Policies"},{"location":"tasks/using-opa/#input-to-policies","text":"The input object includes the application properties and the requested action as well as dataset details (id, metadata). context : request context includes application/workload properties defined in FybrikApplication, e.g. context.properties.intent action : request action includes information about the request such as action.actionType as defined in policy manager taxonomy , e.g write , read , delete or copy resource : the request id and metadata as defined in catalog taxonomy, e.g resource.metadata.geography","title":"Input to policies"},{"location":"tasks/using-opa/#managing-opa-policies","text":"There are several ways to manage policies and data of the OPA service. One simple approach is to use OPA kube-mgmt and manage Rego policies in Kubernetes Configmap resources. By default, Fybrik installs OPA with kube-mgmt enabled. The following two sections show how to use OPA with kube-mgmt. Warning Due to size limits you must ensure that each configmap is smaller than 1MB when base64 encoded.","title":"Managing OPA policies"},{"location":"tasks/using-opa/#using-a-configmap-yaml","text":"Create a configmap with a Rego policy and a openpolicyagent.org/policy=rego label in the fybrik-system namespace: apiVersion : v1 kind : ConfigMap metadata : name : <policy-name> namespace : fybrik-system labels : openpolicyagent.org/policy : rego data : main : | <you rego policy here> Apply the configmap: kubectl apply -f <policy-name>.yaml To remove the policy just remove the configmap: kubectl delete -f <policy-name>.yaml","title":"Using a configmap YAML"},{"location":"tasks/using-opa/#using-a-rego-file","text":"You can use kubectl to create a configmap from a Rego file. To create a configmap named <policy-name> from a Rego file in path <policy-name.rego> : kubectl create configmap <policy-name> --from-file = main = <policy-name.rego> -n fybrik-system kubectl label configmap <policy-name> openpolicyagent.org/policy = rego -n fybrik-system Delete the policy with kubectl delete configmap <policy-name> -n fybrik-system .","title":"Using a Rego file"},{"location":"tasks/using-opa/#using-opaserverbootstrappolicies-field","text":"Another method to upload policies to OPA is to write them as opaServer.bootstrapPolicies field in values.yaml file used for the Fybrik deployment. In this approach the policies are uploaded upon OPA startup. opaServer: # Bootstrap policies to load upon startup bootstrapPolicies: allowSamplePolicy: | - package dataapi.authz rule [{}] { true }","title":"Using opaServer.bootstrapPolicies field"}]}